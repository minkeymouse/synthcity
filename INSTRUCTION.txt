INSTRUCTION

Great job! Now that we have almost completed class Syn_SeqDataLoader and syn_seq_encoder.py, and syn_seq.py, please re-write the plugin_syn_seq.py, fetching on latest versions of files I have pasted them below as reference.
Remember, your job is to re-write the file 'src/synthcity/plugins/generic/plugin_syn_seq.py'.

Project Outline

Explanation: From the following example, we have used sample dataset. We assume 'target' column is highly imbalanced which there are majority of 0 values.
That's why loader automatically assigned 0 for its special value. We also designated special value for 'bp' column.
For 'age' column, syn_seq automatically assign it as numeric so user assigned it as category. There are only two column types, numeric and category.
In following example, user changed syn_order and method of some variable. Use also implemented constraint on variable which if bmi is greater than certain value, target must be greater than 0.
Always remember you are building package, not the one time running code. This is just an example. It needs to be highly generalizable.
Package also changed variable selection matrix accordingly and printed it out it fit process.
Also notice that '_cat' columns were created for numeric variables with special values. They were used in fitting and generating the data.
To be more specific, when plugin takes input loader and run fit, loader.encode is run and return new loader. This is processed loader with new dataframe which contains _cat columns, new order, new methods and all the attributes updated.
Given new loader, syn_seq now starts fitting the model and save them for later generation. We believed that new loader contains all the necessary information to decode the synthesized dataframe so we decided not to return encoder.
To elaborate, after synthesizing 'age', if we use default methods, cart fit 'sex'~'age' and save the model. After saving the model, predict 'sex' based on synthesized(sampled) 'age' and generate the next column.
Once this process is done, 'bmi' is generated with 'bmi'~('sex', 'age'). Now 'bmi' is predicted with two synthesized variables. This is why it is called sequential synthesis. In case of _cat columns, it is fitted and generated as the others. However, method is fixed to 'cart'.
Such synthesis order and variable selection is controled with syn_order and variable_selection. Issue is that when _cat column is created, syn_order and variable_selection needs modification.
For example, if 'bp_cat' and 'target_cat' are created, we need to have temporary syn_order ['sex', 'bmi', 'age', 'bp_cat', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target_cat', 'target'] which is not directly shown to the user but we use this order.
If in variable selection we don't use 'bp', that means we also don't use 'bp_cat' to predict such variable.
In case of original 'bp' column, fit is applied only with numerics. 'bp_cat' is fit back later into 'bp' where it is coded nan temporary. In generation step, for example at cart, it is re-sampled at leaf node if constraint is not met.
If it is equality constraint, it is substituted. If it is other constraints, prediction is done again. If prediction never finds value appropriate for constraint, it means there's something wrong with data so through an error.

EXAMPLE OUTPUT

from synthcity.plugins import Plugins
ods = pd.read_csv("ods.csv", header = True)

loader = Syn_SeqDataLoader(ods, col_type = {'age':'category'}, columns_special_values{'bp':[-0.040099, -0.005670]})
[INFO] Most of the time, it is recommened to have category variables before synthesizing numeric variables
[INFO] Syn_SeqDataLoader init complete:
  - syn_order: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target']
  - special_value: {'bp': [-0.040099, -0.00567]}
  - col_type: {'age': 'category'}
  - data shape: (442, 11)
[DEBUG] After encoder.fit(), detected info:
  - encoder.col_map =>
       age : {'original_dtype': 'float64', 'converted_type': 'category', 'method': 'cart'}
       sex : {'original_dtype': 'float64', 'converted_type': 'category', 'method': 'cart'}
       bmi : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       bp : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s1 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s2 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s3 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s4 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s5 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s6 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       target : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
  - variable_selection_:
         age  sex  bmi  bp  s1  s2  s3  s4  s5  s6  target
age       0    0    0   0   0   0   0   0   0   0       0
sex       1    0    0   0   0   0   0   0   0   0       0
bmi       1    1    0   0   0   0   0   0   0   0       0
bp        1    1    1   0   0   0   0   0   0   0       0
s1        1    1    1   1   0   0   0   0   0   0       0
s2        1    1    1   1   1   0   0   0   0   0       0
s3        1    1    1   1   1   1   0   0   0   0       0
s4        1    1    1   1   1   1   1   0   0   0       0
s5        1    1    1   1   1   1   1   1   0   0       0
s6        1    1    1   1   1   1   1   1   1   0       0
target    1    1    1   1   1   1   1   1   1   1       0
  - date_mins: {}
----------------------------------------------------------------

[INFO] Loader created. loader.shape = (442, 11)
loader columns = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target']


user_custom = {
  'syn_order' : ['sex', 'bmi', 'age', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target'],
  'method' : {'bp':'polyreg'},
  'variable_selection' : {
    "s4": ['sex', 'bmi', 'age', 'bp', 's1', 's2']
    "target": ['sex', 'bmi', 'age', 'bp', 's1', 's2', 's3']
  }
}

syn_model = Plugins().get("syn_seq")
syn_model.fit(nrows = len(ods),user_custom)

[INFO] final synthesis:
  - syn_order: ['sex', 'bmi', 'age', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target']
  - method: {'age':'swr', 'sex':'cart', 'bmi':'cart', 'bp':'polyreg', 's1':'cart', 's2':'cart', 's3':'cart', 's4':'cart', 's5':'cart', 's6':'cart', 'target':'cart'}
  - variable_selection_:
        sex  bmi  age  bp  s1  s2  s3  s4  s5  s6  target
age       0    0    0   0   0   0   0   0   0   0       0
sex       1    0    0   0   0   0   0   0   0   0       0
bmi       1    1    0   0   0   0   0   0   0   0       0
bp        1    1    1   0   0   0   0   0   0   0       0
s1        1    1    1   1   0   0   0   0   0   0       0
s2        1    1    1   1   1   0   0   0   0   0       0
s3        1    1    1   1   1   1   0   0   0   0       0
s4        1    1    1   1   1   1   0   0   0   0       0
s5        1    1    1   1   1   1   1   1   0   0       0
s6        1    1    1   1   1   1   1   1   1   0       0
target    1    1    1   1   1   1   1   0   0   0       0

[INFO] model fitting
Fitting 'sex' ... Done!
Fitting 'bmi' ... Done!
Fitting 'age' ... Done!
Fitting 'bp_cat' ... Done!
Fitting 'bp' ... Done!
Fitting 's1' ... Done!
Fitting 's2' ... Done!
Fitting 's3' ... Done!
Fitting 's4' ... Done!
Fitting 's5' ... Done!
Fitting 's6' ... Done!
Fitting 'target_cat' ... Done!
Fitting 'target' ... Done!

constraints = {
  "target":[
    ("bmi", ">", 0.15),
    ("target", ">", 0)
  ]
}
synthetic_data = syn_model.generate(nrows = len(ods), constraints = constraints).dataframe()

Generating 'sex' ... Done!
Generating 'bmi' ... Done!
Generating 'age' ... Done!
Generating 'bp_cat' ... Done!
Generating 'bp' ... Done!
Generating 's1' ... Done!
Generating 's2' ... Done!
Generating 's3' ... Done!
Generating 's4' ... Done!
Generating 's5' ... Done!
Generating 's6' ... Done!
Generating 'target_cat' ... Done!
Generating 'target' ... Done!

src/synthcity/plugins/core/models/syn_seq/syn_seq_encoder.py
from typing import Optional, Dict, List, Any
import pandas as pd
import numpy as np
from sklearn.base import TransformerMixin, BaseEstimator

class Syn_SeqEncoder(TransformerMixin, BaseEstimator):
    """
    A minimal version of the syn_seq encoder that:
      - Maintains col_map with { original_dtype, converted_type, method } for each column
      - For date columns, does day-offset numeric transformation => converted_type="numeric"
      - Splits numeric columns into col + col_cat, with col_cat placed before the original col in syn_order
      - Expands variable_selection_ accordingly
      - inverse_transform can restore date columns from day-offset,
        and attempt to restore original_dtype if possible
    """

    def __init__(
        self,
        columns_special_values: Optional[Dict[str, List]] = None,
        syn_order: Optional[List[str]] = None,
        max_categories: int = 20,
        col_type: Optional[Dict[str, str]] = None,
        default_method: str = "cart",
    ) -> None:
        """
        Args:
            columns_special_values: { colName : [specialVals...] } ex) {"age":[999], "bp":[-0.04]}
            syn_order: column order to follow
            max_categories: threshold for deciding numeric vs category if not declared
            col_type: user overrides => { "birthdate":"date","sex":"category","bp":"numeric"... }
            default_method: default method name for newly recognized columns
        """
        self.columns_special_values = columns_special_values or {}
        self.syn_order = syn_order or []
        self.max_categories = max_categories
        self.col_type = (col_type or {}).copy()  # user override
        self.default_method = default_method

        # col_map: each col -> { "original_dtype", "converted_type", "method" }
        # converted_type ∈ {"numeric","category"}
        self.col_map: Dict[str, Dict[str, Any]] = {}

        # date minimums for offset
        self.date_mins: Dict[str, pd.Timestamp] = {}

        # variable_selection matrix
        self.variable_selection_: Optional[pd.DataFrame] = None

    # ----------------------------------------------------------------
    # Fit
    # ----------------------------------------------------------------
    def fit(self, X: pd.DataFrame, y=None) -> "Syn_SeqEncoder":
        X = X.copy()
        # 1) Decide syn_order (if not given)
        self._detect_syn_order(X)
        # 2) init col_map
        self._init_col_map(X)
        # 3) detect major special values
        self._detect_special_values(X)
        # 4) build default variable_selection
        self._build_variable_selection(X)
        # 5) store date mins
        self._store_date_min(X)
        return self

    # ----------------------------------------------------------------
    # Transform
    # ----------------------------------------------------------------
    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        # 1) reorder columns
        X = self._reorder_columns(X)
        # 2) date -> day offset => converted_type="numeric"
        X = self._convert_date_to_offset(X)
        # 3) numeric -> split col_cat, place col_cat before original col
        X = self._split_numeric_cols_in_front(X)
        # 4) apply final dtype => numeric or category
        X = self._apply_converted_dtype(X)
        # 5) expand variable_selection if splitted col
        self._update_variable_selection_after_split(X)
        return X

    # ----------------------------------------------------------------
    # inverse_transform
    # ----------------------------------------------------------------
    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        # 1) single special => restore
        for col, specs in self.columns_special_values.items():
            if col in X.columns and len(specs)==1:
                X[col] = X[col].replace(pd.NA, specs[0])

        # 2) offset -> date
        X = self._convert_offset_to_date(X)

        # 3) restore original_dtype if possible
        for col in X.columns:
            if col not in self.col_map:
                # splitted col?
                continue
            orig_dt = self.col_map[col]["original_dtype"]
            if orig_dt is not None:
                try:
                    X[col] = X[col].astype(orig_dt)
                except:
                    pass
        return X

    # ----------------------------------------------------------------
    # set_method / get_method : if aggregator or user wants to override
    # ----------------------------------------------------------------
    def set_method(self, col: str, method: str) -> None:
        """Override method for a column."""
        if col not in self.col_map:
            print(f"[WARNING] col_map has no '{col}'")
            return
        self.col_map[col]["method"] = method

    def get_method_map(self) -> Dict[str, str]:
        """Return { col : method } for all columns in col_map."""
        return { c: info["method"] for c, info in self.col_map.items() }

    # =================================================================
    # Internals
    # =================================================================
    def _detect_syn_order(self, X: pd.DataFrame):
        if not self.syn_order:
            self.syn_order = list(X.columns)
        else:
            self.syn_order = [c for c in self.syn_order if c in X.columns]

    def _init_col_map(self, X: pd.DataFrame):
        """
        For each col in syn_order, decide if numeric or category or date => eventually store converted_type
         - date => will become numeric in transform, but we'll keep track for inverse
        """
        self.col_map.clear()

        for col in self.syn_order:
            if col not in X.columns:
                continue
            orig_dt = str(X[col].dtype)

            # Decide if date => (converted_type="numeric"), else numeric/category
            declared = self.col_type.get(col, "").lower()
            if declared == "date":
                # We'll treat as numeric, but keep date info for inverse
                conv_type = "numeric"
            elif declared == "numeric":
                conv_type = "numeric"
            elif declared == "category":
                conv_type = "category"
            else:
                # fallback auto
                if pd.api.types.is_datetime64_any_dtype(X[col]):
                    conv_type = "numeric"  # internally day-offset
                    self.col_type[col] = "date"  # mark for inverse
                else:
                    nuniq = X[col].nunique()
                    if nuniq > self.max_categories:
                        conv_type = "numeric"
                    else:
                        conv_type = "category"

            # save to col_map
            self.col_map[col] = {
                "original_dtype": orig_dt,
                "converted_type": conv_type,
                "method": self.default_method
            }

    def _detect_special_values(self, X: pd.DataFrame):
        for col in self.syn_order:
            if col not in X.columns:
                continue
            freq = X[col].value_counts(dropna=False, normalize=True)
            big_ones = freq[freq>0.9].index.tolist()
            if big_ones:
                exist = self.columns_special_values.get(col, [])
                merged = set(exist).union(big_ones)
                self.columns_special_values[col] = list(merged)

    def _build_variable_selection(self, X: pd.DataFrame):
        idx = self.syn_order
        vs = pd.DataFrame(0, index=idx, columns=idx)
        for i in range(len(idx)):
            for j in range(i):
                vs.iat[i,j] = 1
        self.variable_selection_ = vs

    def _store_date_min(self, X: pd.DataFrame):
        """For date columns => store min date for offset calc"""
        for col in self.syn_order:
            # if user said date or we auto-detected => self.col_type[col]=="date"
            # but in col_map => "converted_type":"numeric"
            # we can check self.col_type if "date"
            if self.col_type.get(col) == "date":
                dt_series = pd.to_datetime(X[col], errors="coerce")
                self.date_mins[col] = dt_series.min()

    def _reorder_columns(self, X: pd.DataFrame) -> pd.DataFrame:
        keep = [c for c in self.syn_order if c in X.columns]
        return X[keep]

    def _convert_date_to_offset(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        If col_type[col]=="date", then X[col] => day offset vs min
        col_map[col]["converted_type"] stays "numeric"
        """
        for col in self.syn_order:
            if self.col_type.get(col) == "date" and col in X.columns:
                mindt = self.date_mins.get(col, None)
                X[col] = pd.to_datetime(X[col], errors="coerce")
                if mindt is None:
                    mindt = X[col].min()
                    self.date_mins[col] = mindt
                X[col] = (X[col] - mindt).dt.days
        return X

    def _split_numeric_cols_in_front(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        For each col whose converted_type is "numeric", create col_cat.
        Insert col_cat before col in syn_order, add to col_map
        """
        for col in list(self.syn_order):
            if col not in self.col_map:
                continue
            if self.col_map[col]["converted_type"] != "numeric":
                continue
            if col not in X.columns:
                continue

            cat_col = col + "_cat"
            specials = self.columns_special_values.get(col, [])

            X[cat_col] = X[col].apply(
                lambda v: v if (pd.isna(v) or v in specials) else -777
            )
            X[cat_col] = X[cat_col].fillna(-9999)
            X[col] = X[col].apply(
                lambda v: v if (not pd.isna(v) and v not in specials) else pd.NA
            )
            X[cat_col] = X[cat_col].astype("category")

            # insert cat_col in syn_order
            if cat_col not in self.syn_order:
                idx = self.syn_order.index(col)
                self.syn_order.insert(idx, cat_col)

            # add to col_map
            self.col_map[cat_col] = {
                "original_dtype": None,
                "converted_type": "category",
                "method": self.default_method
            }

        return X

    def _apply_converted_dtype(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        If converted_type=="numeric" => to_numeric
        else "category" => astype("category")
        """
        for col in self.syn_order:
            if col not in X.columns:
                continue
            cinfo = self.col_map.get(col, {})
            ctype = cinfo.get("converted_type","category")
            if ctype=="numeric":
                X[col] = pd.to_numeric(X[col], errors="coerce")
            else:
                X[col] = X[col].astype("category")
        return X

    def _convert_offset_to_date(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        If self.col_type[col]=="date", => offset => date
        """
        for col in self.syn_order:
            if self.col_type.get(col)=="date" and col in X.columns:
                offset = pd.to_numeric(X[col], errors="coerce")
                mindt = self.date_mins.get(col, None)
                if mindt is not None:
                    X[col] = pd.to_timedelta(offset, unit="D") + mindt
        return X

    def _update_variable_selection_after_split(self, X: pd.DataFrame):
        if self.variable_selection_ is None:
            return
        old_vs = self.variable_selection_
        old_idx = list(old_vs.index)
        old_cols = list(old_vs.columns)
        new_cols = [c for c in X.columns if c not in old_idx]
        if not new_cols:
            return

        vs_new = pd.DataFrame(0, index=old_idx+new_cols, columns=old_cols+new_cols)
        for r in old_idx:
            for c in old_cols:
                vs_new.at[r,c] = old_vs.at[r,c]

        for c_new in new_cols:
            if c_new.endswith("_cat"):
                c_base = c_new[:-4]
                # copy row/col from base
                if c_base in vs_new.index and c_base in vs_new.columns:
                    for c2 in old_cols:
                        vs_new.at[c_new, c2] = vs_new.at[c_base, c2]
                    for r2 in old_idx:
                        vs_new.at[r2, c_new] = vs_new.at[r2, c_base]
                vs_new.at[c_new, c_new] = 0

        self.variable_selection_ = vs_new

    # ------------------ update_variable_selection
    @staticmethod
    def update_variable_selection(
        var_sel_df: pd.DataFrame,
        user_dict: Dict[str, List[str]]
    ) -> pd.DataFrame:
        """
        For row=target, col= predictor => set 1
        """
        for tgt, preds in user_dict.items():
            if tgt not in var_sel_df.index:
                print(f"[WARNING] {tgt} not in var_sel_df => skip")
                continue
            var_sel_df.loc[tgt, :] = 0
            for p in preds:
                if p in var_sel_df.columns:
                    var_sel_df.loc[tgt, p] = 1
                else:
                    print(f"[WARNING] predictor {p} not in columns => skip")
        return var_sel_df


src/synthcity/plugins/core/models/syn_seq/cart.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/ctree.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/logreg.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/misc.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/norm.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/pmm.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/polyreg.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/rf.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/syn_seq.py
# File: syn_seq.py

from typing import Any, Dict, List, Optional, Tuple, Union
import pandas as pd
import numpy as np

# If your constraints class is in syn_seq_constraints.py
from synthcity.plugins.core.models.syn_seq.syn_seq_constraints import Syn_SeqConstraints

# The column-by-column methods
from synthcity.plugins.core.models.syn_seq.methods.cart import syn_cart
from synthcity.plugins.core.models.syn_seq.methods.ctree import syn_ctree
from synthcity.plugins.core.models.syn_seq.methods.logreg import syn_logreg
from synthcity.plugins.core.models.syn_seq.methods.norm import syn_norm, syn_lognorm
from synthcity.plugins.core.models.syn_seq.methods.pmm import syn_pmm
from synthcity.plugins.core.models.syn_seq.methods.polyreg import syn_polyreg
from synthcity.plugins.core.models.syn_seq.methods.rf import syn_rf
from synthcity.plugins.core.models.syn_seq.methods.misc import syn_random, syn_swr


###############################################
# Define which methods are allowed for numeric vs. category
###############################################
ALLOWED_NUMERIC_METHODS = {
    "norm",
    "lognorm",
    "pmm",
    "cart",
    "ctree",
    "rf",
    "random",
    "swr",
    # possibly "polyreg", "logreg" if numeric is strictly binary, etc.
}

ALLOWED_CATEGORY_METHODS = {
    "cart",
    "ctree",
    "rf",
    "logreg",   # if user wants logistic for binary categories
    "polyreg",  # multi-category
    "random",
    "swr",
}


class Syn_Seq:
    """
    A sequential-synthesis aggregator that:

      1) If user_custom => we call loader.update_user_custom(user_custom)
         so that 'syn_order', 'method', 'variable_selection' are updated.
      2) We do encoded_loader, enc_dict = loader.encode(...) => returns a new encoded loader
         which has:
           - possible _cat columns for special values
           - updated method dictionary
           - updated variable_selection
           - updated col_type
      3) We partial-fit each splitted column in a hidden order (any X_cat first => forced "cart" method),
         then the main col => user-chosen or fallback. We also check col_type (numeric vs. category)
         to ensure the chosen method is appropriate. If not, we fallback.
      4) generate(...) => synthesizes col-by-col in that hidden order, optionally applying constraints.
         If strict => multiple attempts until we gather enough rows passing constraints.
      5) decode => final result is a pd.DataFrame with only the original columns (no _cat).
    """

    def __init__(
        self,
        random_state: int = 0,
        strict: bool = True,
        sampling_patience: int = 100,
        default_first_method: str = "swr",
        default_other_method: str = "cart",
        seq_id_col: str = "seq_id",
        seq_time_col: str = "seq_time_id",
    ):
        """
        Args:
            random_state: reproducibility
            strict: if True => repeated attempts to meet constraints
            sampling_patience: max tries if strict
            default_first_method: fallback if the first column has no method
            default_other_method: fallback for subsequent columns if user didn't specify
            seq_id_col, seq_time_col: for sequence-based constraints if needed
        """
        self.random_state = random_state
        self.strict = strict
        self.sampling_patience = sampling_patience
        self.default_first_method = default_first_method
        self.default_other_method = default_other_method
        self.seq_id_col = seq_id_col
        self.seq_time_col = seq_time_col

        # We'll store partial fits: splitted_col => {method, predictors, fit_info}
        self._col_models: Dict[str, Dict[str, Any]] = {}
        self._model_trained = False

        # We'll store enc_dict from loader.encode(...) so we can decode after generation
        self._enc_dict: Dict[str, Any] = {}

    def fit(
        self,
        loader: Any,  # typically a Syn_SeqDataLoader
        user_custom: Optional[dict] = None,
        *args,
        **kwargs
    ) -> "Syn_Seq":
        """
        1) Merge user_custom => loader.update_user_custom(...)
        2) encode => splitted loader => new df with _cat columns => encoded_loader
        3) partial-fit each splitted column, forcing 'cart' for *cat columns,
           adjusting method based on col_type if there's a mismatch
        4) store partial fits
        """
        # (1) incorporate user overrides
        if user_custom:
            loader.update_user_custom(user_custom)

        # (2) encode => splitted columns
        encoded_loader, enc_dict = loader.encode(encoders=None)
        self._enc_dict = enc_dict

        # get references
        df_encoded = encoded_loader.dataframe()
        if df_encoded.empty:
            raise ValueError("No data after encoding => cannot train on empty DataFrame.")

        syn_order = encoded_loader.syn_order            # e.g. ["age","sex","bmi","bp","target"]
        method_dict = getattr(encoded_loader, "_method", {})  # user might store it here
        varsel_df = getattr(encoded_loader._encoder, "variable_selection_", None)
        if varsel_df is None:
            varsel_df = pd.DataFrame(0, index=syn_order, columns=syn_order)

        # fallback if user didn't specify a method => default
        final_method: Dict[str, str] = {}
        for i, col_name in enumerate(syn_order):
            if col_name not in method_dict:
                final_method[col_name] = self.default_first_method if i == 0 else self.default_other_method
            else:
                final_method[col_name] = method_dict[col_name]

        # read col_type from encoded_loader
        col_type_map = getattr(encoded_loader, "col_type", {})

        # now do a splitted_map => e.g. "bp" => ["bp_cat","bp"], etc.
        splitted_map: Dict[str, List[str]] = {c: [] for c in syn_order}
        for c_enc in df_encoded.columns:
            base_c = c_enc[:-4] if c_enc.endswith("_cat") else c_enc
            splitted_map.setdefault(base_c, []).append(c_enc)

        # build final "fit_order" => cat first => forced 'cart'
        fit_order: List[str] = []
        method_map: Dict[str, str] = {}

        for i, base_col in enumerate(syn_order):
            sub_cols = splitted_map.get(base_col, [])
            cat_cols = [x for x in sub_cols if x.endswith("_cat")]
            main_cols = [x for x in sub_cols if not x.endswith("_cat")]

            # (A) cat columns => forced "cart"
            for sc in cat_cols:
                fit_order.append(sc)
                method_map[sc] = "cart"

            # (B) main col => check col_type => fix method if mismatch
            chosen_m = final_method.get(base_col, self.default_other_method)
            declared_t = col_type_map.get(base_col, "category")  # default "category"

            # if numeric => ensure chosen_m in ALLOWED_NUMERIC_METHODS
            # if category => ensure chosen_m in ALLOWED_CATEGORY_METHODS
            cfix = chosen_m.strip().lower()
            if declared_t.lower() == "numeric":
                if cfix not in ALLOWED_NUMERIC_METHODS:
                    # fallback => e.g. "norm"
                    fallback_m = self.default_first_method if i == 0 else "norm"
                    print(f"[TYPE-CHECK] '{base_col}' is numeric but method '{cfix}' invalid => fallback '{fallback_m}'")
                    cfix = fallback_m
            else:
                # category => ensure cfix in ALLOWED_CATEGORY_METHODS
                if cfix not in ALLOWED_CATEGORY_METHODS:
                    # fallback => "cart"
                    print(f"[TYPE-CHECK] '{base_col}' is category but method '{cfix}' invalid => fallback 'cart'")
                    cfix = "cart"

            # store final
            for sc in main_cols:
                fit_order.append(sc)
                method_map[sc] = cfix

        # leftover columns not in syn_order => fallback
        leftover = [c for c in df_encoded.columns if c not in fit_order]
        for c in leftover:
            fit_order.append(c)
            method_map[c] = self.default_other_method

        # Logging
        print("[INFO] final synthesis:")
        print(f"  - syn_order: {syn_order}")
        print(f"  - method: {final_method}")
        print("  - variable_selection_:")
        print(varsel_df)
        print("\n[INFO] model fitting")

        self._col_models.clear()
        for col_enc in fit_order:
            base_c = col_enc[:-4] if col_enc.endswith("_cat") else col_enc
            chosen_m = method_map[col_enc]

            # gather predictors from varsel_df
            preds_list = []
            if base_c in varsel_df.index:
                row_mask = varsel_df.loc[base_c] == 1
                preds_list = varsel_df.columns[row_mask].tolist()
                # if base is not used => also remove base_cat
                preds_list = [
                    p for p in preds_list
                    if not (p.endswith("_cat") and p[:-4] not in preds_list)
                ]

            y = df_encoded[col_enc].values
            X = df_encoded[preds_list].values if preds_list else np.zeros((len(y), 0))

            print(f"Fitting '{col_enc}' ... ", end="")
            fit_info = self._fit_single_column(y, X, chosen_m)
            self._col_models[col_enc] = {
                "method": chosen_m,
                "predictors": preds_list,
                "fit_info": fit_info,
            }
            print("Done!")

        self._model_trained = True
        return self

    def _fit_single_column(self, y: np.ndarray, X: np.ndarray, method_name: str) -> Dict[str, Any]:
        """
        We store partial info for each col => { "type":..., "obs_y":..., "obs_X":... }
        used at generation time to call e.g. syn_cart(...) or syn_norm(...)
        """
        m = method_name.strip().lower()
        if m in {
            "cart", "ctree", "rf", "norm", "lognorm", "pmm", "logreg", "polyreg"
        }:
            return {"type": m, "obs_y": y, "obs_X": X}
        elif m == "swr":
            return {"type": "swr", "obs_y": y}
        elif m == "random":
            return {"type": "random", "obs_y": y}
        else:
            # fallback => random
            return {"type": "random", "obs_y": y}

    def generate(
        self,
        count: int,
        encoded_loader: Any,  # same type of Syn_SeqDataLoader
        constraints: Optional[Dict[str, List[Any]]] = None,
        *args,
        **kwargs
    ) -> pd.DataFrame:
        """
        1) generate splitted columns in the fitted order
        2) apply constraints if any
        3) decode => revert to original columns
        4) return final DataFrame
        """
        if not self._model_trained:
            raise RuntimeError("Must fit(...) aggregator before .generate().")

        syn_constraints = None
        if constraints is not None:
            syn_constraints = Syn_SeqConstraints(chained_rules=constraints)

        if self.strict and syn_constraints:
            df_synth = self._attempt_strict_generation(count, syn_constraints)
        else:
            df_synth = self._generate_once(count)
            if syn_constraints:
                df_synth = syn_constraints.correct_equals(df_synth)
                df_synth = syn_constraints.match(df_synth)

        # decode => final
        tmp_loader = encoded_loader.decorate(df_synth)
        final_loader = tmp_loader.decode(self._enc_dict)
        return final_loader.dataframe()

    def _attempt_strict_generation(self, count: int, syn_constraints: Syn_SeqConstraints) -> pd.DataFrame:
        """
        repeated tries => gather enough rows that pass constraints
        do direct substitution for '=' => then match => drop_duplicates => accumulate
        """
        result_df = pd.DataFrame()
        tries = 0
        while len(result_df) < count and tries < self.sampling_patience:
            tries += 1
            chunk = self._generate_once(count)
            chunk = syn_constraints.correct_equals(chunk)
            chunk = syn_constraints.match(chunk)
            chunk = chunk.drop_duplicates()
            result_df = pd.concat([result_df, chunk], ignore_index=True)

        return result_df.head(count)

    def _generate_once(self, count: int) -> pd.DataFrame:
        """
        single pass => produce splitted columns in fit_order
        """
        fit_order = list(self._col_models.keys())  # splitted columns
        syn_df = pd.DataFrame(index=range(count))

        print("")
        for col_enc in fit_order:
            info = self._col_models[col_enc]
            method_name = info["method"]
            preds = info["predictors"]
            fit_data = info["fit_info"]

            Xp = syn_df[preds].values if preds else np.zeros((count, 0))
            print(f"Generating '{col_enc}' ... ", end="")
            new_vals = self._generate_single_column(method_name, fit_data, Xp, count)
            syn_df[col_enc] = new_vals
            print("Done!")

        return syn_df

    def _generate_single_column(
        self,
        method: str,
        fit_info: Dict[str, Any],
        Xp: np.ndarray,
        count: int
    ) -> pd.Series:
        """
        Dispatch to the specialized column-synthesis function
        """
        m = method.strip().lower()
        y_obs = fit_info.get("obs_y")
        X_obs = fit_info.get("obs_X")

        if m == "cart":
            result = syn_cart(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "ctree":
            result = syn_ctree(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "rf":
            result = syn_rf(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "norm":
            result = syn_norm(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "lognorm":
            result = syn_lognorm(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "pmm":
            result = syn_pmm(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "logreg":
            result = syn_logreg(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "polyreg":
            result = syn_polyreg(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "swr":
            result = syn_swr(
                y=y_obs,
                X=None,
                Xp=np.zeros((count,1)),
                random_state=self.random_state,
            )
            return pd.Series(result["res"])
        elif m == "random":
            result = syn_random(
                y=y_obs,
                X=None,
                Xp=np.zeros((count,1)),
                random_state=self.random_state,
            )
            return pd.Series(result["res"])
        else:
            # fallback => random
            fallback = syn_random(
                y=y_obs,
                X=None,
                Xp=np.zeros((count,1)),
                random_state=self.random_state,
            )
            return pd.Series(fallback["res"])


src/synthcity/plugins/core/models/syn_seq/syn_seq_constraints.py
# File: syn_seq_constraints.py

from typing import Any, List, Dict, Tuple, Union
import pandas as pd
import numpy as np

# We import the base Constraints to inherit from
from synthcity.plugins.core.constraints import Constraints

# 
# A single sub-rule is (feature, op, value).
# For "chained" logic, we might interpret:
#    "For rows that pass all sub-rules (1..k-1), apply sub-rule k as a filter or correction."
#
# Example constraint input:
#   {
#       "N1": [
#           ("C1", "in", ["AAA","BBB"]),
#           ("N1", ">", 125)
#       ]
#   }
# means:
#  1) If a row passes (C1 in [AAA,BBB]), 
#  2) Then also enforce (N1 > 125) on that row.
#
# If the first sub-rule is not satisfied, the second doesn't apply to that row.
# If the first sub-rule is satisfied but not the second => row fails entirely.
#

class SynSeqConstraints(Constraints):
    """
    An extension that supports "chained" constraints for sequential logic:
      - If sub-rule 1 is satisfied => we must also pass sub-rule 2, and so on.
      - 'chained_rules' can be a dict of { targetCol : [ (feature, op, val), (feature, op, val), ... ] }.
        Example:
          {
            "N1": [
                ("C1", "in", ["AAA","BBB"]),
                ("N1", ">", 125)
            ]
          }
        read as: "If (C1 in [AAA,BBB]) => enforce (N1 > 125)".
      
      Each sub-rule uses the same _eval logic for <, <=, >, >=, ==, in, etc.
      If a sub-rule fails => that entire row fails (filtered out) if using .match(), 
      or is corrected if possible (.correct).
    """

    def __init__(
        self,
        # You can still pass standard constraints as "rules",
        # or pass new "chained_rules" in dict format
        rules: List[Tuple[str, str, Any]] = None,
        chained_rules: Dict[str, List[Tuple[str, str, Any]]] = None,
        seq_id_feature: str = "seq_id",
        seq_time_id_feature: str = "seq_time_id",
        **kwargs: Any,
    ):
        # Let the base constructor handle standard 'rules'
        super().__init__(rules=rules if rules else [])
        self.seq_id_feature = seq_id_feature
        self.seq_time_id_feature = seq_time_id_feature

        # We'll store the "chained" sub-rules in a separate structure
        # keyed by "target column" or "some label"
        self.chained_rules = chained_rules if chained_rules else {}

    def match(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Overridden. We can do row-level or entire-sequence filtering. 
        If you still want entire-sequence filtering, you can define match_sequential() and call it below.
        For demonstration, let's do row-level filtering with chained sub-rules.
        """
        df_copy = X.copy()
        # first, apply base constraints (self.rules) at row-level:
        base_mask = super().filter(df_copy)
        df_filtered = df_copy[base_mask].copy()
        if df_filtered.empty:
            return df_filtered

        # next, apply "chained" constraints
        df_filtered = self._match_chained(df_filtered)
        return df_filtered

    def _match_chained(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        For each entry in self.chained_rules => interpret a *sequence* of sub-rules.
        If the row passes sub-rule[0], sub-rule[1], ... sub-rule[n-2], 
        then sub-rule[n-1] is also enforced. 
        If it fails at any step => the row is filtered out or "corrected" (depending on your logic).
        
        For simplicity below, we do "if sub-rule[i] is satisfied => proceed, else row is out."
        """
        df = X.copy()
        for target_key, rule_chain in self.chained_rules.items():
            # We interpret the chain in order
            # e.g. [("C1","in", [...]), ("N1",">",125)]
            # step i=0 => a filter => if row fails => out
            # step i=1 => a further filter => if row fails => out
            # etc.

            # We'll build a mask for these sub-rules
            keep_mask = pd.Series([True]*len(df), index=df.index)

            for (feature, op, operand) in rule_chain:
                cur_mask = self._eval(df, feature, op, operand)
                # only keep the rows that pass this sub-rule
                keep_mask = keep_mask & cur_mask

            # after we apply all sub-rules in the chain, 
            # rows that didn't pass => out
            df = df[keep_mask].copy()
            if df.empty:
                break

        return df

    def _eval(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.Index:
        """
        If we want to also handle direct substitution '=' or '==' or to skip it, we can override _eval.
        If the user wants a different meaning for '=' in chain logic, we can do so.
        Otherwise, we rely on base Constraints._eval for <, <=, >, >=, ==, in, dtype, etc.
        """
        # If you want direct substitution for '=' or '==', do so in _correct or in some separate logic.
        if op in ["=", "=="]:
            # interpret as equality check
            return (X[feature] == operand) | X[feature].isna()
        else:
            # fallback to base method
            return super()._eval(X, feature, op, operand)

    def _correct(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.DataFrame:
        """
        If user wants direct substitution for '=' => set that col's entire column to operand. 
        Or you can do row-level changes only for failing rows, etc.
        """
        if op in ["=", "=="]:
            X.loc[:, feature] = operand
            return X
        return super()._correct(X, feature, op, operand)

    # If you want entire-sequence logic, you'd define match_sequential below:

    # Example
    #     constraint = {
    #   "N1": [
    #     ["C1", "in", ["AAA","BBB"]],
    #     ["N1", ">", 125]
    #   ]
    # }

    # def match_sequential(self, X: pd.DataFrame) -> pd.DataFrame:
    #     """
    #     Example method that applies constraints at the group (sequence) level:
    #       - if a sub-rule fails for ANY row => remove entire sequence
    #       - or do direct substitution in entire sequence
    #     """
    #     df_copy = X.copy()
    #     base_mask = super().filter(df_copy)
    #     # group by seq_id
    #     grouped = df_copy.groupby(self.seq_id_feature)
    #
    #     keep_seq_ids = []
    #     for seq_id, group in grouped:
    #         # if all rows pass => keep entire sequence
    #         if base_mask[group.index].all():
    #             # next, check chained
    #             # for each sub-rule chain, we can test if group passes
    #             # if not => exclude entire seq
    #             # or we can do partial correction
    #             # ...
    #             keep_seq_ids.append(seq_id)
    #
    #     return df_copy[df_copy[self.seq_id_feature].isin(keep_seq_ids)]



src/synthcity/plugins/core/dataloader.py
# stdlib
import random
from abc import ABCMeta, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union

# third party
import numpy as np
import numpy.ma as ma
import pandas as pd
import PIL
import torch
from pydantic import validate_arguments
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torchvision import transforms

# synthcity absolute
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.dataset import FlexibleDataset, TensorDataset
from synthcity.plugins.core.models.feature_encoder import DatetimeEncoder
from synthcity.utils.compression import compress_dataset, decompress_dataset
from synthcity.utils.serialization import dataframe_hash

# Syn_Seq
from synthcity.plugins.core.models.syn_seq.syn_seq_encoder import Syn_SeqEncoder


class DataLoader(metaclass=ABCMeta):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.DataLoader
        :parts: 1

    Base class for all data loaders.

    Each derived class must implement the following methods:
        unpack() - a method that unpacks the columns and returns features and labels (X, y).
        decorate() - a method that creates a new instance of DataLoader by decorating the input data with the same DataLoader properties (e.g. sensitive features, target column, etc.)
        dataframe() - a method that returns the pandas dataframe that contains all features and samples
        numpy() - a method that returns the numpy array that contains all features and samples
        info() - a method that returns a dictionary of DataLoader information
        __len__() - a method that returns the number of samples in the DataLoader
        satisfies() - a method that tests if the current DataLoader satisfies the constraint provided
        match() - a method that returns a new DataLoader where the provided constraints are met
        from_info() - a static method that creates a DataLoader from the data and the information dictionary
        sample() - returns a new DataLoader that contains a random subset of N samples
        drop() - returns a new DataLoader with a list of columns dropped
        __getitem__() - getting features by names
        __setitem__() - setting features by names
        train() - returns a DataLoader containing the training set
        test() - returns a DataLoader containing the testing set
        fillna() - returns a DataLoader with NaN filled by the provided number(s)


    If any method implementation is missing, the class constructor will fail.

    Constructor Args:
        data_type: str
            The type of DataLoader, currently supports "generic", "time_series" and "survival".
        data: Any
            The object that contains the data
        static_features: List[str]
            List of feature names that are static features (as opposed to temporal features).
        temporal_features:
            List of feature names that are temporal features, i.e. observed over time.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        outcome_features:
            The feature name that provides labels for downstream tasks.
    """

    def __init__(
        self,
        data_type: str,
        data: Any,
        static_features: List[str] = [],
        temporal_features: List[str] = [],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        outcome_features: List[str] = [],
        train_size: float = 0.8,
        random_state: int = 0,
        **kwargs: Any,
    ) -> None:
        self.static_features = static_features
        self.temporal_features = temporal_features
        self.sensitive_features = sensitive_features
        self.important_features = important_features
        self.outcome_features = outcome_features
        self.random_state = random_state

        self.data = data
        self.data_type = data_type
        self.train_size = train_size

    def raw(self) -> Any:
        return self.data

    @abstractmethod
    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        ...

    @abstractmethod
    def decorate(self, data: Any) -> "DataLoader":
        ...

    def type(self) -> str:
        return self.data_type

    @property
    @abstractmethod
    def shape(self) -> tuple:
        ...

    @property
    @abstractmethod
    def columns(self) -> list:
        ...

    @abstractmethod
    def dataframe(self) -> pd.DataFrame:
        ...

    @abstractmethod
    def numpy(self) -> np.ndarray:
        ...

    @property
    def values(self) -> np.ndarray:
        return self.numpy()

    @abstractmethod
    def info(self) -> dict:
        ...

    @abstractmethod
    def __len__(self) -> int:
        ...

    @abstractmethod
    def satisfies(self, constraints: Constraints) -> bool:
        ...

    @abstractmethod
    def match(self, constraints: Constraints) -> "DataLoader":
        ...

    @staticmethod
    @abstractmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        ...

    @abstractmethod
    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        ...

    @abstractmethod
    def drop(self, columns: list = []) -> "DataLoader":
        ...

    @abstractmethod
    def __getitem__(self, feature: Union[str, list]) -> Any:
        ...

    @abstractmethod
    def __setitem__(self, feature: str, val: Any) -> None:
        ...

    @abstractmethod
    def train(self) -> "DataLoader":
        ...

    @abstractmethod
    def test(self) -> "DataLoader":
        ...

    def hash(self) -> str:
        return dataframe_hash(self.dataframe())

    def __repr__(self, *args: Any, **kwargs: Any) -> str:
        return self.dataframe().__repr__(*args, **kwargs)

    def _repr_html_(self, *args: Any, **kwargs: Any) -> Any:
        return self.dataframe()._repr_html_(*args, **kwargs)

    @abstractmethod
    def fillna(self, value: Any) -> "DataLoader":
        ...

    @abstractmethod
    def compression_protected_features(self) -> list:
        ...

    def domain(self) -> Optional[str]:
        return None

    def compress(
        self,
    ) -> Tuple["DataLoader", Dict]:
        to_compress = self.data.copy().drop(
            columns=self.compression_protected_features()
        )
        compressed, context = compress_dataset(to_compress)
        for protected_col in self.compression_protected_features():
            compressed[protected_col] = self.data[protected_col]

        return self.decorate(compressed), context

    def decompress(self, context: Dict) -> "DataLoader":
        decompressed = decompress_dataset(self.data, context)

        return self.decorate(decompressed)

    def encode(
        self,
        encoders: Optional[Dict[str, Any]] = None,
    ) -> Tuple["DataLoader", Dict]:
        encoded = self.dataframe().copy()
        if encoders is not None:
            for col in encoders:
                if col not in encoded.columns:
                    continue
                encoded[col] = encoders[col].transform(encoded[col])
        else:
            encoders = {}

            for col in encoded.columns:
                if (
                    encoded[col].infer_objects().dtype.kind == "i"
                    and encoded[col].min() == 0
                    and encoded[col].max() == len(encoded[col].unique()) - 1
                ):
                    continue

                if (
                    encoded[col].infer_objects().dtype.kind in ["O", "b"]
                    or len(encoded[col].unique()) < 15
                ):
                    encoder = LabelEncoder().fit(encoded[col])
                    encoded[col] = encoder.transform(encoded[col])
                    encoders[col] = encoder
                elif encoded[col].infer_objects().dtype.kind in ["M"]:
                    encoder = DatetimeEncoder().fit(encoded[col])
                    encoded[col] = encoder.transform(encoded[col]).values
                    encoders[col] = encoder
        return self.from_info(encoded, self.info()), encoders

    def decode(
        self,
        encoders: Dict[str, Any],
    ) -> "DataLoader":
        decoded = self.dataframe().copy()

        for col in encoders:
            if isinstance(encoders[col], LabelEncoder):
                decoded[col] = decoded[col].astype(int)
            else:
                decoded[col] = decoded[col].astype(float)

            decoded[col] = encoders[col].inverse_transform(decoded[col])

        return self.from_info(decoded, self.info())

    @abstractmethod
    def is_tabular(self) -> bool:
        ...

    @abstractmethod
    def get_fairness_column(self) -> Union[str, Any]:
        ...


class GenericDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.GenericDataLoader
        :parts: 1

    Data loader for generic tabular data.

    Constructor Args:
        data: Union[pd.DataFrame, list, np.ndarray]
            The dataset. Either a Pandas DataFrame or a Numpy Array.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        target_column: Optional[str]
            The feature name that provides labels for downstream tasks.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        domain_column: Optional[str]
            Optional domain label, used for domain adaptation algorithms.
        random_state: int
            Defaults to zero.

    Example:
        >>> from sklearn.datasets import load_diabetes
        >>> from synthcity.plugins.core.dataloader import GenericDataLoader
        >>> X, y = load_diabetes(return_X_y=True, as_frame=True)
        >>> X["target"] = y
        >>> # Important note: preprocessing data with OneHotEncoder or StandardScaler is not needed or recommended.
        >>> # Synthcity handles feature encoding and standardization internally.
        >>> loader = GenericDataLoader(X, target_column="target", sensitive_columns=["sex"],)
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: Union[pd.DataFrame, list, np.ndarray],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        target_column: Optional[str] = None,
        fairness_column: Optional[str] = None,
        domain_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)

        data.columns = data.columns.astype(str)
        if target_column is not None:
            self.target_column = target_column
        elif len(data.columns) > 0:
            self.target_column = data.columns[-1]
        else:
            self.target_column = "---"

        self.fairness_column = fairness_column
        self.domain_column = domain_column

        super().__init__(
            data_type="generic",
            data=data,
            static_features=list(data.columns),
            sensitive_features=sensitive_features,
            important_features=important_features,
            outcome_features=[self.target_column],
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape

    def domain(self) -> Optional[str]:
        return self.domain_column

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    @property
    def columns(self) -> list:
        return list(self.data.columns)

    def compression_protected_features(self) -> list:
        out = [self.target_column]
        domain = self.domain()

        if domain is not None:
            out.append(domain)

        return out

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        X = self.data.drop(columns=[self.target_column])
        y = self.data[self.target_column]

        if as_numpy:
            return np.asarray(X), np.asarray(y)
        return X, y

    def dataframe(self) -> pd.DataFrame:
        return self.data

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": self.static_features,
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "outcome_features": self.outcome_features,
            "target_column": self.target_column,
            "fairness_column": self.fairness_column,
            "domain_column": self.domain_column,
            "train_size": self.train_size,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return GenericDataLoader(
            data,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            target_column=self.target_column,
            random_state=self.random_state,
            train_size=self.train_size,
            fairness_column=self.fairness_column,
            domain_column=self.domain_column,
        )

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self.data)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.decorate(constraints.match(self.data))

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        return self.decorate(self.data.sample(count, random_state=random_state))

    def drop(self, columns: list = []) -> "DataLoader":
        return self.decorate(self.data.drop(columns=columns))

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "GenericDataLoader":
        if not isinstance(data, pd.DataFrame):
            raise ValueError(f"Invalid data type {type(data)}")

        return GenericDataLoader(
            data,
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            target_column=info["target_column"],
            fairness_column=info["fairness_column"],
            domain_column=info["domain_column"],
            train_size=info["train_size"],
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data[feature] = val

    def _train_test_split(self) -> Tuple:
        stratify = None
        if self.target_column in self.data:
            target = self.data[self.target_column]
            if target.value_counts().min() > 1:
                stratify = target

        return train_test_split(
            self.data,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )

    def train(self) -> "DataLoader":
        train_data, _ = self._train_test_split()
        return self.decorate(train_data.reset_index(drop=True))

    def test(self) -> "DataLoader":
        _, test_data = self._train_test_split()
        return self.decorate(test_data.reset_index(drop=True))

    def fillna(self, value: Any) -> "DataLoader":
        self.data = self.data.fillna(value)
        return self

    def is_tabular(self) -> bool:
        return True


class SurvivalAnalysisDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.SurvivalAnalysisDataLoader
        :parts: 1

    Data Loader for Survival Analysis Data

    Constructor Args:
        data: Union[pd.DataFrame, list, np.ndarray]
            The dataset. Either a Pandas DataFrame or a Numpy Array.
        time_to_event_column: str
            Survival Analysis specific time-to-event feature
        target_column: str
            The outcome: event or censoring.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        target_column: str
            The feature name that provides labels for downstream tasks.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        domain_column: Optional[str]
            Optional domain label, used for domain adaptation algorithms.
        random_state: int
            Defaults to zero.
        train_size: float
            The ratio to use for train splits.

    Example:
        >>> TODO
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: pd.DataFrame,
        time_to_event_column: str,
        target_column: str,
        time_horizons: list = [],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if target_column not in data.columns:
            raise ValueError(f"Event column {target_column} not found in the dataframe")

        if time_to_event_column not in data.columns:
            raise ValueError(
                f"Time to event column {time_to_event_column} not found in the dataframe"
            )

        T = data[time_to_event_column]
        data_filtered = data[T > 0]
        row_diff = data.shape[0] - data_filtered.shape[0]
        if row_diff > 0:
            raise ValueError(
                f"The time_to_event_column contains {row_diff} values less than or equal to zero. Please remove them."
            )

        if len(time_horizons) == 0:
            time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1].tolist()

        data.columns = data.columns.astype(str)

        self.target_column = target_column
        self.time_to_event_column = time_to_event_column
        self.time_horizons = time_horizons
        self.fairness_column = fairness_column

        super().__init__(
            data_type="survival_analysis",
            data=data,
            static_features=list(data.columns.astype(str)),
            sensitive_features=sensitive_features,
            important_features=important_features,
            outcome_features=[self.target_column],
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape

    @property
    def columns(self) -> list:
        return list(self.data.columns)

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def compression_protected_features(self) -> list:
        out = [self.target_column, self.time_to_event_column]
        domain = self.domain()

        if domain is not None:
            out.append(domain)

        return out

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        X = self.data.drop(columns=[self.target_column, self.time_to_event_column])
        T = self.data[self.time_to_event_column]
        E = self.data[self.target_column]

        if as_numpy:
            return np.asarray(X), np.asarray(T), np.asarray(E)

        return X, T, E

    def dataframe(self) -> pd.DataFrame:
        return self.data

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": list(self.static_features),
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "outcome_features": self.outcome_features,
            "target_column": self.target_column,
            "fairness_column": self.fairness_column,
            "time_to_event_column": self.time_to_event_column,
            "time_horizons": self.time_horizons,
            "train_size": self.train_size,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return SurvivalAnalysisDataLoader(
            data,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            target_column=self.target_column,
            fairness_column=self.fairness_column,
            time_to_event_column=self.time_to_event_column,
            time_horizons=self.time_horizons,
            random_state=self.random_state,
            train_size=self.train_size,
        )

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self.data)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.decorate(
            constraints.match(self.data),
        )

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        return self.decorate(
            self.data.sample(count, random_state=random_state),
        )

    def drop(self, columns: list = []) -> "DataLoader":
        return self.decorate(
            self.data.drop(columns=columns),
        )

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        if not isinstance(data, pd.DataFrame):
            raise ValueError(f"Invalid data type {type(data)}")

        return SurvivalAnalysisDataLoader(
            data,
            target_column=info["target_column"],
            time_to_event_column=info["time_to_event_column"],
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            time_horizons=info["time_horizons"],
            fairness_column=info["fairness_column"],
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data[feature] = val

    def train(self) -> "DataLoader":
        stratify = self.data[self.target_column]
        train_data, _ = train_test_split(
            self.data, train_size=self.train_size, random_state=0, stratify=stratify
        )
        return self.decorate(
            train_data.reset_index(drop=True),
        )

    def test(self) -> "DataLoader":
        stratify = self.data[self.target_column]
        _, test_data = train_test_split(
            self.data,
            train_size=self.train_size,
            random_state=0,
            stratify=stratify,
        )
        return self.decorate(
            test_data.reset_index(drop=True),
        )

    def fillna(self, value: Any) -> "DataLoader":
        self.data = self.data.fillna(value)
        return self

    def is_tabular(self) -> bool:
        return True


class TimeSeriesDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.TimeSeriesDataLoader
        :parts: 1

    Data Loader for Time Series Data

    Constructor Args:
        temporal data: List[pd.DataFrame]
            The temporal data. A list of pandas DataFrames
        observation times: List
            List of arrays mapping directly to index of each dataframe in temporal_data
        outcome: Optional[pd.DataFrame] = None
            pandas DataFrame thatn can be anything (eg, labels, regression outcome)
        static_data: Optional[pd.DataFrame] = None
            pandas DataFrame mapping directly to index of each dataframe in temporal_data
        sensitive_features: List[str]
            Name of sensitive features
        important_features List[str]
            Default: None. Only relevant for SurvivalGAN method
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        random_state: int
            Defaults to zero.

    Example:
        >>> TODO
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame] = None,
        static_data: Optional[pd.DataFrame] = None,
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        seq_offset: int = 0,
        **kwargs: Any,
    ) -> None:
        static_features = []
        self.outcome_features = []

        if len(temporal_data) == 0:
            raise ValueError("Empty temporal data")

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        max_window_len = max([len(t) for t in temporal_data])
        if static_data is not None:
            if len(static_data) != len(temporal_data):
                raise ValueError("Static and temporal data mismatch")
            static_features = list(static_data.columns)
        else:
            static_data = pd.DataFrame(np.zeros((len(temporal_data), 0)))

        if outcome is not None:
            if len(outcome) != len(temporal_data):
                raise ValueError("Temporal and outcome data mismatch")
            self.outcome_features = list(outcome.columns)
        else:
            outcome = pd.DataFrame(np.zeros((len(temporal_data), 0)))

        self.window_len = max_window_len
        self.fill = np.nan
        self.seq_offset = seq_offset

        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
            seq_df,
            seq_info,
        ) = TimeSeriesDataLoader.pack_raw_data(
            static_data,
            temporal_data,
            observation_times,
            outcome,
            fill=self.fill,
            seq_offset=seq_offset,
        )
        self.seq_info = seq_info
        self.fairness_column = fairness_column

        super().__init__(
            data={
                "static_data": static_data,
                "temporal_data": temporal_data,
                "observation_times": observation_times,
                "outcome": outcome,
                "seq_data": seq_df,
            },
            data_type="time_series",
            static_features=static_features,
            temporal_features=temporal_features,
            outcome_features=self.outcome_features,
            sensitive_features=sensitive_features,
            important_features=important_features,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data["seq_data"].shape

    @property
    def columns(self) -> list:
        return self.data["seq_data"].columns

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def compression_protected_features(self) -> list:
        return self.outcome_features

    @property
    def raw_columns(self) -> list:
        return self.static_features + self.temporal_features + self.outcome_features

    def dataframe(self) -> pd.DataFrame:
        return self.data["seq_data"].copy()

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        generic_info = {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": self.static_features,
            "temporal_features": self.temporal_features,
            "outcome_features": self.outcome_features,
            "outcome_len": len(self.data["outcome"].values.reshape(-1))
            / len(self.data["outcome"]),
            "window_len": self.window_len,
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "fairness_column": self.fairness_column,
            "random_state": self.random_state,
            "train_size": self.train_size,
            "fill": self.fill,
        }

        for key in self.seq_info:
            generic_info[key] = self.seq_info[key]

        return generic_info

    def __len__(self) -> int:
        return len(self.data["seq_data"])

    def decorate(self, data: Any) -> "DataLoader":
        static_data, temporal_data, observation_times, outcome = data

        return TimeSeriesDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            outcome=outcome,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            fairness_column=self.fairness_column,
            random_state=self.random_state,
            train_size=self.train_size,
            seq_offset=self.seq_offset,
        )

    def unpack_and_decorate(self, data: pd.DataFrame) -> "DataLoader":
        unpacked_data = TimeSeriesDataLoader.unpack_raw_data(
            data,
            self.info(),
        )

        return self.decorate(unpacked_data)

    def satisfies(self, constraints: Constraints) -> bool:
        seq_df = self.dataframe()

        return constraints.is_valid(seq_df)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.unpack_and_decorate(
            constraints.match(self.dataframe()),
        )

    def drop(self, columns: list = []) -> "DataLoader":
        new_data = self.data["seq_data"].drop(columns=columns)
        return self.unpack_and_decorate(new_data)

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.unpack_raw_data(
            data,
            info,
        )
        return TimeSeriesDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            outcome=outcome,
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            fairness_column=info["fairness_column"],
            fill=info["fill"],
            seq_offset=info["seq_offset"],
        )

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if pad:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_and_mask(
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )
        else:
            static_data, temporal_data, observation_times, outcome = (
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )

        if as_numpy:
            longest_observation_seq = max([len(seq) for seq in temporal_data])
            padded_temporal_data = np.zeros(
                (len(temporal_data), longest_observation_seq, 5)
            )
            mask = np.ones((len(temporal_data), longest_observation_seq, 5), dtype=bool)
            for i, arr in enumerate(temporal_data):
                padded_temporal_data[i, : arr.shape[0], :] = arr  # Copy the actual data
                mask[
                    i, : arr.shape[0], :
                ] = False  # Set mask to False where actual data is present

            masked_temporal_data = ma.masked_array(padded_temporal_data, mask)
            return (
                np.asarray(static_data),
                masked_temporal_data,  # TODO: check this works with time series benchmarks
                # masked array to handle variable length sequences
                ma.vstack(
                    [
                        ma.array(
                            np.resize(ot, longest_observation_seq),
                            mask=[True for i in range(len(ot))]
                            + [False for j in range(longest_observation_seq - len(ot))],
                        )
                        for ot in observation_times
                    ]
                ),
                np.asarray(outcome),
            )
        return (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data["seq_data"][feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data["seq_data"][feature] = val

    def ids(self) -> list:
        id_col = self.seq_info["seq_id_feature"]
        ids = self.data["seq_data"][id_col]

        return list(ids.unique())

    def filter_ids(self, ids_list: list) -> pd.DataFrame:
        seq_data = self.data["seq_data"]
        id_col = self.info()["seq_id_feature"]

        return seq_data[seq_data[id_col].isin(ids_list)]

    def train(self) -> "DataLoader":
        # TODO: stratify
        ids = self.ids()
        train_ids, _ = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
        )
        return self.unpack_and_decorate(self.filter_ids(train_ids))

    def test(self) -> "DataLoader":
        # TODO: stratify
        ids = self.ids()
        _, test_ids = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
        )
        return self.unpack_and_decorate(self.filter_ids(test_ids))

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        ids = self.ids()
        count = min(count, len(ids))
        sampled_ids = random.sample(ids, count)

        return self.unpack_and_decorate(self.filter_ids(sampled_ids))

    def fillna(self, value: Any) -> "DataLoader":
        for key in ["static_data", "outcome", "seq_data"]:
            if self.data[key] is not None:
                self.data[key] = self.data[key].fillna(value)

        for idx, item in enumerate(self.data["temporal_data"]):
            self.data["temporal_data"][idx] = self.data["temporal_data"][idx].fillna(
                value
            )

        return self

    @staticmethod
    def unique_temporal_features(temporal_data: List[pd.DataFrame]) -> List:
        temporal_features = []
        for item in temporal_data:
            temporal_features.extend(item.columns)
        return sorted(np.unique(temporal_features).tolist())

    # Padding helpers
    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_raw_features(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
    ) -> Any:
        fill = np.nan

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        for idx, item in enumerate(temporal_data):
            # handling missing features
            for col in temporal_features:
                if col not in item.columns:
                    item[col] = fill
            item = item[temporal_features]

            if list(item.columns) != list(temporal_features):
                raise RuntimeError("Invalid features for packing")

            temporal_data[idx] = item.fillna(fill)

        return static_data, temporal_data, observation_times, outcome

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_raw_data(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
    ) -> Any:
        fill = np.nan

        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.pad_raw_features(
            static_data, temporal_data, observation_times, outcome
        )
        max_window_len = max([len(t) for t in temporal_data])
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        for idx, item in enumerate(temporal_data):
            if len(item) != max_window_len:
                pads = fill * np.ones(
                    (max_window_len - len(item), len(temporal_features))
                )
                start = 0
                if len(item.index) > 0:
                    start = max(item.index) + 1
                pads_df = pd.DataFrame(
                    pads,
                    index=[start + i for i in range(len(pads))],
                    columns=item.columns,
                )
                item = pd.concat([item, pads_df])

            # handle missing time points
            if list(item.columns) != list(temporal_features):
                raise RuntimeError(
                    f"Invalid features {item.columns}. Expected {temporal_features}"
                )
            if len(item) != max_window_len:
                raise RuntimeError("Invalid window len")

            temporal_data[idx] = item

        observation_times_padded = []
        for idx, item in enumerate(observation_times):
            item = list(item)
            if len(item) != max_window_len:
                pads = fill * np.ones(max_window_len - len(item))
                item.extend(pads.tolist())
            observation_times_padded.append(item)

        return static_data, temporal_data, observation_times_padded, outcome

    # Masking helpers
    @staticmethod
    def extract_masked_features(full_temporal_features: list) -> tuple:
        temporal_features = []
        mask_features = []
        mask_prefix = "masked_"
        for feat in full_temporal_features:
            feat = str(feat)
            if not feat.startswith(mask_prefix):
                temporal_features.append(feat)
                continue

            other_feat = feat[len(mask_prefix) :]
            if other_feat in full_temporal_features:
                mask_features.append(feat)
            else:
                temporal_features.append(feat)

        return temporal_features, mask_features

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def mask_temporal_data(
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        fill: Any = 0,
    ) -> Any:
        nan_cnt = 0
        for item in temporal_data:
            nan_cnt += np.asarray(np.isnan(item)).sum()

        if nan_cnt == 0:
            return temporal_data, observation_times

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        masked_features = [f"masked_{feat}" for feat in temporal_features]

        for idx, item in enumerate(temporal_data):
            item[masked_features] = (~np.isnan(item)).astype(int)
            item = item.fillna(fill)
            temporal_data[idx] = item

        for idx, item in enumerate(observation_times):
            item = np.nan_to_num(item, nan=fill).tolist()

            observation_times[idx] = item

        return temporal_data, observation_times

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def unmask_temporal_data(
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        fill: Any = np.nan,
    ) -> Any:
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        temporal_features, mask_features = TimeSeriesDataLoader.extract_masked_features(
            temporal_features
        )

        missing_horizons = []
        for idx, item in enumerate(temporal_data):
            # handle existing mask
            if len(mask_features) > 0:
                mask = temporal_data[idx][mask_features].astype(bool)
                item[~mask] = np.nan

            item_missing_rows = item.isna().sum(axis=1).values
            missing_horizons.append(item_missing_rows == len(temporal_features))

            # TODO: review impact on horizons
            temporal_data[idx] = item.dropna()

        observation_times_unmasked = []
        for idx, item in enumerate(observation_times):
            item = list(item)

            for midx, mval in enumerate(missing_horizons[idx]):
                if mval:
                    item[midx] = np.nan

            local_horizons = list(filter(lambda v: v == v, item))
            observation_times_unmasked.append(local_horizons)

        return temporal_data, observation_times_unmasked

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_and_mask(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        only_features: Any = False,
        fill: Any = 0,
    ) -> Any:
        if only_features:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_raw_features(
                static_data,
                temporal_data,
                observation_times,
                outcome,
            )
        else:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_raw_data(
                static_data,
                temporal_data,
                observation_times,
                outcome,
            )

        temporal_data, observation_times = TimeSeriesDataLoader.mask_temporal_data(
            temporal_data, observation_times, fill=fill
        )

        return static_data, temporal_data, observation_times, outcome

    @staticmethod
    def sequential_view(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        id_col: str = "seq_id",
        time_id_col: str = "seq_time_id",
        seq_offset: int = 0,
    ) -> Tuple[pd.DataFrame, dict]:  # sequential dataframe, loader info
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.pad_and_mask(
            static_data, temporal_data, observation_times, outcome, only_features=True
        )
        raw_static_features = list(static_data.columns)
        static_features = [f"seq_static_{col}" for col in raw_static_features]

        raw_outcome_features = list(outcome.columns)
        outcome_features = [f"seq_out_{col}" for col in raw_outcome_features]

        raw_temporal_features = TimeSeriesDataLoader.unique_temporal_features(
            temporal_data
        )
        temporal_features = [f"seq_temporal_{col}" for col in raw_temporal_features]
        cols = (
            [id_col, time_id_col]
            + static_features
            + temporal_features
            + outcome_features
        )

        seq = []
        for sidx, static_item in static_data.iterrows():
            real_tidx = 0
            for tidx, temporal_item in temporal_data[sidx].iterrows():
                local_seq_data = (
                    [
                        sidx + seq_offset,
                        observation_times[sidx][real_tidx],
                    ]
                    + static_item[raw_static_features].values.tolist()
                    + temporal_item[raw_temporal_features].values.tolist()
                    + outcome.loc[sidx, raw_outcome_features].values.tolist()
                )
                seq.append(local_seq_data)
                real_tidx += 1

        seq_df = pd.DataFrame(seq, columns=cols)
        info = {
            "seq_static_features": static_features,
            "seq_temporal_features": temporal_features,
            "seq_outcome_features": outcome_features,
            "seq_offset": seq_offset,
            "seq_id_feature": id_col,
            "seq_time_id_feature": time_id_col,
            "seq_features": list(seq_df.columns),
        }
        return seq_df, info

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pack_raw_data(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        fill: Any = np.nan,
        seq_offset: int = 0,
    ) -> pd.DataFrame:
        # Temporal data: (subjects, temporal_sequence, temporal_feature)
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        temporal_features, mask_features = TimeSeriesDataLoader.extract_masked_features(
            temporal_features
        )
        temporal_data, observation_times = TimeSeriesDataLoader.unmask_temporal_data(
            temporal_data, observation_times
        )
        seq_df, info = TimeSeriesDataLoader.sequential_view(
            static_data=static_data,
            temporal_data=temporal_data,
            observation_times=observation_times,
            outcome=outcome,
            seq_offset=seq_offset,
        )

        return static_data, temporal_data, observation_times, outcome, seq_df, info

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def unpack_raw_data(
        data: pd.DataFrame,
        info: dict,
    ) -> Tuple[
        Optional[pd.DataFrame], List[pd.DataFrame], List, Optional[pd.DataFrame]
    ]:
        id_col = info["seq_id_feature"]
        time_col = info["seq_time_id_feature"]

        static_cols = info["seq_static_features"]
        new_static_cols = [feat.split("seq_static_")[1] for feat in static_cols]

        temporal_cols = info["seq_temporal_features"]
        new_temporal_cols = [feat.split("seq_temporal_")[1] for feat in temporal_cols]

        outcome_cols = info["seq_outcome_features"]
        new_outcome_cols = [feat.split("seq_out_")[1] for feat in outcome_cols]

        ids = sorted(list(set(data[id_col])))

        static_data = []
        temporal_data = []
        observation_times = []
        outcome_data = []

        for item_id in ids:
            item_data = data[data[id_col] == item_id]

            static_data.append(item_data[static_cols].head(1).values.squeeze().tolist())
            outcome_data.append(
                item_data[outcome_cols].head(1).values.squeeze().tolist()
            )
            local_temporal_data = item_data[temporal_cols].copy()
            local_observation_times = item_data[time_col].values.tolist()
            local_temporal_data.columns = new_temporal_cols
            # TODO: review impact on horizons
            local_temporal_data = local_temporal_data.dropna()

            temporal_data.append(local_temporal_data)
            observation_times.append(local_observation_times)

        static_df = pd.DataFrame(static_data, columns=new_static_cols)
        outcome_df = pd.DataFrame(outcome_data, columns=new_outcome_cols)

        return static_df, temporal_data, observation_times, outcome_df

    def is_tabular(self) -> bool:
        return True


class TimeSeriesSurvivalDataLoader(TimeSeriesDataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.TimeSeriesSurvivalDataLoader
        :parts: 1

    Data loader for Time series survival data

    Constructor Args:
        temporal_data: List[pd.DataFrame}
            The temporal data. A list of pandas DataFrames.
        observation_times: List
            List of arrays mapping directly to index of each dataframe in temporal_data
        T: Union[pd.Series, np.ndarray, pd.Series]
            Time-to-event data
        E: Union[pd.Series, np.ndarray, pd.Series]
            E is censored/event data
        static_data Optional[pd.DataFrame] = None
            pandas DataFrame of static features for each subject
        sensitive_features: List[str]
            Name of sensitive features
        important_features: List[str}
            Default: None. Only relevant for SurvivalGAN method.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        random_state. int
            Defaults to zero.

    Example:
        >>> TODO

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: Union[List, np.ndarray, pd.Series],
        T: Union[pd.Series, np.ndarray, pd.Series],
        E: Union[pd.Series, np.ndarray, pd.Series],
        static_data: Optional[pd.DataFrame] = None,
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        time_horizons: list = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        seq_offset: int = 0,
        **kwargs: Any,
    ) -> None:
        self.time_to_event_col = "time_to_event"
        self.event_col = "event"
        self.fairness_column = fairness_column

        if len(time_horizons) == 0:
            time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1].tolist()
        self.time_horizons = time_horizons
        outcome = pd.concat([pd.Series(T), pd.Series(E)], axis=1)
        outcome.columns = [self.time_to_event_col, self.event_col]

        self.fill = np.nan

        super().__init__(
            temporal_data=temporal_data,
            observation_times=observation_times,
            outcome=outcome,
            static_data=static_data,
            sensitive_features=sensitive_features,
            important_features=important_features,
            random_state=random_state,
            train_size=train_size,
            seq_offset=seq_offset,
            **kwargs,
        )
        self.data_type = "time_series_survival"

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def info(self) -> dict:
        parent_info = super().info()
        parent_info["time_to_event_column"] = self.time_to_event_col
        parent_info["event_column"] = self.event_col
        parent_info["time_horizons"] = self.time_horizons
        parent_info["fill"] = self.fill

        return parent_info

    def decorate(self, data: Any) -> "DataLoader":
        static_data, temporal_data, observation_times, outcome = data
        if self.time_to_event_col not in outcome:
            raise ValueError(
                f"Survival outcome is missing tte column {self.time_to_event_col}"
            )
        if self.event_col not in outcome:
            raise ValueError(
                f"Survival outcome is missing event column {self.event_col}"
            )

        return TimeSeriesSurvivalDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            T=outcome[self.time_to_event_col],
            E=outcome[self.event_col],
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            fairness_column=self.fairness_column,
            random_state=self.random_state,
            time_horizons=self.time_horizons,
            train_size=self.train_size,
            seq_offset=self.seq_offset,
        )

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesSurvivalDataLoader.unpack_raw_data(
            data,
            info,
        )
        return TimeSeriesSurvivalDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            T=outcome[info["time_to_event_column"]],
            E=outcome[info["event_column"]],
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            fairness_column=info["fairness_column"],
            time_horizons=info["time_horizons"],
            seq_offset=info["seq_offset"],
        )

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if pad:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesSurvivalDataLoader.pad_and_mask(
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )
        else:
            static_data, temporal_data, observation_times, outcome = (
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )

        if as_numpy:
            return (
                np.asarray(static_data),
                np.asarray(temporal_data, dtype=object),
                np.asarray(observation_times, dtype=object),
                np.asarray(outcome[self.time_to_event_col]),
                np.asarray(outcome[self.event_col]),
            )
        return (
            static_data,
            temporal_data,
            observation_times,
            outcome[self.time_to_event_col],
            outcome[self.event_col],
        )

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.unpack_and_decorate(
            constraints.match(self.dataframe()),
        )

    def train(self) -> "DataLoader":
        stratify = self.data["outcome"][self.event_col]

        ids = self.ids()
        train_ids, _ = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )
        return self.unpack_and_decorate(self.filter_ids(train_ids))

    def test(self) -> "DataLoader":
        stratify = self.data["outcome"][self.event_col]
        ids = self.ids()
        _, test_ids = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )
        return self.unpack_and_decorate(self.filter_ids(test_ids))


class ImageDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.ImageDataLoader
        :parts: 1

    Data loader for generic image data.

    Constructor Args:
        data: torch.utils.data.Dataset or torch.Tensor
            The image dataset or a tuple of (tensor images, tensor labels)
        random_state: int
            Defaults to zero.
        height: int. Default = 32
            Height to use internally
        width: Optional[int]
            Optional width to use internally. If None, it is used the same value as height.
        train_size: float = 0.8
            Train dataset ratio.
    Example:
        >>> dataset = datasets.MNIST(".", download=True)
        >>>
        >>> loader = ImageDataLoader(
        >>>     data=dataset,
        >>>     train_size=0.8,
        >>>     height=32,
        >>>     width=w32,
        >>> )

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: Union[torch.utils.data.Dataset, Tuple[torch.Tensor, torch.Tensor]],
        height: int = 32,
        width: Optional[int] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if width is None:
            width = height

        if isinstance(data, tuple):
            X, y = data
            data = TensorDataset(images=X, targets=y)

        self.data_transform = None

        dummy, _ = data[0]
        img_transform = []
        if not isinstance(dummy, PIL.Image.Image):
            img_transform = [transforms.ToPILImage()]

        img_transform.extend(
            [
                transforms.Resize((height, width)),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.5,), std=(0.5,)),
            ]
        )

        self.data_transform = transforms.Compose(img_transform)
        data = FlexibleDataset(data, transform=self.data_transform)

        self.height = height
        self.width = width
        self.channels = data.shape()[1]

        super().__init__(
            data_type="images",
            data=data,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape()

    def get_fairness_column(self) -> None:
        """Not implemented for ImageDataLoader"""
        ...

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        return self.data

    def numpy(self) -> np.ndarray:
        x, _ = self.data.numpy()

        return x

    def dataframe(self) -> pd.DataFrame:
        x = self.numpy().reshape(len(self), -1)

        x = pd.DataFrame(x)
        x.columns = x.columns.astype(str)

        return x

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "train_size": self.train_size,
            "height": self.height,
            "width": self.width,
            "channels": self.channels,
            "random_state": self.random_state,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return ImageDataLoader(
            data,
            random_state=self.random_state,
            train_size=self.train_size,
            height=self.height,
            width=self.width,
        )

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        idxs = np.random.choice(len(self), count, replace=False)
        subset = FlexibleDataset(self.data.data, indices=idxs)
        return self.decorate(subset)

    @staticmethod
    def from_info(data: torch.utils.data.Dataset, info: dict) -> "ImageDataLoader":
        if not isinstance(data, torch.utils.data.Dataset):
            raise ValueError(f"Invalid data type {type(data)}")

        return ImageDataLoader(
            data,
            train_size=info["train_size"],
            height=info["height"],
            width=info["width"],
            random_state=info["random_state"],
        )

    def __getitem__(self, index: Union[list, int, str]) -> Any:
        if isinstance(index, str):
            return self.dataframe()[index]

        return self.numpy()[index]

    def _train_test_split(self) -> Tuple:
        indices = np.arange(len(self.data))
        _, stratify = self.data.numpy()

        return train_test_split(
            indices,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )

    def train(self) -> "DataLoader":
        train_idx, _ = self._train_test_split()
        subset = FlexibleDataset(self.data.data, indices=train_idx)
        return self.decorate(subset)

    def test(self) -> "DataLoader":
        _, test_idx = self._train_test_split()
        subset = FlexibleDataset(self.data.data, indices=test_idx)
        return self.decorate(subset)

    def compress(
        self,
    ) -> Tuple["DataLoader", Dict]:
        return self, {}

    def decompress(self, context: Dict) -> "DataLoader":
        return self

    def encode(
        self,
        encoders: Optional[Dict[str, Any]] = None,
    ) -> Tuple["DataLoader", Dict]:
        return self, {}

    def decode(
        self,
        encoders: Dict[str, Any],
    ) -> "DataLoader":
        return self

    def is_tabular(self) -> bool:
        return False

    @property
    def columns(self) -> list:
        return list(self.dataframe().columns)

    def satisfies(self, constraints: Constraints) -> bool:
        return True

    def match(self, constraints: Constraints) -> "DataLoader":
        return self

    def compression_protected_features(self) -> list:
        raise NotImplementedError("Images do not support the compression call")

    def drop(self, columns: list = []) -> "DataLoader":
        raise NotImplementedError()

    def __setitem__(self, feature: str, val: Any) -> None:
        raise NotImplementedError()

    def fillna(self, value: Any) -> "DataLoader":
        raise NotImplementedError()


@validate_arguments(config=dict(arbitrary_types_allowed=True))
def create_from_info(
    data: Union[pd.DataFrame, torch.utils.data.Dataset], info: dict
) -> "DataLoader":
    """Helper for creating a DataLoader from existing information."""
    if info["data_type"] == "generic":
        return GenericDataLoader.from_info(data, info)
    elif info["data_type"] == "survival_analysis":
        return SurvivalAnalysisDataLoader.from_info(data, info)
    elif info["data_type"] == "time_series":
        return TimeSeriesDataLoader.from_info(data, info)
    elif info["data_type"] == "time_series_survival":
        return TimeSeriesSurvivalDataLoader.from_info(data, info)
    elif info["data_type"] == "images":
        return ImageDataLoader.from_info(data, info)
    else:
        raise RuntimeError(f"invalid datatype {info}")


class Syn_SeqDataLoader(DataLoader):
    """
    A DataLoader that applies Syn_Seq-style preprocessing to input data,
    and optionally allows user_custom (syn_order, variable_selection, method) updates
    via update_user_custom(...).
    """

    def __init__(
        self,
        data: pd.DataFrame,
        syn_order: Optional[List[str]] = None,
        special_value: Optional[Dict[str, List[Any]]] = None,
        col_type: Optional[Dict[str, str]] = None,
        max_categories: int = 20,
        random_state: int = 0,
        train_size: float = 0.8,
        verbose: bool = True,
        **kwargs: Any,
    ) -> None:
        if not syn_order and verbose:
            print("[INFO] Most of the time, it is recommened to have category variables before synthesizing numeric variables.")
        syn_order = syn_order or list(data.columns)

        missing_columns = set(syn_order) - set(data.columns)
        if missing_columns:
            raise ValueError(f"Missing columns in input data: {missing_columns}")

        self.syn_order = syn_order
        self.columns_special_values = special_value or {}
        self.col_type = col_type or {}
        self.max_categories = max_categories
        self._verbose = verbose

        # 재배치된 DataFrame
        self._df = data[self.syn_order].copy()

        # 부모 DataLoader 생성자
        super().__init__(
            data_type="syn_seq",
            data=self._df,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

        # ---- encoder 생성 + fit ----
        self._encoder = Syn_SeqEncoder(
            columns_special_values=self.columns_special_values,
            syn_order=self.syn_order,
            max_categories=self.max_categories,
            col_type=self.col_type,
        )
        self._encoder.fit(self._df)

        if self._verbose:
            print("[INFO] Syn_SeqDataLoader init complete:")
            print(f"  - syn_order: {self.syn_order}")
            print(f"  - special_value: {self.columns_special_values}")
            print(f"  - col_type: {self.col_type}")
            print(f"  - data shape: {self._df.shape}")

            print("[DEBUG] After encoder.fit(), detected info:")

            # 1) encoder.col_map (각 컬럼: {original_dtype, converted_type, method})
            if hasattr(self._encoder, "col_map"):
                print("  - encoder.col_map =>")
                for col_name, cinfo in self._encoder.col_map.items():
                    print(f"       {col_name} : {cinfo}")

            # 2) variable_selection_ (있으면 출력)
            if self._encoder.variable_selection_ is not None:
                print("  - variable_selection_:\n", self._encoder.variable_selection_)

            # date_mins 등 다른 필드를 보고 싶다면 아래처럼
            if hasattr(self._encoder, "date_mins"):
                print(f"  - date_mins: {self._encoder.date_mins}")

            print("----------------------------------------------------------------")

    # ----------------------------------------------------------------
    # Inherited/required abstract methods
    # ----------------------------------------------------------------
    @property
    def shape(self) -> tuple:
        return self._df.shape

    @property
    def columns(self) -> list:
        return list(self._df.columns)

    def dataframe(self) -> pd.DataFrame:
        return self._df

    def numpy(self) -> pd.DataFrame:
        return self._df.values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "train_size": self.train_size,
            "random_state": self.random_state,
            "syn_order": self.syn_order,
            "max_categories": self.max_categories,
            "col_type": self.col_type,
            "columns_special_values": self.columns_special_values,
        }

    def __len__(self) -> int:
        return len(self._df)

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self._df)

    def match(self, constraints: Constraints) -> "Syn_SeqDataLoader":
        matched_df = constraints.match(self._df)
        return self.decorate(matched_df)

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "Syn_SeqDataLoader":
        return Syn_SeqDataLoader(
            data=data,
            syn_order=info.get("syn_order"),
            special_value=info.get("columns_special_values", {}),
            col_type=info.get("col_type", {}),
            max_categories=info.get("max_categories", 20),
            random_state=info["random_state"],
            train_size=info["train_size"],
        )

    def sample(self, count: int, random_state: int = 0) -> "Syn_SeqDataLoader":
        sampled_df = self._df.sample(count, random_state=random_state)
        return self.decorate(sampled_df)

    def drop(self, columns: list = []) -> "Syn_SeqDataLoader":
        dropped_df = self._df.drop(columns=columns, errors="ignore")
        return self.decorate(dropped_df)

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self._df[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self._df[feature] = val

    def train(self) -> "Syn_SeqDataLoader":
        ntrain = int(len(self._df) * self.train_size)
        train_df = self._df.iloc[:ntrain].copy()
        return self.decorate(train_df)

    def test(self) -> "Syn_SeqDataLoader":
        ntrain = int(len(self._df) * self.train_size)
        test_df = self._df.iloc[ntrain:].copy()
        return self.decorate(test_df)

    def fillna(self, value: Any) -> "Syn_SeqDataLoader":
        filled_df = self._df.fillna(value)
        return self.decorate(filled_df)

    def compression_protected_features(self) -> list:
        return []

    def is_tabular(self) -> bool:
        return True

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if as_numpy:
            return self._df.to_numpy()
        return self._df

    def get_fairness_column(self) -> Union[str, Any]:
        return None

    # ----------------------------------------------------------------
    # encode/decode
    # ----------------------------------------------------------------
    def encode(
        self, encoders: Optional[Dict[str, Any]] = None
    ) -> Tuple["Syn_SeqDataLoader", Dict]:
        """
        Typically used by aggregator's fit step.
        """
        if encoders is None:
            encoded_data = self._encoder.transform(self._df)
            new_loader = self.decorate(encoded_data)
            return new_loader, {"syn_seq_encoder": self._encoder}
        else:
            return self, encoders

    def decode(self, encoders: Dict[str, Any]) -> "Syn_SeqDataLoader":
        if "syn_seq_encoder" in encoders:
            encoder = encoders["syn_seq_encoder"]
            if not isinstance(encoder, Syn_SeqEncoder):
                raise TypeError(f"Expected Syn_SeqEncoder, got {type(encoder)}")
            decoded_data = encoder.inverse_transform(self._df)
            return self.decorate(decoded_data)
        return self

    def decorate(self, data: pd.DataFrame) -> "Syn_SeqDataLoader":
        """
        Helper for creating a new instance with the same settings but new data.
        """
        new_loader = Syn_SeqDataLoader(
            data=data,
            syn_order=self.syn_order,
            special_value=self.columns_special_values,
            col_type=self.col_type,
            max_categories=self.max_categories,
            random_state=self.random_state,
            train_size=self.train_size,
            verbose=self._verbose,
        )
        return new_loader
    
    # ----------------------------------------------------------------
    # RE-INTRODUCE update_user_custom(...)
    # ----------------------------------------------------------------
    def update_user_custom(self, user_custom: Dict[str, Any]) -> "Syn_SeqDataLoader":
        """
        Allows user to update certain aspects (syn_order, variable_selection, method),
        without calling transform() here.
        The aggregator (or user) will later call self.encode() => encoder.transform() once.
        """

        # 1) syn_order
        if "syn_order" in user_custom:
            new_order = user_custom["syn_order"]
            self.syn_order = new_order
            self._encoder.syn_order = new_order

        # 2) variable_selection
        if "variable_selection" in user_custom:
            vs_val = user_custom["variable_selection"]
            if isinstance(vs_val, dict):
                current_vs = self._encoder.variable_selection_
                if current_vs is None:
                    # 만약 encoder.variable_selection_이 아직 None이면,
                    # syn_order 크기에 맞춰 생성
                    all_cols = self.syn_order
                    current_vs = pd.DataFrame(0, index=all_cols, columns=all_cols)
                updated_vs = self._encoder.update_variable_selection(current_vs, vs_val)
                self._encoder.variable_selection_ = updated_vs
            elif isinstance(vs_val, pd.DataFrame):
                # 직접 대입
                self._encoder.variable_selection_ = vs_val.copy()
            else:
                print(f"[WARNING] variable_selection must be dict or DataFrame, got {type(vs_val)}")

        # 3) method
        if "method" in user_custom:
            # 여기서는 데이터로더가 직접 method 정보를 쓰지 않을 수도 있음
            # aggregator fit 시점에 활용하도록, 일단 저장만
            self._method = user_custom["method"]

        # (중요) transform/decorate 호출 제거 → “한 번만 변환” 원칙
        # => 아래 두 줄(이전 버전의 step 4,5)은 제거/주석 처리
        #
        # encoded_df = self._encoder.transform(self._df)
        # updated_loader = self.decorate(encoded_df)
        # self.__dict__.update(updated_loader.__dict__)

        # 대신 자기 자신 그대로 반환
        return self



src/synthcity/plugins/generic/plugin_syn_seq.py
# File: plugin_syn_seq.py

from typing import Any, Dict, List, Optional, Union

import pandas as pd
import numpy as np

# synthcity absolute
from synthcity.plugins.core.plugin import Plugin
from synthcity.plugins.core.dataloader import DataLoader, Syn_SeqDataLoader
from synthcity.plugins.core.schema import Schema
from synthcity.plugins.core.constraints import Constraints

# local aggregator
from synthcity.plugins.core.models.syn_seq.syn_seq import Syn_Seq


class Syn_SeqPlugin(Plugin):
    """
    A plugin for a sequential (column-by-column) synthetic data approach,
    mirroring R's 'synthpop'. Internally, it delegates to the `Syn_Seq`
    aggregator from syn_seq.py for the actual column-by-column (sequential) logic.

    - This is intended for sequential regression style: each column is modeled
      in the order, using prior columns as predictors.
    - We rely on the custom `Syn_SeqDataLoader`, which organizes columns,
      applies a Syn_SeqEncoder, etc.
    - The aggregator's generate(...) returns a Syn_SeqDataLoader as well.

    Basic usage:

        # Suppose we have df, and we wrap it in a Syn_SeqDataLoader:
        loader = Syn_SeqDataLoader(
            data = df, 
            syn_order = [...],
            col_type = {...},
            special_value = {...},
        )

        # Build plugin
        syn_model = Syn_SeqPlugin(
            random_state=42,
            default_first_method="SWR", 
            default_other_method="CART",
            # optionally strict=True, sampling_patience=500, ...
        )

        # Fit with per-column method
        methods = ["SWR"] + ["CART"]*(len(loader.columns)-1)
        var_sel = {"N2": ["C1","C2"], "N1":["C1","C2","N2"]}

        syn_model.fit(loader, method=methods, variable_selection=var_sel)

        # Now generate
        synthetic_data_loader = syn_model.generate(count=100)
        synthetic_df = synthetic_data_loader.dataframe()

        # If you need constraints:
        constraints = {
          "N1": [">", 100],
          "C2": ["in", ["A","B"]]
        }
        synthetic_data_loader = syn_model.generate(count=100, constraints=constraints)
        # Above remains a Syn_SeqDataLoader
    """

    @staticmethod
    def name() -> str:
        return "syn_seq"

    @staticmethod
    def type() -> str:
        return "syn_seq"

    @staticmethod
    def hyperparameter_space(**kwargs: Any) -> list:
        # No tunable hyperparameters for now
        return []

    def __init__(
        self,
        random_state: int = 0,
        default_first_method: str = "SWR",
        default_other_method: str = "CART",
        **kwargs: Any,
    ):
        """
        Args:
            random_state: For reproducibility.
            default_first_method: fallback method for the first column if user doesn't override.
            default_other_method: fallback method for subsequent columns.
            **kwargs: forwarded to Plugin(...) => can contain strict, sampling_patience, etc.
        """
        super().__init__(random_state=random_state, **kwargs)

        # We hold a Syn_Seq aggregator. This aggregator does the real sequential logic:
        self._aggregator = Syn_Seq(
            random_state=random_state,
            default_first_method=default_first_method,
            default_other_method=default_other_method,
            strict=self.strict,
            sampling_patience=self.sampling_patience,
        )
        self._model_trained = False

    def _fit(
        self,
        X: DataLoader,
        method: Optional[List[str]] = None,
        variable_selection: Optional[Dict[str, List[str]]] = None,
        *args: Any,
        **kwargs: Any
    ) -> "Syn_SeqPlugin":
        """
        We expect X to be a Syn_SeqDataLoader for sequential usage.
        We pass 'method' and 'variable_selection' to the aggregator.
        """
        if not isinstance(X, Syn_SeqDataLoader):
            raise TypeError("Syn_SeqPlugin expects a Syn_SeqDataLoader for sequential usage.")

        self._aggregator.fit(
            loader=X,
            method=method,
            variable_selection=variable_selection,
            *args,
            **kwargs
        )
        self._model_trained = True
        return self

    def _generate(
        self,
        count: int,
        syn_schema: Schema,
        constraints: Optional[Constraints] = None,
        **kwargs: Any
    ) -> DataLoader:
        """
        Create synthetic data as a Syn_SeqDataLoader. We pass optional constraints
        (which might be a dictionary or a Constraints object) to aggregator.
        """
        if not self._model_trained:
            raise RuntimeError("Must fit Syn_SeqPlugin before generating data.")

        return self._aggregator.generate(
            count=count,
            constraint=constraints,
            **kwargs
        )

plugin = Syn_SeqPlugin

TREE
(base) minkeychang@Minkeys-Laptop synthcity % tree
.
├── CONTRIBUTING.MD
├── INSTRUCTION.txt
├── LICENSE
├── README.md
├── code_collector.py
├── code_collector2.py
├── docs
│   ├── Makefile
│   ├── README.md
│   ├── _templates
│   │   └── module.rst_t
│   ├── advanced.rst
│   ├── arch.png
│   ├── conf.py
│   ├── dataloaders.rst
│   ├── examples.rst
│   ├── generators.rst
│   ├── index.rst
│   ├── logo.png
│   ├── logo_text.png
│   ├── make.bat
│   ├── metrics.rst
│   ├── requirements.txt
│   └── tutorials -> ../tutorials/\012
├── prereq.txt
├── prompt.txt
├── prompt2.txt
├── pyproject.toml
├── setup.cfg
├── setup.py
├── src
│   └── synthcity
│       ├── __init__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-39.pyc
│       │   ├── logger.cpython-39.pyc
│       │   └── version.cpython-39.pyc
│       ├── benchmark
│       │   ├── __init__.py
│       │   └── utils.py
│       ├── logger.py
│       ├── metrics
│       │   ├── __init__.py
│       │   ├── __pycache__
│       │   │   ├── __init__.cpython-39.pyc
│       │   │   ├── _utils.cpython-39.pyc
│       │   │   ├── eval.cpython-39.pyc
│       │   │   ├── eval_detection.cpython-39.pyc
│       │   │   ├── eval_performance.cpython-39.pyc
│       │   │   ├── eval_privacy.cpython-39.pyc
│       │   │   ├── eval_sanity.cpython-39.pyc
│       │   │   ├── eval_statistical.cpython-39.pyc
│       │   │   ├── plots.cpython-39.pyc
│       │   │   ├── scores.cpython-39.pyc
│       │   │   └── weighted_metrics.cpython-39.pyc
│       │   ├── _utils.py
│       │   ├── core
│       │   │   ├── __init__.py
│       │   │   ├── __pycache__
│       │   │   │   ├── __init__.cpython-39.pyc
│       │   │   │   └── metric.cpython-39.pyc
│       │   │   └── metric.py
│       │   ├── eval.py
│       │   ├── eval_attacks.py
│       │   ├── eval_detection.py
│       │   ├── eval_performance.py
│       │   ├── eval_privacy.py
│       │   ├── eval_sanity.py
│       │   ├── eval_statistical.py
│       │   ├── plots.py
│       │   ├── representations
│       │   │   ├── OneClass.py
│       │   │   ├── __init__.py
│       │   │   ├── __pycache__
│       │   │   │   ├── OneClass.cpython-39.pyc
│       │   │   │   ├── __init__.cpython-39.pyc
│       │   │   │   └── networks.cpython-39.pyc
│       │   │   └── networks.py
│       │   ├── scores.py
│       │   └── weighted_metrics.py
│       ├── plugins
│       │   ├── __init__.py
│       │   ├── __pycache__
│       │   │   └── __init__.cpython-39.pyc
│       │   ├── core
│       │   │   ├── __init__.py
│       │   │   ├── __pycache__
│       │   │   │   ├── __init__.cpython-39.pyc
│       │   │   │   ├── constraints.cpython-39.pyc
│       │   │   │   ├── dataloader.cpython-39.pyc
│       │   │   │   ├── dataset.cpython-39.pyc
│       │   │   │   ├── distribution.cpython-39.pyc
│       │   │   │   ├── plugin.cpython-39.pyc
│       │   │   │   ├── schema.cpython-39.pyc
│       │   │   │   └── serializable.cpython-39.pyc
│       │   │   ├── constraints.py
│       │   │   ├── dataloader.py
│       │   │   ├── dataset.py
│       │   │   ├── distribution.py
│       │   │   ├── models
│       │   │   │   ├── RGCNConv.py
│       │   │   │   ├── __init__.py
│       │   │   │   ├── __pycache__
│       │   │   │   │   ├── __init__.cpython-39.pyc
│       │   │   │   │   ├── bnaf.cpython-39.pyc
│       │   │   │   │   ├── convnet.cpython-39.pyc
│       │   │   │   │   ├── factory.cpython-39.pyc
│       │   │   │   │   ├── feature_encoder.cpython-39.pyc
│       │   │   │   │   ├── functions.cpython-39.pyc
│       │   │   │   │   ├── layers.cpython-39.pyc
│       │   │   │   │   ├── mlp.cpython-39.pyc
│       │   │   │   │   ├── tabular_encoder.cpython-39.pyc
│       │   │   │   │   ├── transformer.cpython-39.pyc
│       │   │   │   │   └── ts_model.cpython-39.pyc
│       │   │   │   ├── aim.py
│       │   │   │   ├── bnaf.py
│       │   │   │   ├── convnet.py
│       │   │   │   ├── dag
│       │   │   │   │   ├── __init__.py
│       │   │   │   │   ├── data.py
│       │   │   │   │   ├── dsl.py
│       │   │   │   │   ├── dstruct.py
│       │   │   │   │   └── utils.py
│       │   │   │   ├── factory.py
│       │   │   │   ├── feature_encoder.py
│       │   │   │   ├── flows.py
│       │   │   │   ├── functions.py
│       │   │   │   ├── gan.py
│       │   │   │   ├── goggle.py
│       │   │   │   ├── image_gan.py
│       │   │   │   ├── layers.py
│       │   │   │   ├── mbi
│       │   │   │   │   ├── __init__.py
│       │   │   │   │   ├── callbacks.py
│       │   │   │   │   ├── clique_vector.py
│       │   │   │   │   ├── dataset.py
│       │   │   │   │   ├── domain.py
│       │   │   │   │   ├── factor.py
│       │   │   │   │   ├── factor_graph.py
│       │   │   │   │   ├── graphical_model.py
│       │   │   │   │   ├── identity.py
│       │   │   │   │   ├── inference.py
│       │   │   │   │   ├── junction_tree.py
│       │   │   │   │   ├── local_inference.py
│       │   │   │   │   ├── mechanism.py
│       │   │   │   │   ├── mixture_inference.py
│       │   │   │   │   ├── public_inference.py
│       │   │   │   │   ├── region_graph.py
│       │   │   │   │   └── torch_factor.py
│       │   │   │   ├── mlp.py
│       │   │   │   ├── survival_analysis
│       │   │   │   │   ├── __init__.py
│       │   │   │   │   ├── __pycache__
│       │   │   │   │   │   ├── __init__.cpython-39.pyc
│       │   │   │   │   │   ├── _base.cpython-39.pyc
│       │   │   │   │   │   ├── benchmarks.cpython-39.pyc
│       │   │   │   │   │   ├── loader.cpython-39.pyc
│       │   │   │   │   │   ├── metrics.cpython-39.pyc
│       │   │   │   │   │   ├── surv_aft.cpython-39.pyc
│       │   │   │   │   │   ├── surv_coxph.cpython-39.pyc
│       │   │   │   │   │   ├── surv_deephit.cpython-39.pyc
│       │   │   │   │   │   └── surv_xgb.cpython-39.pyc
│       │   │   │   │   ├── _base.py
│       │   │   │   │   ├── benchmarks.py
│       │   │   │   │   ├── loader.py
│       │   │   │   │   ├── metrics.py
│       │   │   │   │   ├── surv_aft.py
│       │   │   │   │   ├── surv_coxph.py
│       │   │   │   │   ├── surv_deephit.py
│       │   │   │   │   ├── surv_xgb.py
│       │   │   │   │   └── third_party
│       │   │   │   │       ├── __init__.py
│       │   │   │   │       ├── __pycache__
│       │   │   │   │       │   ├── __init__.cpython-39.pyc
│       │   │   │   │       │   ├── metrics.cpython-39.pyc
│       │   │   │   │       │   ├── nonparametric.cpython-39.pyc
│       │   │   │   │       │   └── util.cpython-39.pyc
│       │   │   │   │       ├── metrics.py
│       │   │   │   │       ├── nonparametric.py
│       │   │   │   │       └── util.py
│       │   │   │   ├── syn_seq
│       │   │   │   │   ├── __init__.py
│       │   │   │   │   ├── __pycache__
│       │   │   │   │   │   ├── __init__.cpython-39.pyc
│       │   │   │   │   │   └── syn_seq_encoder.cpython-39.pyc
│       │   │   │   │   ├── methods
│       │   │   │   │   │   ├── cart.py
│       │   │   │   │   │   ├── ctree.py
│       │   │   │   │   │   ├── logreg.py
│       │   │   │   │   │   ├── misc.py
│       │   │   │   │   │   ├── norm.py
│       │   │   │   │   │   ├── pmm.py
│       │   │   │   │   │   ├── polyreg.py
│       │   │   │   │   │   └── rf.py
│       │   │   │   │   ├── syn_seq.py
│       │   │   │   │   ├── syn_seq_constraints.py
│       │   │   │   │   └── syn_seq_encoder.py
│       │   │   │   ├── tabnet.py
│       │   │   │   ├── tabular_aim.py
│       │   │   │   ├── tabular_arf.py
│       │   │   │   ├── tabular_ddpm
│       │   │   │   │   ├── __init__.py
│       │   │   │   │   ├── gaussian_multinomial_diffsuion.py
│       │   │   │   │   ├── modules.py
│       │   │   │   │   └── utils.py
│       │   │   │   ├── tabular_encoder.py
│       │   │   │   ├── tabular_flows.py
│       │   │   │   ├── tabular_gan.py
│       │   │   │   ├── tabular_goggle.py
│       │   │   │   ├── tabular_great.py
│       │   │   │   ├── tabular_vae.py
│       │   │   │   ├── time_series_survival
│       │   │   │   │   ├── __init__.py
│       │   │   │   │   ├── __pycache__
│       │   │   │   │   │   ├── __init__.cpython-39.pyc
│       │   │   │   │   │   ├── _base.cpython-39.pyc
│       │   │   │   │   │   ├── benchmarks.cpython-39.pyc
│       │   │   │   │   │   ├── loader.cpython-39.pyc
│       │   │   │   │   │   ├── ts_surv_coxph.cpython-39.pyc
│       │   │   │   │   │   ├── ts_surv_dynamic_deephit.cpython-39.pyc
│       │   │   │   │   │   ├── ts_surv_xgb.cpython-39.pyc
│       │   │   │   │   │   └── utils.cpython-39.pyc
│       │   │   │   │   ├── _base.py
│       │   │   │   │   ├── benchmarks.py
│       │   │   │   │   ├── loader.py
│       │   │   │   │   ├── ts_surv_coxph.py
│       │   │   │   │   ├── ts_surv_dynamic_deephit.py
│       │   │   │   │   ├── ts_surv_xgb.py
│       │   │   │   │   └── utils.py
│       │   │   │   ├── time_to_event
│       │   │   │   │   ├── __init__.py
│       │   │   │   │   ├── _base.py
│       │   │   │   │   ├── benchmarks.py
│       │   │   │   │   ├── loader.py
│       │   │   │   │   ├── metrics.py
│       │   │   │   │   ├── tte_aft.py
│       │   │   │   │   ├── tte_coxph.py
│       │   │   │   │   ├── tte_date.py
│       │   │   │   │   ├── tte_deephit.py
│       │   │   │   │   ├── tte_survival_function_regression.py
│       │   │   │   │   ├── tte_survival_time_series.py
│       │   │   │   │   ├── tte_tenn.py
│       │   │   │   │   └── tte_xgb.py
│       │   │   │   ├── transformer.py
│       │   │   │   ├── ts_gan.py
│       │   │   │   ├── ts_model.py
│       │   │   │   ├── ts_tabular_gan.py
│       │   │   │   ├── ts_tabular_vae.py
│       │   │   │   ├── ts_vae.py
│       │   │   │   └── vae.py
│       │   │   ├── plugin.py
│       │   │   ├── schema.py
│       │   │   └── serializable.py
│       │   ├── domain_adaptation
│       │   │   ├── __init__.py
│       │   │   └── plugin_radialgan.py
│       │   ├── generic
│       │   │   ├── __init__.py
│       │   │   ├── plugin_arf.py
│       │   │   ├── plugin_bayesian_network.py
│       │   │   ├── plugin_ctgan.py
│       │   │   ├── plugin_ddpm.py
│       │   │   ├── plugin_dummy_sampler.py
│       │   │   ├── plugin_goggle.py
│       │   │   ├── plugin_great.py
│       │   │   ├── plugin_marginal_distributions.py
│       │   │   ├── plugin_nflow.py
│       │   │   ├── plugin_rtvae.py
│       │   │   ├── plugin_syn_seq.py
│       │   │   ├── plugin_tvae.py
│       │   │   └── plugin_uniform_sampler.py
│       │   ├── images
│       │   │   ├── __init__.py
│       │   │   ├── plugin_image_adsgan.py
│       │   │   └── plugin_image_cgan.py
│       │   ├── privacy
│       │   │   ├── __init__.py
│       │   │   ├── plugin_adsgan.py
│       │   │   ├── plugin_aim.py
│       │   │   ├── plugin_decaf.py
│       │   │   ├── plugin_dpgan.py
│       │   │   ├── plugin_pategan.py
│       │   │   └── plugin_privbayes.py
│       │   ├── survival_analysis
│       │   │   ├── __init__.py
│       │   │   ├── _survival_pipeline.py
│       │   │   ├── plugin_survae.py
│       │   │   ├── plugin_survival_ctgan.py
│       │   │   ├── plugin_survival_gan.py
│       │   │   └── plugin_survival_nflow.py
│       │   └── time_series
│       │       ├── __init__.py
│       │       ├── plugin_fflows.py
│       │       ├── plugin_timegan.py
│       │       └── plugin_timevae.py
│       ├── utils
│       │   ├── __pycache__
│       │   │   ├── compression.cpython-39.pyc
│       │   │   ├── constants.cpython-39.pyc
│       │   │   ├── dataframe.cpython-39.pyc
│       │   │   ├── evaluation.cpython-39.pyc
│       │   │   ├── optimizer.cpython-39.pyc
│       │   │   ├── redis_wrapper.cpython-39.pyc
│       │   │   ├── reproducibility.cpython-39.pyc
│       │   │   ├── samplers.cpython-39.pyc
│       │   │   └── serialization.cpython-39.pyc
│       │   ├── anonymization.py
│       │   ├── callbacks.py
│       │   ├── compression.py
│       │   ├── constants.py
│       │   ├── dataframe.py
│       │   ├── datasets
│       │   │   ├── __init__.py
│       │   │   ├── categorical
│       │   │   │   ├── __init__.py
│       │   │   │   ├── categorical_adult.py
│       │   │   │   └── data
│       │   │   │       └── __init__ .py
│       │   │   └── time_series
│       │   │       ├── __init__.py
│       │   │       ├── data
│       │   │       │   └── __init__.py
│       │   │       ├── google_stocks.py
│       │   │       ├── pbc.py
│       │   │       └── sine.py
│       │   ├── evaluation.py
│       │   ├── optimizer.py
│       │   ├── optuna_sample.py
│       │   ├── redis_wrapper.py
│       │   ├── reproducibility.py
│       │   ├── samplers.py
│       │   └── serialization.py
│       └── version.py



71 directories, 415 files
(base) minkeychang@Minkeys-Laptop synthcity % 