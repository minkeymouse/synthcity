INSTRUCTION:


SAMPLE RUN RESULT:

import sys
import warnings

warnings.filterwarnings("ignore")

from sklearn.datasets import load_diabetes

# synthcity absolute
import synthcity.logger as log
from synthcity.plugins import Plugins

log.add(sink=sys.stderr, level="INFO")



CURRENT WORKING CODE:
# synthcity/plugins/core/models/syn_seq/syn_seq_encoder.py

from typing import Optional, Dict, Any, List, Union
import pandas as pd
import numpy as np
from sklearn.base import TransformerMixin, BaseEstimator

from synthcity.plugins.core.models.feature_encoder import DatetimeEncoder

class Syn_SeqEncoder(TransformerMixin, BaseEstimator):
    """
    Syn_SeqEncoder handles preprocessing and postprocessing tasks using fit/transform pattern,
    plus manages a 'variable_selection' matrix (like a prediction matrix).
    
    This encoder:
      - Orders columns (if syn_order specified).
      - Attempts to detect if a column is numeric, category, or date (or uses the user’s col_type).
      - Splits numeric columns into a “clean numeric” + a “_cat” column for special-value or missing marking.
      - Builds a default or user-provided variable_selection matrix (row=target, col=predictor).
      - Assigns placeholder methods (so that we have some field to see them). 
        (Now revised to match aggregator’s recognized method strings like “SWR” and “CART”.)
    """

    def __init__(
        self,
        columns_special_values: Optional[Dict[str, Any]] = None,
        syn_order: Optional[List[str]] = None,
        max_categories: int = 20,
        user_variable_selection: Optional[pd.DataFrame] = None,
        # user-declared column types (e.g. {"C1":"category","N1":"numeric","D1":"date"})
        col_type: Optional[Dict[str, str]] = None,
    ) -> None:
        """
        Args:
            columns_special_values : {colName : [specialVals...]}
            syn_order : optional column order to use
            max_categories : threshold for deciding numeric vs categorical
            user_variable_selection : optional DataFrame(n x n) with row=target, col=predictor
            col_type : user-declared column types, e.g. {"C2":"category","N1":"numeric","D1":"date"}
        """
        self.columns_special_values = columns_special_values or {}
        self.syn_order = syn_order or []
        self.max_categories = max_categories
        self.user_variable_selection = user_variable_selection

        # track info about columns
        self.categorical_info_: Dict[str, Dict[str, Any]] = {}
        self.numeric_info_: Dict[str, Dict[str, Any]] = {}
        # We'll track date columns separately
        self.date_info_: Dict[str, Dict[str, Any]] = {}

        self.column_order_: List[str] = []
        self.method_assignments: Dict[str, str] = {}
        self.variable_selection_: Optional[pd.DataFrame] = None

        # store user col_type
        self.col_type = col_type or {}

    def fit(self, X: pd.DataFrame, y=None) -> "Syn_SeqEncoder":
        # copy X to avoid side effects
        X = X.copy()

        self._detect_column_order(X)
        self._detect_col_types(X)
        self._detect_special_values(X)
        self._build_variable_selection_matrix(X)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        # 1) reorder columns
        X = self._reorder_columns(X)
        # 2) split numeric => numeric + numeric_cat
        X = self._split_numeric_cols(X)
        # 3) update dtypes (including date if declared)
        X = self._update_column_types(X)
        # 4) assign placeholder methods (aligned with aggregator’s method names)
        X = self._assign_methods(X)
        # 5) update variable_selection for newly created columns
        self._update_variable_selection_after_split(X)

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        # 1) restore special values for numeric/date
        for col, vals in self.columns_special_values.items():
            if col in X.columns:
                # if a single special value, use it directly
                if isinstance(vals, list) and len(vals) == 1:
                    vals = vals[0]
                X[col] = X[col].replace(pd.NA, vals)

        # 2) restore dtype for categorical
        for col, info in self.categorical_info_.items():
            if col in X.columns:
                X[col] = X[col].astype(info["dtype"])

        # 3) restore date columns
        for col, info in self.date_info_.items():
            if col in X.columns:
                X[col] = pd.to_datetime(X[col], errors="coerce")

        # 4) restore numeric
        for col, info in self.numeric_info_.items():
            if col in X.columns:
                X[col] = X[col].astype(info["dtype"])

        return X

    # -----------------------------------------------------
    # variable_selection building
    # -----------------------------------------------------
    def _build_variable_selection_matrix(self, X: pd.DataFrame) -> None:
        """
        If user provided a variable_selection DataFrame, validate it. Otherwise,
        build a default (row=target, columns=all prior columns).
        """
        n = len(self.column_order_)
        df_cols = self.column_order_

        if self.user_variable_selection is not None:
            vs = self.user_variable_selection.copy()
            if vs.shape != (n, n):
                raise ValueError(
                    f"user_variable_selection must be shape ({n},{n}), got {vs.shape}"
                )
            # check row/col alignment
            if list(vs.index) != df_cols or list(vs.columns) != df_cols:
                raise ValueError(
                    "Mismatch in user_variable_selection index/columns vs syn_order."
                )
            self.variable_selection_ = vs
        else:
            # default: row=target col=predictor => row i uses columns [0..i-1]
            vs = pd.DataFrame(0, index=df_cols, columns=df_cols)
            for i in range(n):
                for j in range(i):
                    vs.iat[i, j] = 1
            self.variable_selection_ = vs

    def _update_variable_selection_after_split(self, X: pd.DataFrame) -> None:
        """
        If numeric columns were split (col => col + col_cat), we add the new
        columns into the variable_selection matrix. They replicate the row/col
        from the original but don't self-predict.
        """
        if self.variable_selection_ is None:
            return
        old_vs = self.variable_selection_
        old_rows = old_vs.index.tolist()
        old_cols = old_vs.columns.tolist()

        final_cols = list(X.columns)
        new_cols = [c for c in final_cols if c not in old_rows]
        if not new_cols:
            return

        vs_new = pd.DataFrame(0, index=old_rows + new_cols, columns=old_cols + new_cols)

        # copy old content
        for r in old_rows:
            for c in old_cols:
                vs_new.at[r, c] = old_vs.at[r, c]

        # handle new splitted columns
        for c_new in new_cols:
            if c_new.endswith("_cat"):
                c_base = c_new[:-4]  # e.g. "age_cat" => "age"
                if c_base in vs_new.index and c_base in vs_new.columns:
                    # copy entire row from base
                    for c2 in old_cols:
                        vs_new.at[c_new, c2] = vs_new.at[c_base, c2]
                    # copy entire column from base
                    for r2 in old_rows:
                        vs_new.at[r2, c_new] = vs_new.at[r2, c_base]

                vs_new.at[c_new, c_new] = 0  # new col doesn't predict itself
            else:
                # brand new col => remain all zeros
                pass

        self.variable_selection_ = vs_new

    # -----------------------------------------------------
    # HELPER sub-routines
    # -----------------------------------------------------
    def _detect_column_order(self, X: pd.DataFrame):
        """
        If user gave syn_order, we filter columns. Otherwise, use X.columns.
        """
        if self.syn_order:
            self.column_order_ = [c for c in self.syn_order if c in X.columns]
        else:
            self.column_order_ = list(X.columns)

    def _detect_col_types(self, X: pd.DataFrame):
        """
        For each column, decide if numeric/categorical/date, unless
        col_type overrides. Then store in numeric_info_, categorical_info_, date_info_.
        """
        self.numeric_info_.clear()
        self.categorical_info_.clear()
        self.date_info_.clear()

        for col in X.columns:
            declared_type = self.col_type.get(col, "").lower()
            if declared_type == "category":
                self.categorical_info_[col] = {"dtype": "category"}
            elif declared_type == "numeric":
                self.numeric_info_[col] = {"dtype": X[col].dtype}
            elif declared_type == "date":
                self.date_info_[col] = {"dtype": "datetime64[ns]"}
            else:
                # fallback auto-detect
                nuniq = X[col].nunique()
                # if #unique <= max_categories => treat as category
                if nuniq > self.max_categories:
                    # might be numeric or date
                    if pd.api.types.is_datetime64_any_dtype(X[col]):
                        self.date_info_[col] = {"dtype": "datetime64[ns]"}
                    else:
                        self.numeric_info_[col] = {"dtype": X[col].dtype}
                else:
                    self.categorical_info_[col] = {"dtype": "category"}

    def _detect_special_values(self, X: pd.DataFrame):
        """
        For each col, if a single value has >90% freq, consider it a "special" value
        and store in columns_special_values. The user can override or supply additional.
        """
        for col in X.columns:
            freq = X[col].value_counts(dropna=False, normalize=True)
            high_vals = freq[freq > 0.9].index.tolist()
            if high_vals:
                existing_vals = self.columns_special_values.get(col, [])
                combined = set(existing_vals).union(set(high_vals))
                self.columns_special_values[col] = list(combined)

    def _reorder_columns(self, X: pd.DataFrame) -> pd.DataFrame:
        if self.column_order_:
            new_cols = [c for c in self.column_order_ if c in X.columns]
            return X[new_cols]
        return X

    def _split_numeric_cols(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        For numeric columns, create an extra col for "special or missing" => col_cat
        Then in col, we set them to NA to ensure they are not used as numeric. 
        """
        for col in list(self.numeric_info_.keys()):
            if col not in X.columns:
                continue
            cat_col = f"{col}_cat"
            special_vals = self.columns_special_values.get(col, [])

            # Mark special or missing in cat_col
            X[cat_col] = X[col].apply(
                lambda x: x if (x in special_vals or pd.isna(x)) else -777
            )
            X[cat_col] = X[cat_col].fillna(-9999)  # placeholder for NA
            # In the numeric column, remove those special/NA
            X[col] = X[col].apply(
                lambda x: x if (x not in special_vals and not pd.isna(x)) else pd.NA
            )
            X[cat_col] = X[cat_col].astype("category")

        return X

    def _update_column_types(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Attempt to cast date columns to datetime,
        numeric to numeric dtype,
        categorical to category dtype.
        """
        # handle date columns first
        for col in self.date_info_:
            if col in X.columns:
                X[col] = pd.to_datetime(X[col], errors="coerce")

        # handle numeric / categorical
        for col in X.columns:
            if col in self.numeric_info_:
                X[col] = X[col].astype(self.numeric_info_[col]["dtype"])
            elif col in self.categorical_info_:
                X[col] = X[col].astype("category")

        return X

    def _assign_methods(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Assign placeholder methods to each column, 
        matching the aggregator's known method strings:
          - the aggregator typically uses "SWR" for the first column
          - default "CART" for subsequent columns
        """
        self.method_assignments.clear()
        first = True
        for c in X.columns:
            if first:
                self.method_assignments[c] = "SWR"
                first = False
            else:
                self.method_assignments[c] = "CART"
        return X

    @staticmethod
    def update_variable_selection(
        var_sel_df: pd.DataFrame,
        user_dict: Dict[str, List[str]]
    ) -> pd.DataFrame:
        """
        In the variable_selection matrix, set row=target_col => 1 in the predictor columns.
          user_dict = { "D": ["B","C"], "A": ["B"] }
          => row "D", col "B","C" => 1, row "A", col "B" => 1
        """
        for target_col, predictor_list in user_dict.items():
            if target_col not in var_sel_df.index:
                print(f"[WARNING] '{target_col}' not in var_sel_df.index => skipping.")
                continue
            # set row=target_col to 0 first
            var_sel_df.loc[target_col, :] = 0
            # set 1 only for the user-specified predictors
            for pred in predictor_list:
                if pred in var_sel_df.columns:
                    var_sel_df.loc[target_col, pred] = 1
                else:
                    print(f"[WARNING] predictor '{pred}' not in columns => skipping.")
        return var_sel_df


src/synthcity/plugins/core/models/syn_seq/cart.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/ctree.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/logreg.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/misc.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/norm.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/pmm.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/polyreg.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/rf.py
** Not a valid file path or file does not exist. **

src/synthcity/plugins/core/models/syn_seq/syn_seq.py
# File: syn_seq.py

from typing import Any, Dict, List, Optional, Union
import pandas as pd
import numpy as np

# synergy imports
from synthcity.plugins.core.dataloader import Syn_SeqDataLoader
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.models.syn_seq.syn_seq_constraints import SynSeqConstraints

# methods for actual column-by-column synthesis
from synthcity.plugins.core.models.syn_seq.methods.cart import syn_cart
from synthcity.plugins.core.models.syn_seq.methods.ctree import syn_ctree
from synthcity.plugins.core.models.syn_seq.methods.logreg import syn_logreg
from synthcity.plugins.core.models.syn_seq.methods.norm import syn_norm, syn_lognorm
from synthcity.plugins.core.models.syn_seq.methods.pmm import syn_pmm
from synthcity.plugins.core.models.syn_seq.methods.polyreg import syn_polyreg
from synthcity.plugins.core.models.syn_seq.methods.rf import syn_rf
from synthcity.plugins.core.models.syn_seq.methods.misc import syn_random, syn_swr


def _to_synseq_constraints(
    constraint_input: Union[None, Dict[str, List[Any]], Constraints]
) -> Optional[SynSeqConstraints]:
    """
    Converts user-supplied constraints (dict or Constraints object) 
    into a SynSeqConstraints object, so we can do direct substitution or row filtering.
    """
    if constraint_input is None:
        return None

    if isinstance(constraint_input, Constraints):
        if isinstance(constraint_input, SynSeqConstraints):
            return constraint_input
        else:
            return SynSeqConstraints(rules=constraint_input.rules)

    if isinstance(constraint_input, dict):
        rules_list = []
        for col, rule_list in constraint_input.items():
            if not isinstance(rule_list, list) or len(rule_list) < 2:
                print(f"[WARNING] Malformed constraint for '{col}' => {rule_list}")
                continue
            op = rule_list[0]
            val = rule_list[1]
            rules_list.append((col, op, val))
        return SynSeqConstraints(rules=rules_list)

    raise ValueError(f"Unsupported constraint type: {type(constraint_input)}")


class Syn_Seq:
    """
    A 'synthpop'-like sequential aggregator for synthetic data generation.
    
    Overview of the process:
      - We accept a Syn_SeqDataLoader, but we do NOT automatically rely on the 
        numeric splitting from the loader itself. Instead, we explicitly call
        'encode(...)' here using the same Syn_SeqEncoder. 
      - This means, at .fit() time, we transform the user’s data => an encoded DataFrame
        with any numeric columns possibly split into "col" + "col_cat." We then fit
        each column method (cart/pmm/etc.) on that encoded data.
      - At .generate() time, we produce synthetic data in that encoded space, then 
        call 'decode(...)' to revert to the user’s original columns.
      - If constraints are provided, we either do repeated tries ('strict=True') 
        or a single pass filtering approach.

    NOTE: We are NOT modifying the DataLoader or the Syn_SeqEncoder code itself. 
          We only use them as intended: 'encode' for splitting before .fit, 
          then 'decode' after generating the final data.
    """

    def __init__(
        self,
        random_state: int = 0,
        default_first_method: str = "SWR",
        default_other_method: str = "CART",
        strict: bool = True,
        sampling_patience: int = 500,
        seq_id_col: str = "seq_id",
        seq_time_col: str = "seq_time_id",
        **kwargs: Any
    ):
        """
        Args:
            random_state: for reproducibility
            default_first_method: fallback if the user does not specify method for col 0
            default_other_method: fallback for subsequent columns
            strict: if True => repeated tries to meet constraints
            sampling_patience: max tries in strict mode
            seq_id_col, seq_time_col: used by constraints if needed
            **kwargs: aggregator-level arguments (unused here)
        """
        self.random_state = random_state
        self.default_first_method = default_first_method
        self.default_other_method = default_other_method
        self.strict = strict
        self.sampling_patience = sampling_patience
        self.seq_id_col = seq_id_col
        self.seq_time_col = seq_time_col

        # Store per-column model info: method, predictor columns, fit_info, etc.
        self._column_models: Dict[str, Dict[str, Any]] = {}

        # The user can pass a list of methods or a variable_selection dict
        self.method_list: List[str] = []
        self.variable_selection: Dict[str, List[str]] = {}

        # For encode/decode usage
        self._encoders: Dict[str, Any] = {}

        self._model_trained = False

    # ----------------------------------------------------------------
    # fit(...)
    # ----------------------------------------------------------------
    def fit(
        self,
        loader: Syn_SeqDataLoader,
        method: Optional[List[str]] = None,
        variable_selection: Optional[Dict[str, List[str]]] = None,
        *args: Any,
        **kwargs: Any
    ) -> "Syn_Seq":
        """
        1) We call loader.encode(...) => splitted/encoded data, plus the encoder dict.
        2) Build predictor matrix from user variable_selection or default sequential logic.
        3) For each (encoded) column => store a partial model fit.
        """
        if not isinstance(loader, Syn_SeqDataLoader):
            raise TypeError("Syn_Seq aggregator requires a Syn_SeqDataLoader.")

        # 1) encode => splitted/encoded data
        encoded_loader, self._encoders = loader.encode(encoders=None)
        df_encoded = encoded_loader.dataframe()
        if df_encoded.empty:
            raise ValueError("No data after encoding. Cannot train on empty DataFrame.")

        self.method_list = method or []
        self.variable_selection = variable_selection or {}

        col_list = list(df_encoded.columns)
        n_cols = len(col_list)

        # 2) expand user methods if needed
        final_methods = []
        for i, col in enumerate(col_list):
            if i < len(self.method_list):
                final_methods.append(self.method_list[i])
            else:
                fallback = (
                    self.default_first_method if i == 0 else self.default_other_method
                )
                final_methods.append(fallback)

        # Build a default predictor matrix
        vs_matrix = pd.DataFrame(0, index=col_list, columns=col_list)
        for i in range(n_cols):
            vs_matrix.iloc[i, :i] = 1

        # incorporate user variable_selection
        for target_col, pred_cols in self.variable_selection.items():
            if target_col in vs_matrix.index:
                vs_matrix.loc[target_col, :] = 0
                for pc in pred_cols:
                    if pc in vs_matrix.columns:
                        vs_matrix.loc[target_col, pc] = 1

        print("[INFO] aggregator: final method assignment:")
        for col, meth in zip(col_list, final_methods):
            print(f"   - {col} => {meth}")
        print("[INFO] aggregator: final variable_selection matrix:")
        print(vs_matrix)

        # 3) train each encoded column
        self._column_models.clear()
        for i, col in enumerate(col_list):
            chosen_method = final_methods[i]
            pred_mask = vs_matrix.loc[col] == 1
            preds = vs_matrix.columns[pred_mask].tolist()

            y = df_encoded[col].values
            X = df_encoded[preds].values if preds else np.zeros((len(y), 0))

            print(f"[INFO] Fitting encoded column '{col}' with method '{chosen_method}'...")
            self._column_models[col] = {
                "method": chosen_method,
                "predictors": preds,
                "fit_info": self._fit_single_column(y, X, chosen_method),
            }

        self._model_trained = True
        return self

    def _fit_single_column(
        self, y: np.ndarray, X: np.ndarray, method: str
    ) -> Dict[str, Any]:
        """
        For each column, store (obs_y, obs_X) and the method type. 
        Actual training might happen again at generation time or partial store here.
        """
        method_lower = method.strip().lower()

        if method_lower in {
            "cart", "ctree", "rf", "norm", "lognorm", 
            "pmm", "logreg", "polyreg",
        }:
            return {"type": method_lower, "obs_y": y, "obs_X": X}
        elif method_lower == "swr":
            return {"type": "swr", "obs_y": y}
        elif method_lower == "random":
            return {"type": "random", "obs_y": y}
        else:
            # fallback => random
            return {"type": "random", "obs_y": y}

    # ----------------------------------------------------------------
    # generate(...)
    # ----------------------------------------------------------------
    def generate(
        self,
        count: int,
        constraint: Union[None, Dict[str, List[Any]], Constraints] = None,
        *args: Any,
        **kwargs: Any
    ) -> Syn_SeqDataLoader:
        """
        1) Generate 'encoded' synthetic data column by column.
        2) If constraints => either repeated tries or single pass filter.
        3) Finally, 'decode' to revert to the original user columns 
           (merging splitted numeric columns, etc.).
        """
        if not self._model_trained:
            raise RuntimeError("Must fit Syn_Seq before calling generate().")

        # unify constraints as a SynSeqConstraints
        syn_constraints = _to_synseq_constraints(constraint)
        if syn_constraints:
            syn_constraints.seq_id_feature = self.seq_id_col
            syn_constraints.seq_time_id_feature = self.seq_time_col

        # Use strict or single-pass approach
        if self.strict and syn_constraints is not None:
            encoded_df = self._attempt_strict_generation(count, syn_constraints)
        else:
            encoded_df = self._generate_once(count)
            if syn_constraints:
                encoded_df = self._apply_constraint_corrections(encoded_df, syn_constraints)
                encoded_df = syn_constraints.match(encoded_df)

        # Now decode to revert from splitted columns => original user columns
        # Wrap 'encoded_df' in a minimal loader so we can call decode(...)
        temp_loader = Syn_SeqDataLoader(data=encoded_df, syn_order=list(encoded_df.columns))
        decoded_loader = temp_loader.decode(self._encoders)

        # Return the final (decoded) data loader
        return decoded_loader

    def _generate_once(self, count: int) -> pd.DataFrame:
        """
        Single pass ignoring constraints. 
        Produces data in the encoded space (splitted columns).
        """
        col_list = list(self._column_models.keys())
        syn_df = pd.DataFrame(index=range(count))

        for col in col_list:
            info = self._column_models[col]
            method = info["method"]
            preds = info["predictors"]
            fit_data = info["fit_info"]

            Xp = syn_df[preds].values if preds else np.zeros((count, 0))
            print(f"[INFO] Generating encoded column '{col}' with method '{method}'...")

            new_vals = self._generate_single_column(method, fit_data, Xp, count)
            syn_df[col] = new_vals

        return syn_df

    def _generate_single_column(
        self,
        method: str,
        fit_model: Dict[str, Any],
        Xp: np.ndarray,
        count: int
    ) -> pd.Series:
        """
        Calls the appropriate method for generating that encoded column's data.
        """
        method_lower = method.strip().lower()

        y_obs = fit_model.get("obs_y", None)
        X_obs = fit_model.get("obs_X", None)

        if method_lower == "cart":
            res = syn_cart(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(res["res"])
        elif method_lower == "ctree":
            res = syn_ctree(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(res["res"])
        elif method_lower == "rf":
            res = syn_rf(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(res["res"])
        elif method_lower == "norm":
            res = syn_norm(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(res["res"])
        elif method_lower == "lognorm":
            res = syn_lognorm(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(res["res"])
        elif method_lower == "pmm":
            res = syn_pmm(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(res["res"])
        elif method_lower == "logreg":
            res = syn_logreg(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(res["res"])
        elif method_lower == "polyreg":
            res = syn_polyreg(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(res["res"])
        elif method_lower == "swr":
            res = syn_swr(y=y_obs, X=None, Xp=np.zeros((count, 1)), random_state=self.random_state)
            return pd.Series(res["res"])
        elif method_lower == "random":
            res = syn_random(y=y_obs, X=None, Xp=np.zeros((count, 1)), random_state=self.random_state)
            return pd.Series(res["res"])
        else:
            # fallback => random
            fallback = syn_random(y=y_obs, X=None, Xp=np.zeros((count, 1)), random_state=self.random_state)
            return pd.Series(fallback["res"])

    # ----------------------------------------------------------------
    # strict approach
    # ----------------------------------------------------------------
    def _attempt_strict_generation(
        self,
        count: int,
        syn_constraints: SynSeqConstraints
    ) -> pd.DataFrame:
        """
        If 'strict' => we generate repeatedly until constraints are satisfied 
        or we exhaust the sampling_patience. 
        We keep accumulating unique rows in the encoded space.
        """
        result_df = pd.DataFrame()
        tries = 0
        while len(result_df) < count and tries < self.sampling_patience:
            tries += 1
            chunk = self._generate_once(count)

            chunk = self._apply_constraint_corrections(chunk, syn_constraints)
            chunk = syn_constraints.match(chunk)
            chunk = chunk.drop_duplicates()

            result_df = pd.concat([result_df, chunk], ignore_index=True)

        return result_df.head(count)

    def _apply_constraint_corrections(
        self,
        df: pd.DataFrame,
        syn_constraints: SynSeqConstraints
    ) -> pd.DataFrame:
        """
        For '=' constraints, do direct substitution first. 
        For other constraints => let match(...) do filtering.
        """
        new_df = df.copy()
        for (feature, op, val) in syn_constraints.rules:
            if op in ["=", "=="]:
                new_df = syn_constraints._correct(new_df, feature, op, val)
        return new_df


src/synthcity/plugins/core/models/syn_seq/syn_seq_constraints.py
# File: syn_seq_constraints.py

from typing import Any, List, Dict, Tuple, Union
import pandas as pd
import numpy as np

# We import the base Constraints to inherit from
from synthcity.plugins.core.constraints import Constraints

# 
# A single sub-rule is (feature, op, value).
# For "chained" logic, we might interpret:
#    "For rows that pass all sub-rules (1..k-1), apply sub-rule k as a filter or correction."
#
# Example constraint input:
#   {
#       "N1": [
#           ("C1", "in", ["AAA","BBB"]),
#           ("N1", ">", 125)
#       ]
#   }
# means:
#  1) If a row passes (C1 in [AAA,BBB]), 
#  2) Then also enforce (N1 > 125) on that row.
#
# If the first sub-rule is not satisfied, the second doesn't apply to that row.
# If the first sub-rule is satisfied but not the second => row fails entirely.
#

class SynSeqConstraints(Constraints):
    """
    An extension that supports "chained" constraints for sequential logic:
      - If sub-rule 1 is satisfied => we must also pass sub-rule 2, and so on.
      - 'chained_rules' can be a dict of { targetCol : [ (feature, op, val), (feature, op, val), ... ] }.
        Example:
          {
            "N1": [
                ("C1", "in", ["AAA","BBB"]),
                ("N1", ">", 125)
            ]
          }
        read as: "If (C1 in [AAA,BBB]) => enforce (N1 > 125)".
      
      Each sub-rule uses the same _eval logic for <, <=, >, >=, ==, in, etc.
      If a sub-rule fails => that entire row fails (filtered out) if using .match(), 
      or is corrected if possible (.correct).
    """

    def __init__(
        self,
        # You can still pass standard constraints as "rules",
        # or pass new "chained_rules" in dict format
        rules: List[Tuple[str, str, Any]] = None,
        chained_rules: Dict[str, List[Tuple[str, str, Any]]] = None,
        seq_id_feature: str = "seq_id",
        seq_time_id_feature: str = "seq_time_id",
        **kwargs: Any,
    ):
        # Let the base constructor handle standard 'rules'
        super().__init__(rules=rules if rules else [])
        self.seq_id_feature = seq_id_feature
        self.seq_time_id_feature = seq_time_id_feature

        # We'll store the "chained" sub-rules in a separate structure
        # keyed by "target column" or "some label"
        self.chained_rules = chained_rules if chained_rules else {}

    def match(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Overridden. We can do row-level or entire-sequence filtering. 
        If you still want entire-sequence filtering, you can define match_sequential() and call it below.
        For demonstration, let's do row-level filtering with chained sub-rules.
        """
        df_copy = X.copy()
        # first, apply base constraints (self.rules) at row-level:
        base_mask = super().filter(df_copy)
        df_filtered = df_copy[base_mask].copy()
        if df_filtered.empty:
            return df_filtered

        # next, apply "chained" constraints
        df_filtered = self._match_chained(df_filtered)
        return df_filtered

    def _match_chained(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        For each entry in self.chained_rules => interpret a *sequence* of sub-rules.
        If the row passes sub-rule[0], sub-rule[1], ... sub-rule[n-2], 
        then sub-rule[n-1] is also enforced. 
        If it fails at any step => the row is filtered out or "corrected" (depending on your logic).
        
        For simplicity below, we do "if sub-rule[i] is satisfied => proceed, else row is out."
        """
        df = X.copy()
        for target_key, rule_chain in self.chained_rules.items():
            # We interpret the chain in order
            # e.g. [("C1","in", [...]), ("N1",">",125)]
            # step i=0 => a filter => if row fails => out
            # step i=1 => a further filter => if row fails => out
            # etc.

            # We'll build a mask for these sub-rules
            keep_mask = pd.Series([True]*len(df), index=df.index)

            for (feature, op, operand) in rule_chain:
                cur_mask = self._eval(df, feature, op, operand)
                # only keep the rows that pass this sub-rule
                keep_mask = keep_mask & cur_mask

            # after we apply all sub-rules in the chain, 
            # rows that didn't pass => out
            df = df[keep_mask].copy()
            if df.empty:
                break

        return df

    def _eval(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.Index:
        """
        If we want to also handle direct substitution '=' or '==' or to skip it, we can override _eval.
        If the user wants a different meaning for '=' in chain logic, we can do so.
        Otherwise, we rely on base Constraints._eval for <, <=, >, >=, ==, in, dtype, etc.
        """
        # If you want direct substitution for '=' or '==', do so in _correct or in some separate logic.
        if op in ["=", "=="]:
            # interpret as equality check
            return (X[feature] == operand) | X[feature].isna()
        else:
            # fallback to base method
            return super()._eval(X, feature, op, operand)

    def _correct(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.DataFrame:
        """
        If user wants direct substitution for '=' => set that col's entire column to operand. 
        Or you can do row-level changes only for failing rows, etc.
        """
        if op in ["=", "=="]:
            X.loc[:, feature] = operand
            return X
        return super()._correct(X, feature, op, operand)

    # If you want entire-sequence logic, you'd define match_sequential below:

    # Example
    #     constraint = {
    #   "N1": [
    #     ["C1", "in", ["AAA","BBB"]],
    #     ["N1", ">", 125]
    #   ]
    # }

    # def match_sequential(self, X: pd.DataFrame) -> pd.DataFrame:
    #     """
    #     Example method that applies constraints at the group (sequence) level:
    #       - if a sub-rule fails for ANY row => remove entire sequence
    #       - or do direct substitution in entire sequence
    #     """
    #     df_copy = X.copy()
    #     base_mask = super().filter(df_copy)
    #     # group by seq_id
    #     grouped = df_copy.groupby(self.seq_id_feature)
    #
    #     keep_seq_ids = []
    #     for seq_id, group in grouped:
    #         # if all rows pass => keep entire sequence
    #         if base_mask[group.index].all():
    #             # next, check chained
    #             # for each sub-rule chain, we can test if group passes
    #             # if not => exclude entire seq
    #             # or we can do partial correction
    #             # ...
    #             keep_seq_ids.append(seq_id)
    #
    #     return df_copy[df_copy[self.seq_id_feature].isin(keep_seq_ids)]



src/synthcity/plugins/core/dataloader.py
# stdlib
import random
from abc import ABCMeta, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union

# third party
import numpy as np
import numpy.ma as ma
import pandas as pd
import PIL
import torch
from pydantic import validate_arguments
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torchvision import transforms

# synthcity absolute
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.dataset import FlexibleDataset, TensorDataset
from synthcity.plugins.core.models.feature_encoder import DatetimeEncoder
from synthcity.utils.compression import compress_dataset, decompress_dataset
from synthcity.utils.serialization import dataframe_hash

# Syn_Seq
from synthcity.plugins.core.models.syn_seq.syn_seq_encoder import Syn_SeqEncoder


class DataLoader(metaclass=ABCMeta):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.DataLoader
        :parts: 1

    Base class for all data loaders.

    Each derived class must implement the following methods:
        unpack() - a method that unpacks the columns and returns features and labels (X, y).
        decorate() - a method that creates a new instance of DataLoader by decorating the input data with the same DataLoader properties (e.g. sensitive features, target column, etc.)
        dataframe() - a method that returns the pandas dataframe that contains all features and samples
        numpy() - a method that returns the numpy array that contains all features and samples
        info() - a method that returns a dictionary of DataLoader information
        __len__() - a method that returns the number of samples in the DataLoader
        satisfies() - a method that tests if the current DataLoader satisfies the constraint provided
        match() - a method that returns a new DataLoader where the provided constraints are met
        from_info() - a static method that creates a DataLoader from the data and the information dictionary
        sample() - returns a new DataLoader that contains a random subset of N samples
        drop() - returns a new DataLoader with a list of columns dropped
        __getitem__() - getting features by names
        __setitem__() - setting features by names
        train() - returns a DataLoader containing the training set
        test() - returns a DataLoader containing the testing set
        fillna() - returns a DataLoader with NaN filled by the provided number(s)


    If any method implementation is missing, the class constructor will fail.

    Constructor Args:
        data_type: str
            The type of DataLoader, currently supports "generic", "time_series" and "survival".
        data: Any
            The object that contains the data
        static_features: List[str]
            List of feature names that are static features (as opposed to temporal features).
        temporal_features:
            List of feature names that are temporal features, i.e. observed over time.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        outcome_features:
            The feature name that provides labels for downstream tasks.
    """

    def __init__(
        self,
        data_type: str,
        data: Any,
        static_features: List[str] = [],
        temporal_features: List[str] = [],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        outcome_features: List[str] = [],
        train_size: float = 0.8,
        random_state: int = 0,
        **kwargs: Any,
    ) -> None:
        self.static_features = static_features
        self.temporal_features = temporal_features
        self.sensitive_features = sensitive_features
        self.important_features = important_features
        self.outcome_features = outcome_features
        self.random_state = random_state

        self.data = data
        self.data_type = data_type
        self.train_size = train_size

    def raw(self) -> Any:
        return self.data

    @abstractmethod
    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        ...

    @abstractmethod
    def decorate(self, data: Any) -> "DataLoader":
        ...

    def type(self) -> str:
        return self.data_type

    @property
    @abstractmethod
    def shape(self) -> tuple:
        ...

    @property
    @abstractmethod
    def columns(self) -> list:
        ...

    @abstractmethod
    def dataframe(self) -> pd.DataFrame:
        ...

    @abstractmethod
    def numpy(self) -> np.ndarray:
        ...

    @property
    def values(self) -> np.ndarray:
        return self.numpy()

    @abstractmethod
    def info(self) -> dict:
        ...

    @abstractmethod
    def __len__(self) -> int:
        ...

    @abstractmethod
    def satisfies(self, constraints: Constraints) -> bool:
        ...

    @abstractmethod
    def match(self, constraints: Constraints) -> "DataLoader":
        ...

    @staticmethod
    @abstractmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        ...

    @abstractmethod
    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        ...

    @abstractmethod
    def drop(self, columns: list = []) -> "DataLoader":
        ...

    @abstractmethod
    def __getitem__(self, feature: Union[str, list]) -> Any:
        ...

    @abstractmethod
    def __setitem__(self, feature: str, val: Any) -> None:
        ...

    @abstractmethod
    def train(self) -> "DataLoader":
        ...

    @abstractmethod
    def test(self) -> "DataLoader":
        ...

    def hash(self) -> str:
        return dataframe_hash(self.dataframe())

    def __repr__(self, *args: Any, **kwargs: Any) -> str:
        return self.dataframe().__repr__(*args, **kwargs)

    def _repr_html_(self, *args: Any, **kwargs: Any) -> Any:
        return self.dataframe()._repr_html_(*args, **kwargs)

    @abstractmethod
    def fillna(self, value: Any) -> "DataLoader":
        ...

    @abstractmethod
    def compression_protected_features(self) -> list:
        ...

    def domain(self) -> Optional[str]:
        return None

    def compress(
        self,
    ) -> Tuple["DataLoader", Dict]:
        to_compress = self.data.copy().drop(
            columns=self.compression_protected_features()
        )
        compressed, context = compress_dataset(to_compress)
        for protected_col in self.compression_protected_features():
            compressed[protected_col] = self.data[protected_col]

        return self.decorate(compressed), context

    def decompress(self, context: Dict) -> "DataLoader":
        decompressed = decompress_dataset(self.data, context)

        return self.decorate(decompressed)

    def encode(
        self,
        encoders: Optional[Dict[str, Any]] = None,
    ) -> Tuple["DataLoader", Dict]:
        encoded = self.dataframe().copy()
        if encoders is not None:
            for col in encoders:
                if col not in encoded.columns:
                    continue
                encoded[col] = encoders[col].transform(encoded[col])
        else:
            encoders = {}

            for col in encoded.columns:
                if (
                    encoded[col].infer_objects().dtype.kind == "i"
                    and encoded[col].min() == 0
                    and encoded[col].max() == len(encoded[col].unique()) - 1
                ):
                    continue

                if (
                    encoded[col].infer_objects().dtype.kind in ["O", "b"]
                    or len(encoded[col].unique()) < 15
                ):
                    encoder = LabelEncoder().fit(encoded[col])
                    encoded[col] = encoder.transform(encoded[col])
                    encoders[col] = encoder
                elif encoded[col].infer_objects().dtype.kind in ["M"]:
                    encoder = DatetimeEncoder().fit(encoded[col])
                    encoded[col] = encoder.transform(encoded[col]).values
                    encoders[col] = encoder
        return self.from_info(encoded, self.info()), encoders

    def decode(
        self,
        encoders: Dict[str, Any],
    ) -> "DataLoader":
        decoded = self.dataframe().copy()

        for col in encoders:
            if isinstance(encoders[col], LabelEncoder):
                decoded[col] = decoded[col].astype(int)
            else:
                decoded[col] = decoded[col].astype(float)

            decoded[col] = encoders[col].inverse_transform(decoded[col])

        return self.from_info(decoded, self.info())

    @abstractmethod
    def is_tabular(self) -> bool:
        ...

    @abstractmethod
    def get_fairness_column(self) -> Union[str, Any]:
        ...


class GenericDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.GenericDataLoader
        :parts: 1

    Data loader for generic tabular data.

    Constructor Args:
        data: Union[pd.DataFrame, list, np.ndarray]
            The dataset. Either a Pandas DataFrame or a Numpy Array.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        target_column: Optional[str]
            The feature name that provides labels for downstream tasks.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        domain_column: Optional[str]
            Optional domain label, used for domain adaptation algorithms.
        random_state: int
            Defaults to zero.

    Example:
        >>> from sklearn.datasets import load_diabetes
        >>> from synthcity.plugins.core.dataloader import GenericDataLoader
        >>> X, y = load_diabetes(return_X_y=True, as_frame=True)
        >>> X["target"] = y
        >>> # Important note: preprocessing data with OneHotEncoder or StandardScaler is not needed or recommended.
        >>> # Synthcity handles feature encoding and standardization internally.
        >>> loader = GenericDataLoader(X, target_column="target", sensitive_columns=["sex"],)
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: Union[pd.DataFrame, list, np.ndarray],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        target_column: Optional[str] = None,
        fairness_column: Optional[str] = None,
        domain_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)

        data.columns = data.columns.astype(str)
        if target_column is not None:
            self.target_column = target_column
        elif len(data.columns) > 0:
            self.target_column = data.columns[-1]
        else:
            self.target_column = "---"

        self.fairness_column = fairness_column
        self.domain_column = domain_column

        super().__init__(
            data_type="generic",
            data=data,
            static_features=list(data.columns),
            sensitive_features=sensitive_features,
            important_features=important_features,
            outcome_features=[self.target_column],
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape

    def domain(self) -> Optional[str]:
        return self.domain_column

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    @property
    def columns(self) -> list:
        return list(self.data.columns)

    def compression_protected_features(self) -> list:
        out = [self.target_column]
        domain = self.domain()

        if domain is not None:
            out.append(domain)

        return out

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        X = self.data.drop(columns=[self.target_column])
        y = self.data[self.target_column]

        if as_numpy:
            return np.asarray(X), np.asarray(y)
        return X, y

    def dataframe(self) -> pd.DataFrame:
        return self.data

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": self.static_features,
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "outcome_features": self.outcome_features,
            "target_column": self.target_column,
            "fairness_column": self.fairness_column,
            "domain_column": self.domain_column,
            "train_size": self.train_size,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return GenericDataLoader(
            data,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            target_column=self.target_column,
            random_state=self.random_state,
            train_size=self.train_size,
            fairness_column=self.fairness_column,
            domain_column=self.domain_column,
        )

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self.data)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.decorate(constraints.match(self.data))

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        return self.decorate(self.data.sample(count, random_state=random_state))

    def drop(self, columns: list = []) -> "DataLoader":
        return self.decorate(self.data.drop(columns=columns))

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "GenericDataLoader":
        if not isinstance(data, pd.DataFrame):
            raise ValueError(f"Invalid data type {type(data)}")

        return GenericDataLoader(
            data,
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            target_column=info["target_column"],
            fairness_column=info["fairness_column"],
            domain_column=info["domain_column"],
            train_size=info["train_size"],
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data[feature] = val

    def _train_test_split(self) -> Tuple:
        stratify = None
        if self.target_column in self.data:
            target = self.data[self.target_column]
            if target.value_counts().min() > 1:
                stratify = target

        return train_test_split(
            self.data,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )

    def train(self) -> "DataLoader":
        train_data, _ = self._train_test_split()
        return self.decorate(train_data.reset_index(drop=True))

    def test(self) -> "DataLoader":
        _, test_data = self._train_test_split()
        return self.decorate(test_data.reset_index(drop=True))

    def fillna(self, value: Any) -> "DataLoader":
        self.data = self.data.fillna(value)
        return self

    def is_tabular(self) -> bool:
        return True


class SurvivalAnalysisDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.SurvivalAnalysisDataLoader
        :parts: 1

    Data Loader for Survival Analysis Data

    Constructor Args:
        data: Union[pd.DataFrame, list, np.ndarray]
            The dataset. Either a Pandas DataFrame or a Numpy Array.
        time_to_event_column: str
            Survival Analysis specific time-to-event feature
        target_column: str
            The outcome: event or censoring.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        target_column: str
            The feature name that provides labels for downstream tasks.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        domain_column: Optional[str]
            Optional domain label, used for domain adaptation algorithms.
        random_state: int
            Defaults to zero.
        train_size: float
            The ratio to use for train splits.

    Example:
        >>> TODO
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: pd.DataFrame,
        time_to_event_column: str,
        target_column: str,
        time_horizons: list = [],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if target_column not in data.columns:
            raise ValueError(f"Event column {target_column} not found in the dataframe")

        if time_to_event_column not in data.columns:
            raise ValueError(
                f"Time to event column {time_to_event_column} not found in the dataframe"
            )

        T = data[time_to_event_column]
        data_filtered = data[T > 0]
        row_diff = data.shape[0] - data_filtered.shape[0]
        if row_diff > 0:
            raise ValueError(
                f"The time_to_event_column contains {row_diff} values less than or equal to zero. Please remove them."
            )

        if len(time_horizons) == 0:
            time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1].tolist()

        data.columns = data.columns.astype(str)

        self.target_column = target_column
        self.time_to_event_column = time_to_event_column
        self.time_horizons = time_horizons
        self.fairness_column = fairness_column

        super().__init__(
            data_type="survival_analysis",
            data=data,
            static_features=list(data.columns.astype(str)),
            sensitive_features=sensitive_features,
            important_features=important_features,
            outcome_features=[self.target_column],
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape

    @property
    def columns(self) -> list:
        return list(self.data.columns)

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def compression_protected_features(self) -> list:
        out = [self.target_column, self.time_to_event_column]
        domain = self.domain()

        if domain is not None:
            out.append(domain)

        return out

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        X = self.data.drop(columns=[self.target_column, self.time_to_event_column])
        T = self.data[self.time_to_event_column]
        E = self.data[self.target_column]

        if as_numpy:
            return np.asarray(X), np.asarray(T), np.asarray(E)

        return X, T, E

    def dataframe(self) -> pd.DataFrame:
        return self.data

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": list(self.static_features),
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "outcome_features": self.outcome_features,
            "target_column": self.target_column,
            "fairness_column": self.fairness_column,
            "time_to_event_column": self.time_to_event_column,
            "time_horizons": self.time_horizons,
            "train_size": self.train_size,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return SurvivalAnalysisDataLoader(
            data,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            target_column=self.target_column,
            fairness_column=self.fairness_column,
            time_to_event_column=self.time_to_event_column,
            time_horizons=self.time_horizons,
            random_state=self.random_state,
            train_size=self.train_size,
        )

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self.data)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.decorate(
            constraints.match(self.data),
        )

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        return self.decorate(
            self.data.sample(count, random_state=random_state),
        )

    def drop(self, columns: list = []) -> "DataLoader":
        return self.decorate(
            self.data.drop(columns=columns),
        )

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        if not isinstance(data, pd.DataFrame):
            raise ValueError(f"Invalid data type {type(data)}")

        return SurvivalAnalysisDataLoader(
            data,
            target_column=info["target_column"],
            time_to_event_column=info["time_to_event_column"],
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            time_horizons=info["time_horizons"],
            fairness_column=info["fairness_column"],
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data[feature] = val

    def train(self) -> "DataLoader":
        stratify = self.data[self.target_column]
        train_data, _ = train_test_split(
            self.data, train_size=self.train_size, random_state=0, stratify=stratify
        )
        return self.decorate(
            train_data.reset_index(drop=True),
        )

    def test(self) -> "DataLoader":
        stratify = self.data[self.target_column]
        _, test_data = train_test_split(
            self.data,
            train_size=self.train_size,
            random_state=0,
            stratify=stratify,
        )
        return self.decorate(
            test_data.reset_index(drop=True),
        )

    def fillna(self, value: Any) -> "DataLoader":
        self.data = self.data.fillna(value)
        return self

    def is_tabular(self) -> bool:
        return True


class TimeSeriesDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.TimeSeriesDataLoader
        :parts: 1

    Data Loader for Time Series Data

    Constructor Args:
        temporal data: List[pd.DataFrame]
            The temporal data. A list of pandas DataFrames
        observation times: List
            List of arrays mapping directly to index of each dataframe in temporal_data
        outcome: Optional[pd.DataFrame] = None
            pandas DataFrame thatn can be anything (eg, labels, regression outcome)
        static_data: Optional[pd.DataFrame] = None
            pandas DataFrame mapping directly to index of each dataframe in temporal_data
        sensitive_features: List[str]
            Name of sensitive features
        important_features List[str]
            Default: None. Only relevant for SurvivalGAN method
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        random_state: int
            Defaults to zero.

    Example:
        >>> TODO
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame] = None,
        static_data: Optional[pd.DataFrame] = None,
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        seq_offset: int = 0,
        **kwargs: Any,
    ) -> None:
        static_features = []
        self.outcome_features = []

        if len(temporal_data) == 0:
            raise ValueError("Empty temporal data")

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        max_window_len = max([len(t) for t in temporal_data])
        if static_data is not None:
            if len(static_data) != len(temporal_data):
                raise ValueError("Static and temporal data mismatch")
            static_features = list(static_data.columns)
        else:
            static_data = pd.DataFrame(np.zeros((len(temporal_data), 0)))

        if outcome is not None:
            if len(outcome) != len(temporal_data):
                raise ValueError("Temporal and outcome data mismatch")
            self.outcome_features = list(outcome.columns)
        else:
            outcome = pd.DataFrame(np.zeros((len(temporal_data), 0)))

        self.window_len = max_window_len
        self.fill = np.nan
        self.seq_offset = seq_offset

        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
            seq_df,
            seq_info,
        ) = TimeSeriesDataLoader.pack_raw_data(
            static_data,
            temporal_data,
            observation_times,
            outcome,
            fill=self.fill,
            seq_offset=seq_offset,
        )
        self.seq_info = seq_info
        self.fairness_column = fairness_column

        super().__init__(
            data={
                "static_data": static_data,
                "temporal_data": temporal_data,
                "observation_times": observation_times,
                "outcome": outcome,
                "seq_data": seq_df,
            },
            data_type="time_series",
            static_features=static_features,
            temporal_features=temporal_features,
            outcome_features=self.outcome_features,
            sensitive_features=sensitive_features,
            important_features=important_features,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data["seq_data"].shape

    @property
    def columns(self) -> list:
        return self.data["seq_data"].columns

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def compression_protected_features(self) -> list:
        return self.outcome_features

    @property
    def raw_columns(self) -> list:
        return self.static_features + self.temporal_features + self.outcome_features

    def dataframe(self) -> pd.DataFrame:
        return self.data["seq_data"].copy()

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        generic_info = {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": self.static_features,
            "temporal_features": self.temporal_features,
            "outcome_features": self.outcome_features,
            "outcome_len": len(self.data["outcome"].values.reshape(-1))
            / len(self.data["outcome"]),
            "window_len": self.window_len,
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "fairness_column": self.fairness_column,
            "random_state": self.random_state,
            "train_size": self.train_size,
            "fill": self.fill,
        }

        for key in self.seq_info:
            generic_info[key] = self.seq_info[key]

        return generic_info

    def __len__(self) -> int:
        return len(self.data["seq_data"])

    def decorate(self, data: Any) -> "DataLoader":
        static_data, temporal_data, observation_times, outcome = data

        return TimeSeriesDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            outcome=outcome,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            fairness_column=self.fairness_column,
            random_state=self.random_state,
            train_size=self.train_size,
            seq_offset=self.seq_offset,
        )

    def unpack_and_decorate(self, data: pd.DataFrame) -> "DataLoader":
        unpacked_data = TimeSeriesDataLoader.unpack_raw_data(
            data,
            self.info(),
        )

        return self.decorate(unpacked_data)

    def satisfies(self, constraints: Constraints) -> bool:
        seq_df = self.dataframe()

        return constraints.is_valid(seq_df)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.unpack_and_decorate(
            constraints.match(self.dataframe()),
        )

    def drop(self, columns: list = []) -> "DataLoader":
        new_data = self.data["seq_data"].drop(columns=columns)
        return self.unpack_and_decorate(new_data)

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.unpack_raw_data(
            data,
            info,
        )
        return TimeSeriesDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            outcome=outcome,
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            fairness_column=info["fairness_column"],
            fill=info["fill"],
            seq_offset=info["seq_offset"],
        )

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if pad:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_and_mask(
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )
        else:
            static_data, temporal_data, observation_times, outcome = (
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )

        if as_numpy:
            longest_observation_seq = max([len(seq) for seq in temporal_data])
            padded_temporal_data = np.zeros(
                (len(temporal_data), longest_observation_seq, 5)
            )
            mask = np.ones((len(temporal_data), longest_observation_seq, 5), dtype=bool)
            for i, arr in enumerate(temporal_data):
                padded_temporal_data[i, : arr.shape[0], :] = arr  # Copy the actual data
                mask[
                    i, : arr.shape[0], :
                ] = False  # Set mask to False where actual data is present

            masked_temporal_data = ma.masked_array(padded_temporal_data, mask)
            return (
                np.asarray(static_data),
                masked_temporal_data,  # TODO: check this works with time series benchmarks
                # masked array to handle variable length sequences
                ma.vstack(
                    [
                        ma.array(
                            np.resize(ot, longest_observation_seq),
                            mask=[True for i in range(len(ot))]
                            + [False for j in range(longest_observation_seq - len(ot))],
                        )
                        for ot in observation_times
                    ]
                ),
                np.asarray(outcome),
            )
        return (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data["seq_data"][feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data["seq_data"][feature] = val

    def ids(self) -> list:
        id_col = self.seq_info["seq_id_feature"]
        ids = self.data["seq_data"][id_col]

        return list(ids.unique())

    def filter_ids(self, ids_list: list) -> pd.DataFrame:
        seq_data = self.data["seq_data"]
        id_col = self.info()["seq_id_feature"]

        return seq_data[seq_data[id_col].isin(ids_list)]

    def train(self) -> "DataLoader":
        # TODO: stratify
        ids = self.ids()
        train_ids, _ = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
        )
        return self.unpack_and_decorate(self.filter_ids(train_ids))

    def test(self) -> "DataLoader":
        # TODO: stratify
        ids = self.ids()
        _, test_ids = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
        )
        return self.unpack_and_decorate(self.filter_ids(test_ids))

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        ids = self.ids()
        count = min(count, len(ids))
        sampled_ids = random.sample(ids, count)

        return self.unpack_and_decorate(self.filter_ids(sampled_ids))

    def fillna(self, value: Any) -> "DataLoader":
        for key in ["static_data", "outcome", "seq_data"]:
            if self.data[key] is not None:
                self.data[key] = self.data[key].fillna(value)

        for idx, item in enumerate(self.data["temporal_data"]):
            self.data["temporal_data"][idx] = self.data["temporal_data"][idx].fillna(
                value
            )

        return self

    @staticmethod
    def unique_temporal_features(temporal_data: List[pd.DataFrame]) -> List:
        temporal_features = []
        for item in temporal_data:
            temporal_features.extend(item.columns)
        return sorted(np.unique(temporal_features).tolist())

    # Padding helpers
    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_raw_features(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
    ) -> Any:
        fill = np.nan

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        for idx, item in enumerate(temporal_data):
            # handling missing features
            for col in temporal_features:
                if col not in item.columns:
                    item[col] = fill
            item = item[temporal_features]

            if list(item.columns) != list(temporal_features):
                raise RuntimeError("Invalid features for packing")

            temporal_data[idx] = item.fillna(fill)

        return static_data, temporal_data, observation_times, outcome

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_raw_data(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
    ) -> Any:
        fill = np.nan

        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.pad_raw_features(
            static_data, temporal_data, observation_times, outcome
        )
        max_window_len = max([len(t) for t in temporal_data])
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        for idx, item in enumerate(temporal_data):
            if len(item) != max_window_len:
                pads = fill * np.ones(
                    (max_window_len - len(item), len(temporal_features))
                )
                start = 0
                if len(item.index) > 0:
                    start = max(item.index) + 1
                pads_df = pd.DataFrame(
                    pads,
                    index=[start + i for i in range(len(pads))],
                    columns=item.columns,
                )
                item = pd.concat([item, pads_df])

            # handle missing time points
            if list(item.columns) != list(temporal_features):
                raise RuntimeError(
                    f"Invalid features {item.columns}. Expected {temporal_features}"
                )
            if len(item) != max_window_len:
                raise RuntimeError("Invalid window len")

            temporal_data[idx] = item

        observation_times_padded = []
        for idx, item in enumerate(observation_times):
            item = list(item)
            if len(item) != max_window_len:
                pads = fill * np.ones(max_window_len - len(item))
                item.extend(pads.tolist())
            observation_times_padded.append(item)

        return static_data, temporal_data, observation_times_padded, outcome

    # Masking helpers
    @staticmethod
    def extract_masked_features(full_temporal_features: list) -> tuple:
        temporal_features = []
        mask_features = []
        mask_prefix = "masked_"
        for feat in full_temporal_features:
            feat = str(feat)
            if not feat.startswith(mask_prefix):
                temporal_features.append(feat)
                continue

            other_feat = feat[len(mask_prefix) :]
            if other_feat in full_temporal_features:
                mask_features.append(feat)
            else:
                temporal_features.append(feat)

        return temporal_features, mask_features

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def mask_temporal_data(
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        fill: Any = 0,
    ) -> Any:
        nan_cnt = 0
        for item in temporal_data:
            nan_cnt += np.asarray(np.isnan(item)).sum()

        if nan_cnt == 0:
            return temporal_data, observation_times

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        masked_features = [f"masked_{feat}" for feat in temporal_features]

        for idx, item in enumerate(temporal_data):
            item[masked_features] = (~np.isnan(item)).astype(int)
            item = item.fillna(fill)
            temporal_data[idx] = item

        for idx, item in enumerate(observation_times):
            item = np.nan_to_num(item, nan=fill).tolist()

            observation_times[idx] = item

        return temporal_data, observation_times

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def unmask_temporal_data(
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        fill: Any = np.nan,
    ) -> Any:
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        temporal_features, mask_features = TimeSeriesDataLoader.extract_masked_features(
            temporal_features
        )

        missing_horizons = []
        for idx, item in enumerate(temporal_data):
            # handle existing mask
            if len(mask_features) > 0:
                mask = temporal_data[idx][mask_features].astype(bool)
                item[~mask] = np.nan

            item_missing_rows = item.isna().sum(axis=1).values
            missing_horizons.append(item_missing_rows == len(temporal_features))

            # TODO: review impact on horizons
            temporal_data[idx] = item.dropna()

        observation_times_unmasked = []
        for idx, item in enumerate(observation_times):
            item = list(item)

            for midx, mval in enumerate(missing_horizons[idx]):
                if mval:
                    item[midx] = np.nan

            local_horizons = list(filter(lambda v: v == v, item))
            observation_times_unmasked.append(local_horizons)

        return temporal_data, observation_times_unmasked

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_and_mask(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        only_features: Any = False,
        fill: Any = 0,
    ) -> Any:
        if only_features:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_raw_features(
                static_data,
                temporal_data,
                observation_times,
                outcome,
            )
        else:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_raw_data(
                static_data,
                temporal_data,
                observation_times,
                outcome,
            )

        temporal_data, observation_times = TimeSeriesDataLoader.mask_temporal_data(
            temporal_data, observation_times, fill=fill
        )

        return static_data, temporal_data, observation_times, outcome

    @staticmethod
    def sequential_view(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        id_col: str = "seq_id",
        time_id_col: str = "seq_time_id",
        seq_offset: int = 0,
    ) -> Tuple[pd.DataFrame, dict]:  # sequential dataframe, loader info
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.pad_and_mask(
            static_data, temporal_data, observation_times, outcome, only_features=True
        )
        raw_static_features = list(static_data.columns)
        static_features = [f"seq_static_{col}" for col in raw_static_features]

        raw_outcome_features = list(outcome.columns)
        outcome_features = [f"seq_out_{col}" for col in raw_outcome_features]

        raw_temporal_features = TimeSeriesDataLoader.unique_temporal_features(
            temporal_data
        )
        temporal_features = [f"seq_temporal_{col}" for col in raw_temporal_features]
        cols = (
            [id_col, time_id_col]
            + static_features
            + temporal_features
            + outcome_features
        )

        seq = []
        for sidx, static_item in static_data.iterrows():
            real_tidx = 0
            for tidx, temporal_item in temporal_data[sidx].iterrows():
                local_seq_data = (
                    [
                        sidx + seq_offset,
                        observation_times[sidx][real_tidx],
                    ]
                    + static_item[raw_static_features].values.tolist()
                    + temporal_item[raw_temporal_features].values.tolist()
                    + outcome.loc[sidx, raw_outcome_features].values.tolist()
                )
                seq.append(local_seq_data)
                real_tidx += 1

        seq_df = pd.DataFrame(seq, columns=cols)
        info = {
            "seq_static_features": static_features,
            "seq_temporal_features": temporal_features,
            "seq_outcome_features": outcome_features,
            "seq_offset": seq_offset,
            "seq_id_feature": id_col,
            "seq_time_id_feature": time_id_col,
            "seq_features": list(seq_df.columns),
        }
        return seq_df, info

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pack_raw_data(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        fill: Any = np.nan,
        seq_offset: int = 0,
    ) -> pd.DataFrame:
        # Temporal data: (subjects, temporal_sequence, temporal_feature)
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        temporal_features, mask_features = TimeSeriesDataLoader.extract_masked_features(
            temporal_features
        )
        temporal_data, observation_times = TimeSeriesDataLoader.unmask_temporal_data(
            temporal_data, observation_times
        )
        seq_df, info = TimeSeriesDataLoader.sequential_view(
            static_data=static_data,
            temporal_data=temporal_data,
            observation_times=observation_times,
            outcome=outcome,
            seq_offset=seq_offset,
        )

        return static_data, temporal_data, observation_times, outcome, seq_df, info

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def unpack_raw_data(
        data: pd.DataFrame,
        info: dict,
    ) -> Tuple[
        Optional[pd.DataFrame], List[pd.DataFrame], List, Optional[pd.DataFrame]
    ]:
        id_col = info["seq_id_feature"]
        time_col = info["seq_time_id_feature"]

        static_cols = info["seq_static_features"]
        new_static_cols = [feat.split("seq_static_")[1] for feat in static_cols]

        temporal_cols = info["seq_temporal_features"]
        new_temporal_cols = [feat.split("seq_temporal_")[1] for feat in temporal_cols]

        outcome_cols = info["seq_outcome_features"]
        new_outcome_cols = [feat.split("seq_out_")[1] for feat in outcome_cols]

        ids = sorted(list(set(data[id_col])))

        static_data = []
        temporal_data = []
        observation_times = []
        outcome_data = []

        for item_id in ids:
            item_data = data[data[id_col] == item_id]

            static_data.append(item_data[static_cols].head(1).values.squeeze().tolist())
            outcome_data.append(
                item_data[outcome_cols].head(1).values.squeeze().tolist()
            )
            local_temporal_data = item_data[temporal_cols].copy()
            local_observation_times = item_data[time_col].values.tolist()
            local_temporal_data.columns = new_temporal_cols
            # TODO: review impact on horizons
            local_temporal_data = local_temporal_data.dropna()

            temporal_data.append(local_temporal_data)
            observation_times.append(local_observation_times)

        static_df = pd.DataFrame(static_data, columns=new_static_cols)
        outcome_df = pd.DataFrame(outcome_data, columns=new_outcome_cols)

        return static_df, temporal_data, observation_times, outcome_df

    def is_tabular(self) -> bool:
        return True


class TimeSeriesSurvivalDataLoader(TimeSeriesDataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.TimeSeriesSurvivalDataLoader
        :parts: 1

    Data loader for Time series survival data

    Constructor Args:
        temporal_data: List[pd.DataFrame}
            The temporal data. A list of pandas DataFrames.
        observation_times: List
            List of arrays mapping directly to index of each dataframe in temporal_data
        T: Union[pd.Series, np.ndarray, pd.Series]
            Time-to-event data
        E: Union[pd.Series, np.ndarray, pd.Series]
            E is censored/event data
        static_data Optional[pd.DataFrame] = None
            pandas DataFrame of static features for each subject
        sensitive_features: List[str]
            Name of sensitive features
        important_features: List[str}
            Default: None. Only relevant for SurvivalGAN method.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        random_state. int
            Defaults to zero.

    Example:
        >>> TODO

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: Union[List, np.ndarray, pd.Series],
        T: Union[pd.Series, np.ndarray, pd.Series],
        E: Union[pd.Series, np.ndarray, pd.Series],
        static_data: Optional[pd.DataFrame] = None,
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        time_horizons: list = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        seq_offset: int = 0,
        **kwargs: Any,
    ) -> None:
        self.time_to_event_col = "time_to_event"
        self.event_col = "event"
        self.fairness_column = fairness_column

        if len(time_horizons) == 0:
            time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1].tolist()
        self.time_horizons = time_horizons
        outcome = pd.concat([pd.Series(T), pd.Series(E)], axis=1)
        outcome.columns = [self.time_to_event_col, self.event_col]

        self.fill = np.nan

        super().__init__(
            temporal_data=temporal_data,
            observation_times=observation_times,
            outcome=outcome,
            static_data=static_data,
            sensitive_features=sensitive_features,
            important_features=important_features,
            random_state=random_state,
            train_size=train_size,
            seq_offset=seq_offset,
            **kwargs,
        )
        self.data_type = "time_series_survival"

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def info(self) -> dict:
        parent_info = super().info()
        parent_info["time_to_event_column"] = self.time_to_event_col
        parent_info["event_column"] = self.event_col
        parent_info["time_horizons"] = self.time_horizons
        parent_info["fill"] = self.fill

        return parent_info

    def decorate(self, data: Any) -> "DataLoader":
        static_data, temporal_data, observation_times, outcome = data
        if self.time_to_event_col not in outcome:
            raise ValueError(
                f"Survival outcome is missing tte column {self.time_to_event_col}"
            )
        if self.event_col not in outcome:
            raise ValueError(
                f"Survival outcome is missing event column {self.event_col}"
            )

        return TimeSeriesSurvivalDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            T=outcome[self.time_to_event_col],
            E=outcome[self.event_col],
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            fairness_column=self.fairness_column,
            random_state=self.random_state,
            time_horizons=self.time_horizons,
            train_size=self.train_size,
            seq_offset=self.seq_offset,
        )

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesSurvivalDataLoader.unpack_raw_data(
            data,
            info,
        )
        return TimeSeriesSurvivalDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            T=outcome[info["time_to_event_column"]],
            E=outcome[info["event_column"]],
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            fairness_column=info["fairness_column"],
            time_horizons=info["time_horizons"],
            seq_offset=info["seq_offset"],
        )

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if pad:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesSurvivalDataLoader.pad_and_mask(
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )
        else:
            static_data, temporal_data, observation_times, outcome = (
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )

        if as_numpy:
            return (
                np.asarray(static_data),
                np.asarray(temporal_data, dtype=object),
                np.asarray(observation_times, dtype=object),
                np.asarray(outcome[self.time_to_event_col]),
                np.asarray(outcome[self.event_col]),
            )
        return (
            static_data,
            temporal_data,
            observation_times,
            outcome[self.time_to_event_col],
            outcome[self.event_col],
        )

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.unpack_and_decorate(
            constraints.match(self.dataframe()),
        )

    def train(self) -> "DataLoader":
        stratify = self.data["outcome"][self.event_col]

        ids = self.ids()
        train_ids, _ = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )
        return self.unpack_and_decorate(self.filter_ids(train_ids))

    def test(self) -> "DataLoader":
        stratify = self.data["outcome"][self.event_col]
        ids = self.ids()
        _, test_ids = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )
        return self.unpack_and_decorate(self.filter_ids(test_ids))


class ImageDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.ImageDataLoader
        :parts: 1

    Data loader for generic image data.

    Constructor Args:
        data: torch.utils.data.Dataset or torch.Tensor
            The image dataset or a tuple of (tensor images, tensor labels)
        random_state: int
            Defaults to zero.
        height: int. Default = 32
            Height to use internally
        width: Optional[int]
            Optional width to use internally. If None, it is used the same value as height.
        train_size: float = 0.8
            Train dataset ratio.
    Example:
        >>> dataset = datasets.MNIST(".", download=True)
        >>>
        >>> loader = ImageDataLoader(
        >>>     data=dataset,
        >>>     train_size=0.8,
        >>>     height=32,
        >>>     width=w32,
        >>> )

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: Union[torch.utils.data.Dataset, Tuple[torch.Tensor, torch.Tensor]],
        height: int = 32,
        width: Optional[int] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if width is None:
            width = height

        if isinstance(data, tuple):
            X, y = data
            data = TensorDataset(images=X, targets=y)

        self.data_transform = None

        dummy, _ = data[0]
        img_transform = []
        if not isinstance(dummy, PIL.Image.Image):
            img_transform = [transforms.ToPILImage()]

        img_transform.extend(
            [
                transforms.Resize((height, width)),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.5,), std=(0.5,)),
            ]
        )

        self.data_transform = transforms.Compose(img_transform)
        data = FlexibleDataset(data, transform=self.data_transform)

        self.height = height
        self.width = width
        self.channels = data.shape()[1]

        super().__init__(
            data_type="images",
            data=data,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape()

    def get_fairness_column(self) -> None:
        """Not implemented for ImageDataLoader"""
        ...

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        return self.data

    def numpy(self) -> np.ndarray:
        x, _ = self.data.numpy()

        return x

    def dataframe(self) -> pd.DataFrame:
        x = self.numpy().reshape(len(self), -1)

        x = pd.DataFrame(x)
        x.columns = x.columns.astype(str)

        return x

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "train_size": self.train_size,
            "height": self.height,
            "width": self.width,
            "channels": self.channels,
            "random_state": self.random_state,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return ImageDataLoader(
            data,
            random_state=self.random_state,
            train_size=self.train_size,
            height=self.height,
            width=self.width,
        )

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        idxs = np.random.choice(len(self), count, replace=False)
        subset = FlexibleDataset(self.data.data, indices=idxs)
        return self.decorate(subset)

    @staticmethod
    def from_info(data: torch.utils.data.Dataset, info: dict) -> "ImageDataLoader":
        if not isinstance(data, torch.utils.data.Dataset):
            raise ValueError(f"Invalid data type {type(data)}")

        return ImageDataLoader(
            data,
            train_size=info["train_size"],
            height=info["height"],
            width=info["width"],
            random_state=info["random_state"],
        )

    def __getitem__(self, index: Union[list, int, str]) -> Any:
        if isinstance(index, str):
            return self.dataframe()[index]

        return self.numpy()[index]

    def _train_test_split(self) -> Tuple:
        indices = np.arange(len(self.data))
        _, stratify = self.data.numpy()

        return train_test_split(
            indices,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )

    def train(self) -> "DataLoader":
        train_idx, _ = self._train_test_split()
        subset = FlexibleDataset(self.data.data, indices=train_idx)
        return self.decorate(subset)

    def test(self) -> "DataLoader":
        _, test_idx = self._train_test_split()
        subset = FlexibleDataset(self.data.data, indices=test_idx)
        return self.decorate(subset)

    def compress(
        self,
    ) -> Tuple["DataLoader", Dict]:
        return self, {}

    def decompress(self, context: Dict) -> "DataLoader":
        return self

    def encode(
        self,
        encoders: Optional[Dict[str, Any]] = None,
    ) -> Tuple["DataLoader", Dict]:
        return self, {}

    def decode(
        self,
        encoders: Dict[str, Any],
    ) -> "DataLoader":
        return self

    def is_tabular(self) -> bool:
        return False

    @property
    def columns(self) -> list:
        return list(self.dataframe().columns)

    def satisfies(self, constraints: Constraints) -> bool:
        return True

    def match(self, constraints: Constraints) -> "DataLoader":
        return self

    def compression_protected_features(self) -> list:
        raise NotImplementedError("Images do not support the compression call")

    def drop(self, columns: list = []) -> "DataLoader":
        raise NotImplementedError()

    def __setitem__(self, feature: str, val: Any) -> None:
        raise NotImplementedError()

    def fillna(self, value: Any) -> "DataLoader":
        raise NotImplementedError()



@validate_arguments(config=dict(arbitrary_types_allowed=True))
def create_from_info(
    data: Union[pd.DataFrame, torch.utils.data.Dataset], info: dict
) -> "DataLoader":
    """Helper for creating a DataLoader from existing information."""
    if info["data_type"] == "generic":
        return GenericDataLoader.from_info(data, info)
    elif info["data_type"] == "survival_analysis":
        return SurvivalAnalysisDataLoader.from_info(data, info)
    elif info["data_type"] == "time_series":
        return TimeSeriesDataLoader.from_info(data, info)
    elif info["data_type"] == "time_series_survival":
        return TimeSeriesSurvivalDataLoader.from_info(data, info)
    elif info["data_type"] == "images":
        return ImageDataLoader.from_info(data, info)
    else:
        raise RuntimeError(f"invalid datatype {info}")


class Syn_SeqDataLoader(DataLoader):
    """
    A DataLoader that applies Syn_Seq-style preprocessing to input data,
    inheriting directly from DataLoader and implementing all required
    abstract methods.

    - syn_order: The order of columns to keep or process. If not provided (None or empty),
                 use the raw DataFrame column order.
    - columns_special_values: A dict of { column_name : list_of_special_values },
      specifying which values to treat as 'special' or missing in numeric columns.
    - col_type: A dict { column_name : "category"/"numeric"/"date"/... } used to force
      how each column is handled.
    - max_categories: numeric threshold for deciding numeric vs. categorical if col_type
      isn't specified.
    """

    def __init__(
        self,
        data: pd.DataFrame,
        syn_order: Optional[List[str]] = None,
        special_value: Optional[Dict[str, List[Any]]] = None,
        col_type: Optional[Dict[str, str]] = None,
        max_categories: int = 20,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        """
        Args:
            data: the raw DataFrame
            syn_order: optional list of columns in the desired processing order
            special_value: { "col_name": [list_of_special_values], ... }
            col_type: { "col_name": "category"/"numeric"/"date"/... }
            max_categories: threshold for deciding numeric vs. categorical if not declared
            random_state: for reproducibility
            train_size: fraction of data for 'train' (rest goes to 'test')
        """
        if not syn_order:
            print("[INFO] syn_order not provided; using data.columns as default.")
            syn_order = list(data.columns)

        # ensure all requested columns exist
        missing_columns = set(syn_order) - set(data.columns)
        if missing_columns:
            raise ValueError(f"Missing columns in input data: {missing_columns}")

        # store user parameters
        self.syn_order = syn_order
        self.columns_special_values = special_value or {}
        self.col_type = col_type or {}
        self.max_categories = max_categories

        # reorder data based on syn_order
        filtered_data = data[self.syn_order].copy()

        # call parent constructor
        super().__init__(
            data_type="syn_seq",
            data=filtered_data,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

        # keep a reference
        self._df = filtered_data

        # debug print
        print("[INFO] Syn_SeqDataLoader init complete:")
        print(f"  - syn_order: {self.syn_order}")
        print(f"  - special_value (columns_special_values): {self.columns_special_values}")
        print(f"  - col_type: {self.col_type}")
        print(f"  - data shape: {self._df.shape}")

        # create + fit our Syn_SeqEncoder (just fit, actual transform is in encode())
        self._encoder = Syn_SeqEncoder(
            columns_special_values=self.columns_special_values,
            syn_order=self.syn_order,
            max_categories=self.max_categories,
            col_type=self.col_type,
        )
        self._encoder.fit(self._df)

        print("[DEBUG] After encoder.fit(), detected info:")
        print(f"  - encoder.column_order_: {self._encoder.column_order_}")
        print(f"  - numeric_info_: {self._encoder.numeric_info_}")
        print(f"  - categorical_info_: {self._encoder.categorical_info_}")
        if self._encoder.variable_selection_ is not None:
            print("  - variable_selection_:\n", self._encoder.variable_selection_)
        print("----------------------------------------------------------------")

    # ----------------------------------------------------------------
    # Inherited/required abstract methods
    # ----------------------------------------------------------------
    @property
    def shape(self) -> tuple:
        return self._df.shape

    @property
    def columns(self) -> list:
        return list(self._df.columns)

    def dataframe(self) -> pd.DataFrame:
        return self._df

    def numpy(self) -> pd.DataFrame:
        return self._df.values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "train_size": self.train_size,
            "random_state": self.random_state,
            "syn_order": self.syn_order,
            "max_categories": self.max_categories,
            "col_type": self.col_type,
            "columns_special_values": self.columns_special_values,
        }

    def __len__(self) -> int:
        return len(self._df)

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self._df)

    def match(self, constraints: Constraints) -> "Syn_SeqDataLoader":
        matched_df = constraints.match(self._df)
        return self.decorate(matched_df)

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "Syn_SeqDataLoader":
        return Syn_SeqDataLoader(
            data=data,
            syn_order=info.get("syn_order"),
            special_value=info.get("columns_special_values", {}),
            col_type=info.get("col_type", {}),
            max_categories=info.get("max_categories", 20),
            random_state=info["random_state"],
            train_size=info["train_size"],
        )

    def sample(self, count: int, random_state: int = 0) -> "Syn_SeqDataLoader":
        sampled_df = self._df.sample(count, random_state=random_state)
        return self.decorate(sampled_df)

    def drop(self, columns: list = []) -> "Syn_SeqDataLoader":
        dropped_df = self._df.drop(columns=columns, errors="ignore")
        return self.decorate(dropped_df)

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self._df[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self._df[feature] = val

    def train(self) -> "Syn_SeqDataLoader":
        ntrain = int(len(self._df) * self.train_size)
        train_df = self._df.iloc[:ntrain].copy()
        return self.decorate(train_df)

    def test(self) -> "Syn_SeqDataLoader":
        ntrain = int(len(self._df) * self.train_size)
        test_df = self._df.iloc[ntrain:].copy()
        return self.decorate(test_df)

    def fillna(self, value: Any) -> "Syn_SeqDataLoader":
        filled_df = self._df.fillna(value)
        return self.decorate(filled_df)

    def compression_protected_features(self) -> list:
        return []

    def is_tabular(self) -> bool:
        return True

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if as_numpy:
            return self._df.to_numpy()
        return self._df

    def get_fairness_column(self) -> Union[str, Any]:
        return None

    # ----------------------------------------------------------------
    # Syn_Seq-specific encode/decode
    # ----------------------------------------------------------------
    def encode(
        self, encoders: Optional[Dict[str, Any]] = None
    ) -> Tuple["Syn_SeqDataLoader", Dict]:
        if encoders is None:
            encoded_data = self._encoder.transform(self._df)
            new_loader = self.decorate(encoded_data)
            return new_loader, {"syn_seq_encoder": self._encoder}
        else:
            return self, encoders

    def decode(self, encoders: Dict[str, Any]) -> "Syn_SeqDataLoader":
        if "syn_seq_encoder" in encoders:
            encoder = encoders["syn_seq_encoder"]
            if not isinstance(encoder, Syn_SeqEncoder):
                raise TypeError(f"Expected Syn_SeqEncoder, got {type(encoder)}")
            decoded_data = encoder.inverse_transform(self._df)
            return self.decorate(decoded_data)
        return self

    def decorate(self, data: pd.DataFrame) -> "Syn_SeqDataLoader":
        """
        Helper for creating a new instance with the same settings but new data.
        """
        return Syn_SeqDataLoader(
            data=data,
            syn_order=self.syn_order,
            special_value=self.columns_special_values,
            col_type=self.col_type,
            max_categories=self.max_categories,
            random_state=self.random_state,
            train_size=self.train_size,
        )


src/synthcity/plugins/generic/plugin_syn_seq.py
# File: plugin_syn_seq.py

from typing import Any, Dict, List, Optional, Union

import pandas as pd
import numpy as np

# synthcity absolute
from synthcity.plugins.core.plugin import Plugin
from synthcity.plugins.core.dataloader import DataLoader, Syn_SeqDataLoader
from synthcity.plugins.core.schema import Schema
from synthcity.plugins.core.constraints import Constraints

# local aggregator
from synthcity.plugins.core.models.syn_seq.syn_seq import Syn_Seq


class Syn_SeqPlugin(Plugin):
    """
    A plugin for a sequential (column-by-column) synthetic data approach,
    mirroring R's 'synthpop'. Internally, it delegates to the `Syn_Seq`
    aggregator from syn_seq.py for the actual column-by-column (sequential) logic.

    - This is intended for sequential regression style: each column is modeled
      in the order, using prior columns as predictors.
    - We rely on the custom `Syn_SeqDataLoader`, which organizes columns,
      applies a Syn_SeqEncoder, etc.
    - The aggregator's generate(...) returns a Syn_SeqDataLoader as well.

    Basic usage:

        # Suppose we have df, and we wrap it in a Syn_SeqDataLoader:
        loader = Syn_SeqDataLoader(
            data = df, 
            syn_order = [...],
            col_type = {...},
            special_value = {...},
        )

        # Build plugin
        syn_model = Syn_SeqPlugin(
            random_state=42,
            default_first_method="SWR", 
            default_other_method="CART",
            # optionally strict=True, sampling_patience=500, ...
        )

        # Fit with per-column method
        methods = ["SWR"] + ["CART"]*(len(loader.columns)-1)
        var_sel = {"N2": ["C1","C2"], "N1":["C1","C2","N2"]}

        syn_model.fit(loader, method=methods, variable_selection=var_sel)

        # Now generate
        synthetic_data_loader = syn_model.generate(count=100)
        synthetic_df = synthetic_data_loader.dataframe()

        # If you need constraints:
        constraints = {
          "N1": [">", 100],
          "C2": ["in", ["A","B"]]
        }
        synthetic_data_loader = syn_model.generate(count=100, constraints=constraints)
        # Above remains a Syn_SeqDataLoader
    """

    @staticmethod
    def name() -> str:
        return "syn_seq"

    @staticmethod
    def type() -> str:
        return "syn_seq"

    @staticmethod
    def hyperparameter_space(**kwargs: Any) -> list:
        # No tunable hyperparameters for now
        return []

    def __init__(
        self,
        random_state: int = 0,
        default_first_method: str = "SWR",
        default_other_method: str = "CART",
        **kwargs: Any,
    ):
        """
        Args:
            random_state: For reproducibility.
            default_first_method: fallback method for the first column if user doesn't override.
            default_other_method: fallback method for subsequent columns.
            **kwargs: forwarded to Plugin(...) => can contain strict, sampling_patience, etc.
        """
        super().__init__(random_state=random_state, **kwargs)

        # We hold a Syn_Seq aggregator. This aggregator does the real sequential logic:
        self._aggregator = Syn_Seq(
            random_state=random_state,
            default_first_method=default_first_method,
            default_other_method=default_other_method,
            strict=self.strict,
            sampling_patience=self.sampling_patience,
        )
        self._model_trained = False

    def _fit(
        self,
        X: DataLoader,
        method: Optional[List[str]] = None,
        variable_selection: Optional[Dict[str, List[str]]] = None,
        *args: Any,
        **kwargs: Any
    ) -> "Syn_SeqPlugin":
        """
        We expect X to be a Syn_SeqDataLoader for sequential usage.
        We pass 'method' and 'variable_selection' to the aggregator.
        """
        if not isinstance(X, Syn_SeqDataLoader):
            raise TypeError("Syn_SeqPlugin expects a Syn_SeqDataLoader for sequential usage.")

        self._aggregator.fit(
            loader=X,
            method=method,
            variable_selection=variable_selection,
            *args,
            **kwargs
        )
        self._model_trained = True
        return self

    def _generate(
        self,
        count: int,
        syn_schema: Schema,
        constraints: Optional[Constraints] = None,
        **kwargs: Any
    ) -> DataLoader:
        """
        Create synthetic data as a Syn_SeqDataLoader. We pass optional constraints
        (which might be a dictionary or a Constraints object) to aggregator.
        """
        if not self._model_trained:
            raise RuntimeError("Must fit Syn_SeqPlugin before generating data.")

        return self._aggregator.generate(
            count=count,
            constraint=constraints,
            **kwargs
        )

plugin = Syn_SeqPlugin


