INSTRUCTION:
Hey guys, our captain(main model) and I have discussed some changes in syn_seq.py, base model. There are some major modifications.
Revise your parts and generate the code if there's modification to be made. Also, we have new member of the team, 'syn_seq_constraints.py'. Please welcome him. If you don't have anything to change, just tell me there's nothing to change at the moment.
Thanks.

CURRENT CODE AGGREGATED
src/synthcity/plugins/core/models/syn_seq_encoder.py
from typing import Optional, Dict, Any, List, Union
import pandas as pd
import numpy as np
from sklearn.base import TransformerMixin, BaseEstimator

# [ADDED/CHANGED] We'll import datetime conversion if needed
from synthcity.plugins.core.models.feature_encoder import DatetimeEncoder

class Syn_SeqEncoder(TransformerMixin, BaseEstimator):
    """
    Syn_SeqEncoder handles preprocessing and postprocessing tasks using fit/transform pattern,
    plus manages a 'variable_selection' matrix (like a prediction matrix).
    """

    def __init__(
        self,
        columns_special_values: Optional[Dict[str, Any]] = None,
        syn_order: Optional[List[str]] = None,
        max_categories: int = 20,
        user_variable_selection: Optional[pd.DataFrame] = None,
        # [ADDED/CHANGED] user-declared column types (e.g. {"C1":"category","N1":"numeric","D1":"date"})
        col_type: Optional[Dict[str, str]] = None,
    ) -> None:
        """
        Args:
            columns_special_values : {colName : [specialVals...]}
            syn_order : optional column order to use
            max_categories : threshold for deciding numeric vs categorical
            user_variable_selection : optional DataFrame(n x n) with row=target, col=predictor
            col_type : user-declared column types, e.g. {"C2":"category","N1":"numeric","D1":"date"}
        """
        self.columns_special_values = columns_special_values or {}
        self.syn_order = syn_order or []
        self.max_categories = max_categories
        self.user_variable_selection = user_variable_selection

        # track info about columns
        self.categorical_info_: Dict[str, Dict[str, Any]] = {}
        self.numeric_info_: Dict[str, Dict[str, Any]] = {}
        # [ADDED/CHANGED] We'll track date columns separately
        self.date_info_: Dict[str, Dict[str, Any]] = {}

        self.column_order_: List[str] = []
        self.method_assignments: Dict[str, str] = {}
        self.variable_selection_: Optional[pd.DataFrame] = None

        # [ADDED/CHANGED] store user col_type
        self.col_type = col_type or {}

    def fit(self, X: pd.DataFrame, y=None) -> "Syn_SeqEncoder":
        # copy X to avoid side effects
        X = X.copy()

        self._detect_column_order(X)
        self._detect_col_types(X)
        self._detect_special_values(X)
        self._build_variable_selection_matrix(X)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        # 1) reorder columns
        X = self._reorder_columns(X)
        # 2) split numeric => numeric + numeric_cat
        X = self._split_numeric_cols(X)
        # 3) update dtypes (including date if declared)
        X = self._update_column_types(X)
        # 4) assign placeholder methods
        X = self._assign_methods(X)
        # 5) update variable_selection for newly created columns
        self._update_variable_selection_after_split(X)

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        # 1) restore special values for numeric/date
        for col, vals in self.columns_special_values.items():
            if col in X.columns:
                # if a single special value, use it directly
                if isinstance(vals, list) and len(vals) == 1:
                    vals = vals[0]
                X[col] = X[col].replace(pd.NA, vals)

        # 2) restore dtype for categorical
        for col, info in self.categorical_info_.items():
            if col in X.columns:
                X[col] = X[col].astype(info["dtype"])

        # 3) restore date columns
        for col, info in self.date_info_.items():
            if col in X.columns:
                X[col] = pd.to_datetime(X[col], errors="coerce")

        # 4) restore numeric
        for col, info in self.numeric_info_.items():
            if col in X.columns:
                X[col] = X[col].astype(info["dtype"])

        return X

    # -----------------------------------------------------
    # variable_selection building
    # -----------------------------------------------------
    def _build_variable_selection_matrix(self, X: pd.DataFrame) -> None:
        """
        If user provided a variable_selection DataFrame, validate it. Otherwise,
        build a default (row=target, columns=all prior columns).
        """
        n = len(self.column_order_)
        df_cols = self.column_order_

        if self.user_variable_selection is not None:
            vs = self.user_variable_selection.copy()
            if vs.shape != (n, n):
                raise ValueError(
                    f"user_variable_selection must be shape ({n},{n}), got {vs.shape}"
                )
            # check row/col alignment
            if list(vs.index) != df_cols or list(vs.columns) != df_cols:
                raise ValueError(
                    "Mismatch in user_variable_selection index/columns vs syn_order."
                )
            self.variable_selection_ = vs
        else:
            # default: row=target col=predictor => row i uses columns [0..i-1]
            vs = pd.DataFrame(0, index=df_cols, columns=df_cols)
            for i in range(n):
                for j in range(i):
                    vs.iat[i, j] = 1
            self.variable_selection_ = vs

    def _update_variable_selection_after_split(self, X: pd.DataFrame) -> None:
        """
        If numeric columns were split (col => col + col_cat), we add the new
        columns into the variable_selection matrix. They replicate the row/col
        from the original but don't self-predict.
        """
        if self.variable_selection_ is None:
            return
        old_vs = self.variable_selection_
        old_rows = old_vs.index.tolist()
        old_cols = old_vs.columns.tolist()

        final_cols = list(X.columns)
        new_cols = [c for c in final_cols if c not in old_rows]
        if not new_cols:
            return

        vs_new = pd.DataFrame(0, index=old_rows + new_cols, columns=old_cols + new_cols)

        # copy old content
        for r in old_rows:
            for c in old_cols:
                vs_new.at[r, c] = old_vs.at[r, c]

        # handle new splitted columns
        for c_new in new_cols:
            if c_new.endswith("_cat"):
                c_base = c_new[:-4]  # e.g. "age_cat" => "age"
                if c_base in vs_new.index and c_base in vs_new.columns:
                    # copy entire row from base
                    for c2 in old_cols:
                        vs_new.at[c_new, c2] = vs_new.at[c_base, c2]
                    # copy entire column from base
                    for r2 in old_rows:
                        vs_new.at[r2, c_new] = vs_new.at[r2, c_base]

                vs_new.at[c_new, c_new] = 0  # new col doesn't predict itself
            else:
                # brand new col => remain all zeros
                pass

        self.variable_selection_ = vs_new

    # -----------------------------------------------------
    # HELPER sub-routines
    # -----------------------------------------------------
    def _detect_column_order(self, X: pd.DataFrame):
        """
        If user gave syn_order, we filter columns. Otherwise, use X.columns.
        """
        if self.syn_order:
            self.column_order_ = [c for c in self.syn_order if c in X.columns]
        else:
            self.column_order_ = list(X.columns)

    def _detect_col_types(self, X: pd.DataFrame):
        """
        For each column, decide if numeric/categorical/date, unless
        col_type overrides. Then store in numeric_info_, categorical_info_, date_info_.
        """
        self.numeric_info_.clear()
        self.categorical_info_.clear()
        self.date_info_.clear()

        for col in X.columns:
            declared_type = self.col_type.get(col, "").lower()  # [ADDED/CHANGED]
            if declared_type == "category":
                self.categorical_info_[col] = {"dtype": "category"}
            elif declared_type == "numeric":
                self.numeric_info_[col] = {"dtype": X[col].dtype}
            elif declared_type == "date":
                self.date_info_[col] = {"dtype": "datetime64[ns]"}
            else:
                # fallback auto-detect
                nuniq = X[col].nunique()
                # if #unique <= max_categories => treat as category
                if nuniq > self.max_categories:
                    # might be numeric or date
                    if pd.api.types.is_datetime64_any_dtype(X[col]):
                        self.date_info_[col] = {"dtype": "datetime64[ns]"}
                    else:
                        self.numeric_info_[col] = {"dtype": X[col].dtype}
                else:
                    self.categorical_info_[col] = {"dtype": "category"}

    def _detect_special_values(self, X: pd.DataFrame):
        """
        For each col, if a single value has >90% freq, consider it a "special" value
        and store in columns_special_values. The user can override or supply additional.
        """
        for col in X.columns:
            freq = X[col].value_counts(normalize=True)
            high_vals = freq[freq > 0.9].index.tolist()
            if high_vals:
                existing_vals = self.columns_special_values.get(col, [])
                combined = set(existing_vals).union(set(high_vals))
                self.columns_special_values[col] = list(combined)

    def _reorder_columns(self, X: pd.DataFrame) -> pd.DataFrame:
        if self.column_order_:
            new_cols = [c for c in self.column_order_ if c in X.columns]
            return X[new_cols]
        return X

    def _split_numeric_cols(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        For numeric columns, create an extra col for "special or missing" => col_cat
        Then in col, we set them to NA to ensure they are not used as numeric. 
        """
        for col in list(self.numeric_info_.keys()):
            if col not in X.columns:
                continue
            cat_col = f"{col}_cat"
            special_vals = self.columns_special_values.get(col, [])

            # Mark special or missing in cat_col
            X[cat_col] = X[col].apply(
                lambda x: x if (x in special_vals or pd.isna(x)) else -777
            )
            X[cat_col] = X[cat_col].fillna(-9999)  # placeholder for NA
            # In the numeric column, remove those special/NA
            X[col] = X[col].apply(
                lambda x: x if (x not in special_vals and not pd.isna(x)) else pd.NA
            )
            X[cat_col] = X[cat_col].astype("category")

        return X

    def _update_column_types(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Attempt to cast date columns to datetime,
        numeric to numeric dtype,
        categorical to category dtype.
        """
        # handle date columns first
        for col in self.date_info_:
            if col in X.columns:
                X[col] = pd.to_datetime(X[col], errors="coerce")

        # handle numeric / categorical
        for col in X.columns:
            if col in self.numeric_info_:
                X[col] = X[col].astype(self.numeric_info_[col]["dtype"])
            elif col in self.categorical_info_:
                X[col] = X[col].astype("category")

        return X

    def _assign_methods(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Assign placeholder methods to each column, purely for demonstration
        (the real logic in R's synthpop is more dynamic).
        """
        self.method_assignments.clear()
        first = True
        for c in X.columns:
            if first:
                self.method_assignments[c] = "random_sampling"
                first = False
            else:
                self.method_assignments[c] = "CART"
        return X

    # -----------------------------------------------------
    # user update variable selection
    # -----------------------------------------------------
    @staticmethod
    def update_variable_selection(
        var_sel_df: pd.DataFrame,
        user_dict: Dict[str, List[str]]
    ) -> pd.DataFrame:
        """
        In the variable_selection matrix, set row=target_col => 1 in the predictor columns.
          user_dict = { "D": ["B","C"], "A": ["B"] }
          => row "D", col "B","C" => 1, row "A", col "B" => 1
        """
        for target_col, predictor_list in user_dict.items():
            if target_col not in var_sel_df.index:
                print(f"[WARNING] '{target_col}' not in var_sel_df.index => skipping.")
                continue
            # set row=target_col to 0 first
            var_sel_df.loc[target_col, :] = 0
            # set 1 only for the user-specified predictors
            for pred in predictor_list:
                if pred in var_sel_df.columns:
                    var_sel_df.loc[target_col, pred] = 1
                else:
                    print(f"[WARNING] predictor '{pred}' not in columns => skipping.")
        return var_sel_df


src/synthcity/plugins/core/models/syn_seq.py
# File: syn_seq.py
#
# A self-contained aggregator for the sequential column-by-column approach,
# with no separate _fit or _generate. We unify logic into fit(...) and generate(...).
# We now incorporate the `SynSeqConstraints` (or base Constraints) usage for
# both direct substitution AND row/sequence filtering.

from typing import Any, Dict, List, Optional, Union
import pandas as pd
import numpy as np
from random import sample

# local references
from src.synthcity.plugins.core.dataloader import Syn_SeqDataLoader
from src.synthcity.plugins.core.constraints import Constraints
from src.synthcity.plugins.core.models.syn_seq_constraints import SynSeqConstraints


def _to_synseq_constraints(
    constraint_input: Union[None, Dict[str, List[Any]], Constraints]
) -> Optional[SynSeqConstraints]:
    """
    A helper that converts user-supplied constraints (dict or Constraints)
    into a SynSeqConstraints object.
    
    Example dictionary format:
      {
         "N1": ["=", 999],
         "C2": ["in", ["A","B"]]
      }
    => [("N1","=",999),("C2","in",["A","B"])]

    If the user already has a SynSeqConstraints or base Constraints, we wrap or copy.
    """
    if constraint_input is None:
        return None
    
    if isinstance(constraint_input, Constraints):
        # Already a (Syn)Constraints?
        if isinstance(constraint_input, SynSeqConstraints):
            return constraint_input
        else:
            # Copy its rules into a SynSeqConstraints
            return SynSeqConstraints(rules=constraint_input.rules)
    
    if isinstance(constraint_input, dict):
        # Simple parse: each key => (op, val)
        # e.g. "col" : ["=", 999]
        # if len(...) < 2 => skip
        rules_list = []
        for col, rule_list in constraint_input.items():
            if not isinstance(rule_list, list) or len(rule_list) < 2:
                print(f"[WARNING] Malformed constraint for '{col}' => {rule_list}")
                continue
            op = rule_list[0]
            val = rule_list[1]
            rules_list.append((col, op, val))
        return SynSeqConstraints(rules=rules_list)
    
    raise ValueError(f"Unsupported constraint type: {type(constraint_input)}")


class Syn_Seq:
    """
    The aggregator model for a sequential (column-by-column) synthetic data approach.

    - Provides public .fit(...) and .generate(...) (no separate _fit/_generate).
    - Maintains per-column model info, variable selection, constraints, etc.
    - Integrates `SynSeqConstraints` for direct substitution ('=') and row/sequence filtering.

    Usage:
        aggregator = Syn_Seq(...)
        aggregator.fit(loader, method=[...], variable_selection={...})
        syn_data = aggregator.generate(count=..., constraint={...})
    """

    def __init__(
        self,
        random_state: int = 0,
        default_first_method: str = "SWR",
        default_other_method: str = "CART",
        strict: bool = True,
        sampling_patience: int = 500,
        seq_id_col: str = "seq_id",   # if you want sequence-level constraints
        seq_time_col: str = "seq_time_id",
        **kwargs: Any
    ):
        """
        Args:
            random_state: for reproducibility.
            default_first_method: fallback for the first column if not user-specified.
            default_other_method: fallback for subsequent columns if not user-specified.
            strict: if True, constraints are strictly enforced with repeated tries.
            sampling_patience: how many times we attempt new draws if constraints fail.
            seq_id_col: for sequence-level constraints in SynSeqConstraints.
            seq_time_col: time index column, if needed.
            **kwargs: aggregator-level arguments (unused here).
        """
        self.random_state = random_state
        self.default_first_method = default_first_method
        self.default_other_method = default_other_method
        self.strict = strict
        self.sampling_patience = sampling_patience

        # Per-column model info
        self._column_models: Dict[str, Any] = {}

        # For user-supplied or fallback
        self.method_list: List[str] = []
        self.variable_selection: Dict[str, List[str]] = {}

        # If used by constraints
        self.seq_id_col = seq_id_col
        self.seq_time_col = seq_time_col

        self._model_trained = False

    # ----------------------------------------------------------------
    # fit(...)
    # ----------------------------------------------------------------
    def fit(
        self,
        loader: Syn_SeqDataLoader,
        method: Optional[List[str]] = None,
        variable_selection: Optional[Dict[str, List[str]]] = None,
        *args: Any,
        **kwargs: Any
    ) -> "Syn_Seq":
        """
        Fit a column-by-column model.
         1) Expand user method array if needed
         2) Build variable-selection matrix
         3) Train minimal model for each column
        """
        df = loader.dataframe()
        if df.empty:
            raise ValueError("No data in Syn_SeqDataLoader for training.")

        self.method_list = method or []
        self.variable_selection = variable_selection or {}

        col_list = list(df.columns)
        n_cols = len(col_list)

        # 1) Expand methods if needed
        final_methods = []
        for i, col in enumerate(col_list):
            if i < len(self.method_list):
                final_methods.append(self.method_list[i])
            else:
                # fallback
                fallback = self.default_first_method if i == 0 else self.default_other_method
                final_methods.append(fallback)

        # 2) Build variable_selection matrix
        vs_matrix = pd.DataFrame(0, index=col_list, columns=col_list)
        for i in range(n_cols):
            vs_matrix.iloc[i, :i] = 1

        # incorporate user-specified variable_selection
        for target_col, pred_cols in self.variable_selection.items():
            if target_col in vs_matrix.index:
                vs_matrix.loc[target_col, :] = 0
                for pc in pred_cols:
                    if pc in vs_matrix.columns:
                        vs_matrix.loc[target_col, pc] = 1

        print("[INFO] aggregator: final method assignment:")
        for col, m in zip(col_list, final_methods):
            print(f"  {col} => {m}")
        print("[INFO] aggregator: final variable_selection matrix:")
        print(vs_matrix)

        # 3) Train a minimal "model" for each column
        self._column_models.clear()
        for i, col in enumerate(col_list):
            chosen_method = final_methods[i]
            preds = vs_matrix.columns[(vs_matrix.loc[col] == 1)].tolist()
            model_info = self._train_column_model(df, target_col=col, predictor_cols=preds, method=chosen_method)
            self._column_models[col] = {
                "method": chosen_method,
                "predictors": preds,
                "model": model_info,
            }

        self._model_trained = True
        return self

    # ----------------------------------------------------------------
    # generate(...)
    # ----------------------------------------------------------------
    def generate(
        self,
        count: int = 10,
        constraint: Union[None, Dict[str, List[Any]], Constraints] = None,
        *args: Any,
        **kwargs: Any
    ) -> Syn_SeqDataLoader:
        """
        Generate synthetic data row-by-row.

        Steps:
          1) Convert `constraint` to SynSeqConstraints if needed
          2) If strict => repeated tries
             else => single pass + constraints
          3) Return a new Syn_SeqDataLoader
        """
        if not self._model_trained:
            raise RuntimeError("fit() must be called before generate().")

        # 1) unify constraints
        syn_constraints = _to_synseq_constraints(constraint)
        if syn_constraints is not None:
            syn_constraints.seq_id_feature = self.seq_id_col
            syn_constraints.seq_time_id_feature = self.seq_time_col

        # 2) Strict => repeated tries
        if syn_constraints and self.strict:
            syn_df = self._attempt_strict_generation(count, syn_constraints)
        else:
            # single pass
            syn_df = self._generate_once(count)
            # direct substitution + match if we have constraints
            if syn_constraints:
                syn_df = self._apply_synseq_corrections(syn_df, syn_constraints)
                syn_df = syn_constraints.match(syn_df)

        # 3) wrap up
        return Syn_SeqDataLoader(data=syn_df, syn_order=list(syn_df.columns))

    # ----------------------------------------------------------------
    # Helpers
    # ----------------------------------------------------------------
    def _train_column_model(
        self,
        df: pd.DataFrame,
        target_col: str,
        predictor_cols: List[str],
        method: str,
    ) -> dict:
        """
        Minimal training logic; store raw data for now. 
        Real use would implement CART, pmm, etc.
        """
        model_info = {
            "predictors": predictor_cols,
            "target_data": df[target_col].values,
            "method": method,
        }
        return model_info

    def _generate_for_column(
        self,
        count: int,
        col: str,
        method: str,
        predictor_cols: List[str],
        model_obj: dict,
        partial_df: pd.DataFrame,
    ) -> pd.Series:
        """
        Column-level sampling approach:
          - "swr" => sample without replacement
          - "cart", "pmm" => placeholder => random from real
          - fallback => random from real
        """
        real_data = model_obj["target_data"]
        rng = np.random.default_rng(self.random_state + hash(col) % 999999)

        if method.lower() == "swr":
            n_real = len(real_data)
            if count <= n_real:
                picks = sample(list(real_data), count)
            else:
                picks = list(real_data)
                overshoot = count - n_real
                picks += sample(list(real_data), overshoot)
            return pd.Series(picks)

        elif method.lower() in ["cart", "pmm"]:
            # placeholder => random
            picks = rng.choice(real_data, size=count, replace=True)
            return pd.Series(picks)

        else:
            # fallback => random
            picks = rng.choice(real_data, size=count, replace=True)
            return pd.Series(picks)

    def _generate_once(self, count: int) -> pd.DataFrame:
        """
        Single pass ignoring constraints.
        """
        col_list = list(self._column_models.keys())
        syn_df = pd.DataFrame(index=range(count))
        for col in col_list:
            info = self._column_models[col]
            method = info["method"]
            preds = info["predictors"]
            model_obj = info["model"]

            new_vals = self._generate_for_column(
                count, col, method, preds, model_obj, partial_df=syn_df
            )
            syn_df[col] = new_vals

        return syn_df

    def _attempt_strict_generation(
        self,
        count: int,
        syn_constraints: SynSeqConstraints
    ) -> pd.DataFrame:
        """
        Repeated tries if strict => generate, correct '=' constraints, match => 
        accumulate until we have `count` rows or out of patience.
        """
        result_df = pd.DataFrame()
        tries = 0
        while len(result_df) < count and tries < self.sampling_patience:
            tries += 1
            chunk = self._generate_once(count)
            chunk = self._apply_synseq_corrections(chunk, syn_constraints)
            chunk = syn_constraints.match(chunk)
            chunk = chunk.drop_duplicates()

            result_df = pd.concat([result_df, chunk], ignore_index=True)

        return result_df.head(count)

    def _apply_synseq_corrections(
        self,
        df: pd.DataFrame,
        syn_constraints: SynSeqConstraints
    ) -> pd.DataFrame:
        """
        For each (feature, op, val) in constraints, if op in ['=','=='],
        do direct substitution. For other ops => do nothing here.
        """
        new_df = df.copy()
        for (feature, op, val) in syn_constraints.rules:
            if op in ["=", "=="]:
                new_df = syn_constraints._correct(new_df, feature, op, val)
        return new_df


src/synthcity/plugins/core/dataloader.py
# stdlib
import random
from abc import ABCMeta, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union

# third party
import numpy as np
import numpy.ma as ma
import pandas as pd
import PIL
import torch
from pydantic import validate_arguments
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torchvision import transforms

# synthcity absolute
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.dataset import FlexibleDataset, TensorDataset
from synthcity.plugins.core.models.feature_encoder import DatetimeEncoder
from synthcity.utils.compression import compress_dataset, decompress_dataset
from synthcity.utils.serialization import dataframe_hash


class DataLoader(metaclass=ABCMeta):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.DataLoader
        :parts: 1

    Base class for all data loaders.

    Each derived class must implement the following methods:
        unpack() - a method that unpacks the columns and returns features and labels (X, y).
        decorate() - a method that creates a new instance of DataLoader by decorating the input data with the same DataLoader properties (e.g. sensitive features, target column, etc.)
        dataframe() - a method that returns the pandas dataframe that contains all features and samples
        numpy() - a method that returns the numpy array that contains all features and samples
        info() - a method that returns a dictionary of DataLoader information
        __len__() - a method that returns the number of samples in the DataLoader
        satisfies() - a method that tests if the current DataLoader satisfies the constraint provided
        match() - a method that returns a new DataLoader where the provided constraints are met
        from_info() - a static method that creates a DataLoader from the data and the information dictionary
        sample() - returns a new DataLoader that contains a random subset of N samples
        drop() - returns a new DataLoader with a list of columns dropped
        __getitem__() - getting features by names
        __setitem__() - setting features by names
        train() - returns a DataLoader containing the training set
        test() - returns a DataLoader containing the testing set
        fillna() - returns a DataLoader with NaN filled by the provided number(s)


    If any method implementation is missing, the class constructor will fail.

    Constructor Args:
        data_type: str
            The type of DataLoader, currently supports "generic", "time_series" and "survival".
        data: Any
            The object that contains the data
        static_features: List[str]
            List of feature names that are static features (as opposed to temporal features).
        temporal_features:
            List of feature names that are temporal features, i.e. observed over time.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        outcome_features:
            The feature name that provides labels for downstream tasks.
    """

    def __init__(
        self,
        data_type: str,
        data: Any,
        static_features: List[str] = [],
        temporal_features: List[str] = [],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        outcome_features: List[str] = [],
        train_size: float = 0.8,
        random_state: int = 0,
        **kwargs: Any,
    ) -> None:
        self.static_features = static_features
        self.temporal_features = temporal_features
        self.sensitive_features = sensitive_features
        self.important_features = important_features
        self.outcome_features = outcome_features
        self.random_state = random_state

        self.data = data
        self.data_type = data_type
        self.train_size = train_size

    def raw(self) -> Any:
        return self.data

    @abstractmethod
    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        ...

    @abstractmethod
    def decorate(self, data: Any) -> "DataLoader":
        ...

    def type(self) -> str:
        return self.data_type

    @property
    @abstractmethod
    def shape(self) -> tuple:
        ...

    @property
    @abstractmethod
    def columns(self) -> list:
        ...

    @abstractmethod
    def dataframe(self) -> pd.DataFrame:
        ...

    @abstractmethod
    def numpy(self) -> np.ndarray:
        ...

    @property
    def values(self) -> np.ndarray:
        return self.numpy()

    @abstractmethod
    def info(self) -> dict:
        ...

    @abstractmethod
    def __len__(self) -> int:
        ...

    @abstractmethod
    def satisfies(self, constraints: Constraints) -> bool:
        ...

    @abstractmethod
    def match(self, constraints: Constraints) -> "DataLoader":
        ...

    @staticmethod
    @abstractmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        ...

    @abstractmethod
    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        ...

    @abstractmethod
    def drop(self, columns: list = []) -> "DataLoader":
        ...

    @abstractmethod
    def __getitem__(self, feature: Union[str, list]) -> Any:
        ...

    @abstractmethod
    def __setitem__(self, feature: str, val: Any) -> None:
        ...

    @abstractmethod
    def train(self) -> "DataLoader":
        ...

    @abstractmethod
    def test(self) -> "DataLoader":
        ...

    def hash(self) -> str:
        return dataframe_hash(self.dataframe())

    def __repr__(self, *args: Any, **kwargs: Any) -> str:
        return self.dataframe().__repr__(*args, **kwargs)

    def _repr_html_(self, *args: Any, **kwargs: Any) -> Any:
        return self.dataframe()._repr_html_(*args, **kwargs)

    @abstractmethod
    def fillna(self, value: Any) -> "DataLoader":
        ...

    @abstractmethod
    def compression_protected_features(self) -> list:
        ...

    def domain(self) -> Optional[str]:
        return None

    def compress(
        self,
    ) -> Tuple["DataLoader", Dict]:
        to_compress = self.data.copy().drop(
            columns=self.compression_protected_features()
        )
        compressed, context = compress_dataset(to_compress)
        for protected_col in self.compression_protected_features():
            compressed[protected_col] = self.data[protected_col]

        return self.decorate(compressed), context

    def decompress(self, context: Dict) -> "DataLoader":
        decompressed = decompress_dataset(self.data, context)

        return self.decorate(decompressed)

    def encode(
        self,
        encoders: Optional[Dict[str, Any]] = None,
    ) -> Tuple["DataLoader", Dict]:
        encoded = self.dataframe().copy()
        if encoders is not None:
            for col in encoders:
                if col not in encoded.columns:
                    continue
                encoded[col] = encoders[col].transform(encoded[col])
        else:
            encoders = {}

            for col in encoded.columns:
                if (
                    encoded[col].infer_objects().dtype.kind == "i"
                    and encoded[col].min() == 0
                    and encoded[col].max() == len(encoded[col].unique()) - 1
                ):
                    continue

                if (
                    encoded[col].infer_objects().dtype.kind in ["O", "b"]
                    or len(encoded[col].unique()) < 15
                ):
                    encoder = LabelEncoder().fit(encoded[col])
                    encoded[col] = encoder.transform(encoded[col])
                    encoders[col] = encoder
                elif encoded[col].infer_objects().dtype.kind in ["M"]:
                    encoder = DatetimeEncoder().fit(encoded[col])
                    encoded[col] = encoder.transform(encoded[col]).values
                    encoders[col] = encoder
        return self.from_info(encoded, self.info()), encoders

    def decode(
        self,
        encoders: Dict[str, Any],
    ) -> "DataLoader":
        decoded = self.dataframe().copy()

        for col in encoders:
            if isinstance(encoders[col], LabelEncoder):
                decoded[col] = decoded[col].astype(int)
            else:
                decoded[col] = decoded[col].astype(float)

            decoded[col] = encoders[col].inverse_transform(decoded[col])

        return self.from_info(decoded, self.info())

    @abstractmethod
    def is_tabular(self) -> bool:
        ...

    @abstractmethod
    def get_fairness_column(self) -> Union[str, Any]:
        ...


class GenericDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.GenericDataLoader
        :parts: 1

    Data loader for generic tabular data.

    Constructor Args:
        data: Union[pd.DataFrame, list, np.ndarray]
            The dataset. Either a Pandas DataFrame or a Numpy Array.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        target_column: Optional[str]
            The feature name that provides labels for downstream tasks.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        domain_column: Optional[str]
            Optional domain label, used for domain adaptation algorithms.
        random_state: int
            Defaults to zero.

    Example:
        >>> from sklearn.datasets import load_diabetes
        >>> from synthcity.plugins.core.dataloader import GenericDataLoader
        >>> X, y = load_diabetes(return_X_y=True, as_frame=True)
        >>> X["target"] = y
        >>> # Important note: preprocessing data with OneHotEncoder or StandardScaler is not needed or recommended.
        >>> # Synthcity handles feature encoding and standardization internally.
        >>> loader = GenericDataLoader(X, target_column="target", sensitive_columns=["sex"],)
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: Union[pd.DataFrame, list, np.ndarray],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        target_column: Optional[str] = None,
        fairness_column: Optional[str] = None,
        domain_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)

        data.columns = data.columns.astype(str)
        if target_column is not None:
            self.target_column = target_column
        elif len(data.columns) > 0:
            self.target_column = data.columns[-1]
        else:
            self.target_column = "---"

        self.fairness_column = fairness_column
        self.domain_column = domain_column

        super().__init__(
            data_type="generic",
            data=data,
            static_features=list(data.columns),
            sensitive_features=sensitive_features,
            important_features=important_features,
            outcome_features=[self.target_column],
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape

    def domain(self) -> Optional[str]:
        return self.domain_column

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    @property
    def columns(self) -> list:
        return list(self.data.columns)

    def compression_protected_features(self) -> list:
        out = [self.target_column]
        domain = self.domain()

        if domain is not None:
            out.append(domain)

        return out

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        X = self.data.drop(columns=[self.target_column])
        y = self.data[self.target_column]

        if as_numpy:
            return np.asarray(X), np.asarray(y)
        return X, y

    def dataframe(self) -> pd.DataFrame:
        return self.data

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": self.static_features,
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "outcome_features": self.outcome_features,
            "target_column": self.target_column,
            "fairness_column": self.fairness_column,
            "domain_column": self.domain_column,
            "train_size": self.train_size,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return GenericDataLoader(
            data,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            target_column=self.target_column,
            random_state=self.random_state,
            train_size=self.train_size,
            fairness_column=self.fairness_column,
            domain_column=self.domain_column,
        )

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self.data)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.decorate(constraints.match(self.data))

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        return self.decorate(self.data.sample(count, random_state=random_state))

    def drop(self, columns: list = []) -> "DataLoader":
        return self.decorate(self.data.drop(columns=columns))

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "GenericDataLoader":
        if not isinstance(data, pd.DataFrame):
            raise ValueError(f"Invalid data type {type(data)}")

        return GenericDataLoader(
            data,
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            target_column=info["target_column"],
            fairness_column=info["fairness_column"],
            domain_column=info["domain_column"],
            train_size=info["train_size"],
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data[feature] = val

    def _train_test_split(self) -> Tuple:
        stratify = None
        if self.target_column in self.data:
            target = self.data[self.target_column]
            if target.value_counts().min() > 1:
                stratify = target

        return train_test_split(
            self.data,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )

    def train(self) -> "DataLoader":
        train_data, _ = self._train_test_split()
        return self.decorate(train_data.reset_index(drop=True))

    def test(self) -> "DataLoader":
        _, test_data = self._train_test_split()
        return self.decorate(test_data.reset_index(drop=True))

    def fillna(self, value: Any) -> "DataLoader":
        self.data = self.data.fillna(value)
        return self

    def is_tabular(self) -> bool:
        return True


class SurvivalAnalysisDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.SurvivalAnalysisDataLoader
        :parts: 1

    Data Loader for Survival Analysis Data

    Constructor Args:
        data: Union[pd.DataFrame, list, np.ndarray]
            The dataset. Either a Pandas DataFrame or a Numpy Array.
        time_to_event_column: str
            Survival Analysis specific time-to-event feature
        target_column: str
            The outcome: event or censoring.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        target_column: str
            The feature name that provides labels for downstream tasks.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        domain_column: Optional[str]
            Optional domain label, used for domain adaptation algorithms.
        random_state: int
            Defaults to zero.
        train_size: float
            The ratio to use for train splits.

    Example:
        >>> TODO
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: pd.DataFrame,
        time_to_event_column: str,
        target_column: str,
        time_horizons: list = [],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if target_column not in data.columns:
            raise ValueError(f"Event column {target_column} not found in the dataframe")

        if time_to_event_column not in data.columns:
            raise ValueError(
                f"Time to event column {time_to_event_column} not found in the dataframe"
            )

        T = data[time_to_event_column]
        data_filtered = data[T > 0]
        row_diff = data.shape[0] - data_filtered.shape[0]
        if row_diff > 0:
            raise ValueError(
                f"The time_to_event_column contains {row_diff} values less than or equal to zero. Please remove them."
            )

        if len(time_horizons) == 0:
            time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1].tolist()

        data.columns = data.columns.astype(str)

        self.target_column = target_column
        self.time_to_event_column = time_to_event_column
        self.time_horizons = time_horizons
        self.fairness_column = fairness_column

        super().__init__(
            data_type="survival_analysis",
            data=data,
            static_features=list(data.columns.astype(str)),
            sensitive_features=sensitive_features,
            important_features=important_features,
            outcome_features=[self.target_column],
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape

    @property
    def columns(self) -> list:
        return list(self.data.columns)

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def compression_protected_features(self) -> list:
        out = [self.target_column, self.time_to_event_column]
        domain = self.domain()

        if domain is not None:
            out.append(domain)

        return out

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        X = self.data.drop(columns=[self.target_column, self.time_to_event_column])
        T = self.data[self.time_to_event_column]
        E = self.data[self.target_column]

        if as_numpy:
            return np.asarray(X), np.asarray(T), np.asarray(E)

        return X, T, E

    def dataframe(self) -> pd.DataFrame:
        return self.data

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": list(self.static_features),
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "outcome_features": self.outcome_features,
            "target_column": self.target_column,
            "fairness_column": self.fairness_column,
            "time_to_event_column": self.time_to_event_column,
            "time_horizons": self.time_horizons,
            "train_size": self.train_size,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return SurvivalAnalysisDataLoader(
            data,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            target_column=self.target_column,
            fairness_column=self.fairness_column,
            time_to_event_column=self.time_to_event_column,
            time_horizons=self.time_horizons,
            random_state=self.random_state,
            train_size=self.train_size,
        )

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self.data)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.decorate(
            constraints.match(self.data),
        )

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        return self.decorate(
            self.data.sample(count, random_state=random_state),
        )

    def drop(self, columns: list = []) -> "DataLoader":
        return self.decorate(
            self.data.drop(columns=columns),
        )

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        if not isinstance(data, pd.DataFrame):
            raise ValueError(f"Invalid data type {type(data)}")

        return SurvivalAnalysisDataLoader(
            data,
            target_column=info["target_column"],
            time_to_event_column=info["time_to_event_column"],
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            time_horizons=info["time_horizons"],
            fairness_column=info["fairness_column"],
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data[feature] = val

    def train(self) -> "DataLoader":
        stratify = self.data[self.target_column]
        train_data, _ = train_test_split(
            self.data, train_size=self.train_size, random_state=0, stratify=stratify
        )
        return self.decorate(
            train_data.reset_index(drop=True),
        )

    def test(self) -> "DataLoader":
        stratify = self.data[self.target_column]
        _, test_data = train_test_split(
            self.data,
            train_size=self.train_size,
            random_state=0,
            stratify=stratify,
        )
        return self.decorate(
            test_data.reset_index(drop=True),
        )

    def fillna(self, value: Any) -> "DataLoader":
        self.data = self.data.fillna(value)
        return self

    def is_tabular(self) -> bool:
        return True


class TimeSeriesDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.TimeSeriesDataLoader
        :parts: 1

    Data Loader for Time Series Data

    Constructor Args:
        temporal data: List[pd.DataFrame]
            The temporal data. A list of pandas DataFrames
        observation times: List
            List of arrays mapping directly to index of each dataframe in temporal_data
        outcome: Optional[pd.DataFrame] = None
            pandas DataFrame thatn can be anything (eg, labels, regression outcome)
        static_data: Optional[pd.DataFrame] = None
            pandas DataFrame mapping directly to index of each dataframe in temporal_data
        sensitive_features: List[str]
            Name of sensitive features
        important_features List[str]
            Default: None. Only relevant for SurvivalGAN method
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        random_state: int
            Defaults to zero.

    Example:
        >>> TODO
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame] = None,
        static_data: Optional[pd.DataFrame] = None,
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        seq_offset: int = 0,
        **kwargs: Any,
    ) -> None:
        static_features = []
        self.outcome_features = []

        if len(temporal_data) == 0:
            raise ValueError("Empty temporal data")

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        max_window_len = max([len(t) for t in temporal_data])
        if static_data is not None:
            if len(static_data) != len(temporal_data):
                raise ValueError("Static and temporal data mismatch")
            static_features = list(static_data.columns)
        else:
            static_data = pd.DataFrame(np.zeros((len(temporal_data), 0)))

        if outcome is not None:
            if len(outcome) != len(temporal_data):
                raise ValueError("Temporal and outcome data mismatch")
            self.outcome_features = list(outcome.columns)
        else:
            outcome = pd.DataFrame(np.zeros((len(temporal_data), 0)))

        self.window_len = max_window_len
        self.fill = np.nan
        self.seq_offset = seq_offset

        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
            seq_df,
            seq_info,
        ) = TimeSeriesDataLoader.pack_raw_data(
            static_data,
            temporal_data,
            observation_times,
            outcome,
            fill=self.fill,
            seq_offset=seq_offset,
        )
        self.seq_info = seq_info
        self.fairness_column = fairness_column

        super().__init__(
            data={
                "static_data": static_data,
                "temporal_data": temporal_data,
                "observation_times": observation_times,
                "outcome": outcome,
                "seq_data": seq_df,
            },
            data_type="time_series",
            static_features=static_features,
            temporal_features=temporal_features,
            outcome_features=self.outcome_features,
            sensitive_features=sensitive_features,
            important_features=important_features,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data["seq_data"].shape

    @property
    def columns(self) -> list:
        return self.data["seq_data"].columns

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def compression_protected_features(self) -> list:
        return self.outcome_features

    @property
    def raw_columns(self) -> list:
        return self.static_features + self.temporal_features + self.outcome_features

    def dataframe(self) -> pd.DataFrame:
        return self.data["seq_data"].copy()

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        generic_info = {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": self.static_features,
            "temporal_features": self.temporal_features,
            "outcome_features": self.outcome_features,
            "outcome_len": len(self.data["outcome"].values.reshape(-1))
            / len(self.data["outcome"]),
            "window_len": self.window_len,
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "fairness_column": self.fairness_column,
            "random_state": self.random_state,
            "train_size": self.train_size,
            "fill": self.fill,
        }

        for key in self.seq_info:
            generic_info[key] = self.seq_info[key]

        return generic_info

    def __len__(self) -> int:
        return len(self.data["seq_data"])

    def decorate(self, data: Any) -> "DataLoader":
        static_data, temporal_data, observation_times, outcome = data

        return TimeSeriesDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            outcome=outcome,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            fairness_column=self.fairness_column,
            random_state=self.random_state,
            train_size=self.train_size,
            seq_offset=self.seq_offset,
        )

    def unpack_and_decorate(self, data: pd.DataFrame) -> "DataLoader":
        unpacked_data = TimeSeriesDataLoader.unpack_raw_data(
            data,
            self.info(),
        )

        return self.decorate(unpacked_data)

    def satisfies(self, constraints: Constraints) -> bool:
        seq_df = self.dataframe()

        return constraints.is_valid(seq_df)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.unpack_and_decorate(
            constraints.match(self.dataframe()),
        )

    def drop(self, columns: list = []) -> "DataLoader":
        new_data = self.data["seq_data"].drop(columns=columns)
        return self.unpack_and_decorate(new_data)

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.unpack_raw_data(
            data,
            info,
        )
        return TimeSeriesDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            outcome=outcome,
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            fairness_column=info["fairness_column"],
            fill=info["fill"],
            seq_offset=info["seq_offset"],
        )

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if pad:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_and_mask(
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )
        else:
            static_data, temporal_data, observation_times, outcome = (
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )

        if as_numpy:
            longest_observation_seq = max([len(seq) for seq in temporal_data])
            padded_temporal_data = np.zeros(
                (len(temporal_data), longest_observation_seq, 5)
            )
            mask = np.ones((len(temporal_data), longest_observation_seq, 5), dtype=bool)
            for i, arr in enumerate(temporal_data):
                padded_temporal_data[i, : arr.shape[0], :] = arr  # Copy the actual data
                mask[
                    i, : arr.shape[0], :
                ] = False  # Set mask to False where actual data is present

            masked_temporal_data = ma.masked_array(padded_temporal_data, mask)
            return (
                np.asarray(static_data),
                masked_temporal_data,  # TODO: check this works with time series benchmarks
                # masked array to handle variable length sequences
                ma.vstack(
                    [
                        ma.array(
                            np.resize(ot, longest_observation_seq),
                            mask=[True for i in range(len(ot))]
                            + [False for j in range(longest_observation_seq - len(ot))],
                        )
                        for ot in observation_times
                    ]
                ),
                np.asarray(outcome),
            )
        return (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data["seq_data"][feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data["seq_data"][feature] = val

    def ids(self) -> list:
        id_col = self.seq_info["seq_id_feature"]
        ids = self.data["seq_data"][id_col]

        return list(ids.unique())

    def filter_ids(self, ids_list: list) -> pd.DataFrame:
        seq_data = self.data["seq_data"]
        id_col = self.info()["seq_id_feature"]

        return seq_data[seq_data[id_col].isin(ids_list)]

    def train(self) -> "DataLoader":
        # TODO: stratify
        ids = self.ids()
        train_ids, _ = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
        )
        return self.unpack_and_decorate(self.filter_ids(train_ids))

    def test(self) -> "DataLoader":
        # TODO: stratify
        ids = self.ids()
        _, test_ids = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
        )
        return self.unpack_and_decorate(self.filter_ids(test_ids))

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        ids = self.ids()
        count = min(count, len(ids))
        sampled_ids = random.sample(ids, count)

        return self.unpack_and_decorate(self.filter_ids(sampled_ids))

    def fillna(self, value: Any) -> "DataLoader":
        for key in ["static_data", "outcome", "seq_data"]:
            if self.data[key] is not None:
                self.data[key] = self.data[key].fillna(value)

        for idx, item in enumerate(self.data["temporal_data"]):
            self.data["temporal_data"][idx] = self.data["temporal_data"][idx].fillna(
                value
            )

        return self

    @staticmethod
    def unique_temporal_features(temporal_data: List[pd.DataFrame]) -> List:
        temporal_features = []
        for item in temporal_data:
            temporal_features.extend(item.columns)
        return sorted(np.unique(temporal_features).tolist())

    # Padding helpers
    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_raw_features(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
    ) -> Any:
        fill = np.nan

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        for idx, item in enumerate(temporal_data):
            # handling missing features
            for col in temporal_features:
                if col not in item.columns:
                    item[col] = fill
            item = item[temporal_features]

            if list(item.columns) != list(temporal_features):
                raise RuntimeError("Invalid features for packing")

            temporal_data[idx] = item.fillna(fill)

        return static_data, temporal_data, observation_times, outcome

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_raw_data(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
    ) -> Any:
        fill = np.nan

        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.pad_raw_features(
            static_data, temporal_data, observation_times, outcome
        )
        max_window_len = max([len(t) for t in temporal_data])
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        for idx, item in enumerate(temporal_data):
            if len(item) != max_window_len:
                pads = fill * np.ones(
                    (max_window_len - len(item), len(temporal_features))
                )
                start = 0
                if len(item.index) > 0:
                    start = max(item.index) + 1
                pads_df = pd.DataFrame(
                    pads,
                    index=[start + i for i in range(len(pads))],
                    columns=item.columns,
                )
                item = pd.concat([item, pads_df])

            # handle missing time points
            if list(item.columns) != list(temporal_features):
                raise RuntimeError(
                    f"Invalid features {item.columns}. Expected {temporal_features}"
                )
            if len(item) != max_window_len:
                raise RuntimeError("Invalid window len")

            temporal_data[idx] = item

        observation_times_padded = []
        for idx, item in enumerate(observation_times):
            item = list(item)
            if len(item) != max_window_len:
                pads = fill * np.ones(max_window_len - len(item))
                item.extend(pads.tolist())
            observation_times_padded.append(item)

        return static_data, temporal_data, observation_times_padded, outcome

    # Masking helpers
    @staticmethod
    def extract_masked_features(full_temporal_features: list) -> tuple:
        temporal_features = []
        mask_features = []
        mask_prefix = "masked_"
        for feat in full_temporal_features:
            feat = str(feat)
            if not feat.startswith(mask_prefix):
                temporal_features.append(feat)
                continue

            other_feat = feat[len(mask_prefix) :]
            if other_feat in full_temporal_features:
                mask_features.append(feat)
            else:
                temporal_features.append(feat)

        return temporal_features, mask_features

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def mask_temporal_data(
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        fill: Any = 0,
    ) -> Any:
        nan_cnt = 0
        for item in temporal_data:
            nan_cnt += np.asarray(np.isnan(item)).sum()

        if nan_cnt == 0:
            return temporal_data, observation_times

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        masked_features = [f"masked_{feat}" for feat in temporal_features]

        for idx, item in enumerate(temporal_data):
            item[masked_features] = (~np.isnan(item)).astype(int)
            item = item.fillna(fill)
            temporal_data[idx] = item

        for idx, item in enumerate(observation_times):
            item = np.nan_to_num(item, nan=fill).tolist()

            observation_times[idx] = item

        return temporal_data, observation_times

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def unmask_temporal_data(
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        fill: Any = np.nan,
    ) -> Any:
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        temporal_features, mask_features = TimeSeriesDataLoader.extract_masked_features(
            temporal_features
        )

        missing_horizons = []
        for idx, item in enumerate(temporal_data):
            # handle existing mask
            if len(mask_features) > 0:
                mask = temporal_data[idx][mask_features].astype(bool)
                item[~mask] = np.nan

            item_missing_rows = item.isna().sum(axis=1).values
            missing_horizons.append(item_missing_rows == len(temporal_features))

            # TODO: review impact on horizons
            temporal_data[idx] = item.dropna()

        observation_times_unmasked = []
        for idx, item in enumerate(observation_times):
            item = list(item)

            for midx, mval in enumerate(missing_horizons[idx]):
                if mval:
                    item[midx] = np.nan

            local_horizons = list(filter(lambda v: v == v, item))
            observation_times_unmasked.append(local_horizons)

        return temporal_data, observation_times_unmasked

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_and_mask(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        only_features: Any = False,
        fill: Any = 0,
    ) -> Any:
        if only_features:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_raw_features(
                static_data,
                temporal_data,
                observation_times,
                outcome,
            )
        else:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_raw_data(
                static_data,
                temporal_data,
                observation_times,
                outcome,
            )

        temporal_data, observation_times = TimeSeriesDataLoader.mask_temporal_data(
            temporal_data, observation_times, fill=fill
        )

        return static_data, temporal_data, observation_times, outcome

    @staticmethod
    def sequential_view(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        id_col: str = "seq_id",
        time_id_col: str = "seq_time_id",
        seq_offset: int = 0,
    ) -> Tuple[pd.DataFrame, dict]:  # sequential dataframe, loader info
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.pad_and_mask(
            static_data, temporal_data, observation_times, outcome, only_features=True
        )
        raw_static_features = list(static_data.columns)
        static_features = [f"seq_static_{col}" for col in raw_static_features]

        raw_outcome_features = list(outcome.columns)
        outcome_features = [f"seq_out_{col}" for col in raw_outcome_features]

        raw_temporal_features = TimeSeriesDataLoader.unique_temporal_features(
            temporal_data
        )
        temporal_features = [f"seq_temporal_{col}" for col in raw_temporal_features]
        cols = (
            [id_col, time_id_col]
            + static_features
            + temporal_features
            + outcome_features
        )

        seq = []
        for sidx, static_item in static_data.iterrows():
            real_tidx = 0
            for tidx, temporal_item in temporal_data[sidx].iterrows():
                local_seq_data = (
                    [
                        sidx + seq_offset,
                        observation_times[sidx][real_tidx],
                    ]
                    + static_item[raw_static_features].values.tolist()
                    + temporal_item[raw_temporal_features].values.tolist()
                    + outcome.loc[sidx, raw_outcome_features].values.tolist()
                )
                seq.append(local_seq_data)
                real_tidx += 1

        seq_df = pd.DataFrame(seq, columns=cols)
        info = {
            "seq_static_features": static_features,
            "seq_temporal_features": temporal_features,
            "seq_outcome_features": outcome_features,
            "seq_offset": seq_offset,
            "seq_id_feature": id_col,
            "seq_time_id_feature": time_id_col,
            "seq_features": list(seq_df.columns),
        }
        return seq_df, info

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pack_raw_data(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        fill: Any = np.nan,
        seq_offset: int = 0,
    ) -> pd.DataFrame:
        # Temporal data: (subjects, temporal_sequence, temporal_feature)
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        temporal_features, mask_features = TimeSeriesDataLoader.extract_masked_features(
            temporal_features
        )
        temporal_data, observation_times = TimeSeriesDataLoader.unmask_temporal_data(
            temporal_data, observation_times
        )
        seq_df, info = TimeSeriesDataLoader.sequential_view(
            static_data=static_data,
            temporal_data=temporal_data,
            observation_times=observation_times,
            outcome=outcome,
            seq_offset=seq_offset,
        )

        return static_data, temporal_data, observation_times, outcome, seq_df, info

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def unpack_raw_data(
        data: pd.DataFrame,
        info: dict,
    ) -> Tuple[
        Optional[pd.DataFrame], List[pd.DataFrame], List, Optional[pd.DataFrame]
    ]:
        id_col = info["seq_id_feature"]
        time_col = info["seq_time_id_feature"]

        static_cols = info["seq_static_features"]
        new_static_cols = [feat.split("seq_static_")[1] for feat in static_cols]

        temporal_cols = info["seq_temporal_features"]
        new_temporal_cols = [feat.split("seq_temporal_")[1] for feat in temporal_cols]

        outcome_cols = info["seq_outcome_features"]
        new_outcome_cols = [feat.split("seq_out_")[1] for feat in outcome_cols]

        ids = sorted(list(set(data[id_col])))

        static_data = []
        temporal_data = []
        observation_times = []
        outcome_data = []

        for item_id in ids:
            item_data = data[data[id_col] == item_id]

            static_data.append(item_data[static_cols].head(1).values.squeeze().tolist())
            outcome_data.append(
                item_data[outcome_cols].head(1).values.squeeze().tolist()
            )
            local_temporal_data = item_data[temporal_cols].copy()
            local_observation_times = item_data[time_col].values.tolist()
            local_temporal_data.columns = new_temporal_cols
            # TODO: review impact on horizons
            local_temporal_data = local_temporal_data.dropna()

            temporal_data.append(local_temporal_data)
            observation_times.append(local_observation_times)

        static_df = pd.DataFrame(static_data, columns=new_static_cols)
        outcome_df = pd.DataFrame(outcome_data, columns=new_outcome_cols)

        return static_df, temporal_data, observation_times, outcome_df

    def is_tabular(self) -> bool:
        return True


class TimeSeriesSurvivalDataLoader(TimeSeriesDataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.TimeSeriesSurvivalDataLoader
        :parts: 1

    Data loader for Time series survival data

    Constructor Args:
        temporal_data: List[pd.DataFrame}
            The temporal data. A list of pandas DataFrames.
        observation_times: List
            List of arrays mapping directly to index of each dataframe in temporal_data
        T: Union[pd.Series, np.ndarray, pd.Series]
            Time-to-event data
        E: Union[pd.Series, np.ndarray, pd.Series]
            E is censored/event data
        static_data Optional[pd.DataFrame] = None
            pandas DataFrame of static features for each subject
        sensitive_features: List[str]
            Name of sensitive features
        important_features: List[str}
            Default: None. Only relevant for SurvivalGAN method.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        random_state. int
            Defaults to zero.

    Example:
        >>> TODO

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: Union[List, np.ndarray, pd.Series],
        T: Union[pd.Series, np.ndarray, pd.Series],
        E: Union[pd.Series, np.ndarray, pd.Series],
        static_data: Optional[pd.DataFrame] = None,
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        time_horizons: list = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        seq_offset: int = 0,
        **kwargs: Any,
    ) -> None:
        self.time_to_event_col = "time_to_event"
        self.event_col = "event"
        self.fairness_column = fairness_column

        if len(time_horizons) == 0:
            time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1].tolist()
        self.time_horizons = time_horizons
        outcome = pd.concat([pd.Series(T), pd.Series(E)], axis=1)
        outcome.columns = [self.time_to_event_col, self.event_col]

        self.fill = np.nan

        super().__init__(
            temporal_data=temporal_data,
            observation_times=observation_times,
            outcome=outcome,
            static_data=static_data,
            sensitive_features=sensitive_features,
            important_features=important_features,
            random_state=random_state,
            train_size=train_size,
            seq_offset=seq_offset,
            **kwargs,
        )
        self.data_type = "time_series_survival"

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def info(self) -> dict:
        parent_info = super().info()
        parent_info["time_to_event_column"] = self.time_to_event_col
        parent_info["event_column"] = self.event_col
        parent_info["time_horizons"] = self.time_horizons
        parent_info["fill"] = self.fill

        return parent_info

    def decorate(self, data: Any) -> "DataLoader":
        static_data, temporal_data, observation_times, outcome = data
        if self.time_to_event_col not in outcome:
            raise ValueError(
                f"Survival outcome is missing tte column {self.time_to_event_col}"
            )
        if self.event_col not in outcome:
            raise ValueError(
                f"Survival outcome is missing event column {self.event_col}"
            )

        return TimeSeriesSurvivalDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            T=outcome[self.time_to_event_col],
            E=outcome[self.event_col],
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            fairness_column=self.fairness_column,
            random_state=self.random_state,
            time_horizons=self.time_horizons,
            train_size=self.train_size,
            seq_offset=self.seq_offset,
        )

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesSurvivalDataLoader.unpack_raw_data(
            data,
            info,
        )
        return TimeSeriesSurvivalDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            T=outcome[info["time_to_event_column"]],
            E=outcome[info["event_column"]],
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            fairness_column=info["fairness_column"],
            time_horizons=info["time_horizons"],
            seq_offset=info["seq_offset"],
        )

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if pad:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesSurvivalDataLoader.pad_and_mask(
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )
        else:
            static_data, temporal_data, observation_times, outcome = (
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )

        if as_numpy:
            return (
                np.asarray(static_data),
                np.asarray(temporal_data, dtype=object),
                np.asarray(observation_times, dtype=object),
                np.asarray(outcome[self.time_to_event_col]),
                np.asarray(outcome[self.event_col]),
            )
        return (
            static_data,
            temporal_data,
            observation_times,
            outcome[self.time_to_event_col],
            outcome[self.event_col],
        )

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.unpack_and_decorate(
            constraints.match(self.dataframe()),
        )

    def train(self) -> "DataLoader":
        stratify = self.data["outcome"][self.event_col]

        ids = self.ids()
        train_ids, _ = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )
        return self.unpack_and_decorate(self.filter_ids(train_ids))

    def test(self) -> "DataLoader":
        stratify = self.data["outcome"][self.event_col]
        ids = self.ids()
        _, test_ids = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )
        return self.unpack_and_decorate(self.filter_ids(test_ids))


class ImageDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.ImageDataLoader
        :parts: 1

    Data loader for generic image data.

    Constructor Args:
        data: torch.utils.data.Dataset or torch.Tensor
            The image dataset or a tuple of (tensor images, tensor labels)
        random_state: int
            Defaults to zero.
        height: int. Default = 32
            Height to use internally
        width: Optional[int]
            Optional width to use internally. If None, it is used the same value as height.
        train_size: float = 0.8
            Train dataset ratio.
    Example:
        >>> dataset = datasets.MNIST(".", download=True)
        >>>
        >>> loader = ImageDataLoader(
        >>>     data=dataset,
        >>>     train_size=0.8,
        >>>     height=32,
        >>>     width=w32,
        >>> )

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: Union[torch.utils.data.Dataset, Tuple[torch.Tensor, torch.Tensor]],
        height: int = 32,
        width: Optional[int] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if width is None:
            width = height

        if isinstance(data, tuple):
            X, y = data
            data = TensorDataset(images=X, targets=y)

        self.data_transform = None

        dummy, _ = data[0]
        img_transform = []
        if not isinstance(dummy, PIL.Image.Image):
            img_transform = [transforms.ToPILImage()]

        img_transform.extend(
            [
                transforms.Resize((height, width)),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.5,), std=(0.5,)),
            ]
        )

        self.data_transform = transforms.Compose(img_transform)
        data = FlexibleDataset(data, transform=self.data_transform)

        self.height = height
        self.width = width
        self.channels = data.shape()[1]

        super().__init__(
            data_type="images",
            data=data,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape()

    def get_fairness_column(self) -> None:
        """Not implemented for ImageDataLoader"""
        ...

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        return self.data

    def numpy(self) -> np.ndarray:
        x, _ = self.data.numpy()

        return x

    def dataframe(self) -> pd.DataFrame:
        x = self.numpy().reshape(len(self), -1)

        x = pd.DataFrame(x)
        x.columns = x.columns.astype(str)

        return x

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "train_size": self.train_size,
            "height": self.height,
            "width": self.width,
            "channels": self.channels,
            "random_state": self.random_state,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return ImageDataLoader(
            data,
            random_state=self.random_state,
            train_size=self.train_size,
            height=self.height,
            width=self.width,
        )

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        idxs = np.random.choice(len(self), count, replace=False)
        subset = FlexibleDataset(self.data.data, indices=idxs)
        return self.decorate(subset)

    @staticmethod
    def from_info(data: torch.utils.data.Dataset, info: dict) -> "ImageDataLoader":
        if not isinstance(data, torch.utils.data.Dataset):
            raise ValueError(f"Invalid data type {type(data)}")

        return ImageDataLoader(
            data,
            train_size=info["train_size"],
            height=info["height"],
            width=info["width"],
            random_state=info["random_state"],
        )

    def __getitem__(self, index: Union[list, int, str]) -> Any:
        if isinstance(index, str):
            return self.dataframe()[index]

        return self.numpy()[index]

    def _train_test_split(self) -> Tuple:
        indices = np.arange(len(self.data))
        _, stratify = self.data.numpy()

        return train_test_split(
            indices,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )

    def train(self) -> "DataLoader":
        train_idx, _ = self._train_test_split()
        subset = FlexibleDataset(self.data.data, indices=train_idx)
        return self.decorate(subset)

    def test(self) -> "DataLoader":
        _, test_idx = self._train_test_split()
        subset = FlexibleDataset(self.data.data, indices=test_idx)
        return self.decorate(subset)

    def compress(
        self,
    ) -> Tuple["DataLoader", Dict]:
        return self, {}

    def decompress(self, context: Dict) -> "DataLoader":
        return self

    def encode(
        self,
        encoders: Optional[Dict[str, Any]] = None,
    ) -> Tuple["DataLoader", Dict]:
        return self, {}

    def decode(
        self,
        encoders: Dict[str, Any],
    ) -> "DataLoader":
        return self

    def is_tabular(self) -> bool:
        return False

    @property
    def columns(self) -> list:
        return list(self.dataframe().columns)

    def satisfies(self, constraints: Constraints) -> bool:
        return True

    def match(self, constraints: Constraints) -> "DataLoader":
        return self

    def compression_protected_features(self) -> list:
        raise NotImplementedError("Images do not support the compression call")

    def drop(self, columns: list = []) -> "DataLoader":
        raise NotImplementedError()

    def __setitem__(self, feature: str, val: Any) -> None:
        raise NotImplementedError()

    def fillna(self, value: Any) -> "DataLoader":
        raise NotImplementedError()



@validate_arguments(config=dict(arbitrary_types_allowed=True))
def create_from_info(
    data: Union[pd.DataFrame, torch.utils.data.Dataset], info: dict
) -> "DataLoader":
    """Helper for creating a DataLoader from existing information."""
    if info["data_type"] == "generic":
        return GenericDataLoader.from_info(data, info)
    elif info["data_type"] == "survival_analysis":
        return SurvivalAnalysisDataLoader.from_info(data, info)
    elif info["data_type"] == "time_series":
        return TimeSeriesDataLoader.from_info(data, info)
    elif info["data_type"] == "time_series_survival":
        return TimeSeriesSurvivalDataLoader.from_info(data, info)
    elif info["data_type"] == "images":
        return ImageDataLoader.from_info(data, info)
    else:
        raise RuntimeError(f"invalid datatype {info}")


from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.models.syn_seq_encoder import Syn_SeqEncoder


class Syn_SeqDataLoader(DataLoader):
    """
    A DataLoader that applies Syn_Seq-style preprocessing to input data,
    inheriting directly from DataLoader and implementing all required
    abstract methods.

    - syn_order: The order of columns to keep or process. If not provided (None or empty),
                 use the raw DataFrame column order.
    - columns_special_values: A dict of { column_name : list_of_special_values },
      specifying which values to treat as 'special' or missing in numeric columns.
    - col_type: A dict { column_name : "category"/"numeric"/"date"/... } used to force
      how each column is handled.
    - max_categories: numeric threshold for deciding numeric vs. categorical if col_type
      isn't specified.
    """

    def __init__(
        self,
        data: pd.DataFrame,
        syn_order: Optional[List[str]] = None,
        special_value: Optional[Dict[str, List[Any]]] = None,
        col_type: Optional[Dict[str, str]] = None,
        max_categories: int = 20,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        """
        Args:
            data: the raw DataFrame
            syn_order: optional list of columns in the desired processing order
            special_value: { "col_name": [list_of_special_values], ... }
            col_type: { "col_name": "category"/"numeric"/"date"/... }
            max_categories: threshold for deciding numeric vs. categorical if not declared
            random_state: for reproducibility
            train_size: fraction of data for 'train' (rest goes to 'test')
        """
        if not syn_order:
            print("[INFO] syn_order not provided; using data.columns as default.")
            syn_order = list(data.columns)

        # ensure all requested columns exist
        missing_columns = set(syn_order) - set(data.columns)
        if missing_columns:
            raise ValueError(f"Missing columns in input data: {missing_columns}")

        # store user parameters
        self.syn_order = syn_order
        self.columns_special_values = special_value or {}
        self.col_type = col_type or {}
        self.max_categories = max_categories

        # reorder data based on syn_order
        filtered_data = data[self.syn_order].copy()

        # call parent constructor
        super().__init__(
            data_type="syn_seq",
            data=filtered_data,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

        # keep a reference
        self._df = filtered_data

        # debug print
        print("[INFO] Syn_SeqDataLoader init complete:")
        print(f"  - syn_order: {self.syn_order}")
        print(f"  - special_value (columns_special_values): {self.columns_special_values}")
        print(f"  - col_type: {self.col_type}")
        print(f"  - data shape: {self._df.shape}")

        # create + fit our Syn_SeqEncoder (just fit, actual transform is in encode())
        self._encoder = Syn_SeqEncoder(
            columns_special_values=self.columns_special_values,
            syn_order=self.syn_order,
            max_categories=self.max_categories,
            col_type=self.col_type,
        )
        self._encoder.fit(self._df)

        print("[DEBUG] After encoder.fit(), detected info:")
        print(f"  - encoder.column_order_: {self._encoder.column_order_}")
        print(f"  - numeric_info_: {self._encoder.numeric_info_}")
        print(f"  - categorical_info_: {self._encoder.categorical_info_}")
        if self._encoder.variable_selection_ is not None:
            print("  - variable_selection_:\n", self._encoder.variable_selection_)
        print("----------------------------------------------------------------")

    # ----------------------------------------------------------------
    # Inherited/required abstract methods
    # ----------------------------------------------------------------
    @property
    def shape(self) -> tuple:
        return self._df.shape

    @property
    def columns(self) -> list:
        return list(self._df.columns)

    def dataframe(self) -> pd.DataFrame:
        return self._df

    def numpy(self) -> pd.DataFrame:
        return self._df.values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "train_size": self.train_size,
            "random_state": self.random_state,
            "syn_order": self.syn_order,
            "max_categories": self.max_categories,
            "col_type": self.col_type,
            "columns_special_values": self.columns_special_values,
        }

    def __len__(self) -> int:
        return len(self._df)

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self._df)

    def match(self, constraints: Constraints) -> "Syn_SeqDataLoader":
        matched_df = constraints.match(self._df)
        return self.decorate(matched_df)

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "Syn_SeqDataLoader":
        return Syn_SeqDataLoader(
            data=data,
            syn_order=info.get("syn_order"),
            special_value=info.get("columns_special_values", {}),
            col_type=info.get("col_type", {}),
            max_categories=info.get("max_categories", 20),
            random_state=info["random_state"],
            train_size=info["train_size"],
        )

    def sample(self, count: int, random_state: int = 0) -> "Syn_SeqDataLoader":
        sampled_df = self._df.sample(count, random_state=random_state)
        return self.decorate(sampled_df)

    def drop(self, columns: list = []) -> "Syn_SeqDataLoader":
        dropped_df = self._df.drop(columns=columns, errors="ignore")
        return self.decorate(dropped_df)

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self._df[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self._df[feature] = val

    def train(self) -> "Syn_SeqDataLoader":
        ntrain = int(len(self._df) * self.train_size)
        train_df = self._df.iloc[:ntrain].copy()
        return self.decorate(train_df)

    def test(self) -> "Syn_SeqDataLoader":
        ntrain = int(len(self._df) * self.train_size)
        test_df = self._df.iloc[ntrain:].copy()
        return self.decorate(test_df)

    def fillna(self, value: Any) -> "Syn_SeqDataLoader":
        filled_df = self._df.fillna(value)
        return self.decorate(filled_df)

    def compression_protected_features(self) -> list:
        return []

    def is_tabular(self) -> bool:
        return True

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if as_numpy:
            return self._df.to_numpy()
        return self._df

    def get_fairness_column(self) -> Union[str, Any]:
        return None

    # ----------------------------------------------------------------
    # Syn_Seq-specific encode/decode
    # ----------------------------------------------------------------
    def encode(
        self, encoders: Optional[Dict[str, Any]] = None
    ) -> Tuple["Syn_SeqDataLoader", Dict]:
        if encoders is None:
            encoded_data = self._encoder.transform(self._df)
            new_loader = self.decorate(encoded_data)
            return new_loader, {"syn_seq_encoder": self._encoder}
        else:
            return self, encoders

    def decode(self, encoders: Dict[str, Any]) -> "Syn_SeqDataLoader":
        if "syn_seq_encoder" in encoders:
            encoder = encoders["syn_seq_encoder"]
            if not isinstance(encoder, Syn_SeqEncoder):
                raise TypeError(f"Expected Syn_SeqEncoder, got {type(encoder)}")
            decoded_data = encoder.inverse_transform(self._df)
            return self.decorate(decoded_data)
        return self

    def decorate(self, data: pd.DataFrame) -> "Syn_SeqDataLoader":
        """
        Helper for creating a new instance with the same settings but new data.
        """
        return Syn_SeqDataLoader(
            data=data,
            syn_order=self.syn_order,
            special_value=self.columns_special_values,
            col_type=self.col_type,
            max_categories=self.max_categories,
            random_state=self.random_state,
            train_size=self.train_size,
        )


src/synthcity/plugins/generic/plugin_syn_seq.py
# File: plugins/generic/plugin_syn_seq.py

from typing import Any, Dict, List, Optional, Union

import pandas as pd
from random import sample
import numpy as np

# synthcity absolute
from synthcity.plugins.core.plugin import Plugin
from synthcity.plugins.core.dataloader import DataLoader, GenericDataLoader, Syn_SeqDataLoader
from synthcity.plugins.core.schema import Schema
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.models.syn_seq_encoder import Syn_SeqEncoder

class Syn_SeqPlugin(Plugin):
    """
    A plugin for a sequential (column-by-column) synthetic data approach,
    akin to R's 'synthpop' but in Python. It integrates:

    1) A specialized DataLoader (Syn_SeqDataLoader) that reorders columns, handles special values, etc.
    2) An encoder (Syn_SeqEncoder) that splits numeric columns, marks date columns, etc.
    3) A simple column-by-column modeling approach for synthetic generation.
    4) Constraint handling (post-hoc '=' assignment or hooking into parent `_safe_generate` if strict is True).

    Usage example:
        loader = Syn_SeqDataLoader(
            data = my_dataframe,
            syn_order = ["C2", "C1", "N2", "N1", "D1"],
            special_value = {"N2": [-888, 888], "D1": [0]},
            col_type = {"C2":"category", "C1":"category", "N2":"numeric", "N1":"numeric", "D1":"date"}
        )

        syn_model = Plugins().get("syn_seq")  # or Syn_SeqPlugin()
        syn_model.fit(
            loader,
            method = ["SWR", "CART", "CART", "pmm", "CART"],
            variable_selection = {
                "N1": ["C2", "C1"],
                "D1": ["C2", "C1", "N2"]
            }
        )

        constraint = {
            "N1": [">", 125],
            # "C1": ["in", ["AAA","BBB"]],
            # "D1": ["=", "2020-01-01"]
        }

        syn_df = syn_model.generate(count=100, constraint=constraint).dataframe()
    """

    @staticmethod
    def name() -> str:
        return "syn_seq"

    @staticmethod
    def type() -> str:
        return "syn_seq"

    @staticmethod
    def hyperparameter_space(**kwargs: Any) -> list:
        # No tunable hyperparameters here; placeholders for future extension.
        return []

    def __init__(
        self,
        random_state: int = 0,
        default_first_method: str = "SWR",
        default_other_method: str = "CART",
        **kwargs: Any,
    ):
        """
        Args:
            random_state: Fixes seed for reproducibility.
            default_first_method: method for the first column if user doesn't override. e.g. 'SWR'.
            default_other_method: method for subsequent columns if user doesn't override. e.g. 'CART'.
            **kwargs: passes through to Plugin base (e.g. strict, sampling_patience, workspace, etc.)
        """
        super().__init__(random_state=random_state, **kwargs)

        # Where we store each column's fitted "model": { col : { "method":..., "predictors":..., "model":... } }
        self._column_models: Dict[str, dict] = {}

        # For user configuration
        self.method_list: List[str] = []
        self.variable_selection: Dict[str, Union[List[str], List[int]]] = {}

        # If user doesn't supply a method for each column, we fill with these defaults:
        self.default_first_method = default_first_method
        self.default_other_method = default_other_method

        # Encoders, training state
        self._encoders: Dict[str, Any] = {}
        self._model_trained = False

    # ----------------------------------------------------------------
    # OVERRIDES FROM Plugin
    # ----------------------------------------------------------------

    def _fit(
        self,
        X: DataLoader,
        method: Optional[List[str]] = None,
        variable_selection: Optional[Dict[str, List[str]]] = None,
        *args: Any,
        **kwargs: Any,
    ) -> "Syn_SeqPlugin":
        """
        Internal training logic, after parent's .fit sets up schema, etc.

        Steps:
          - Possibly expand user-supplied method_list so each column has a method
          - Build or adapt variable_selection matrix
          - Print out final assignments
          - "Train" a minimal column model
        """
        if not isinstance(X, Syn_SeqDataLoader):
            raise TypeError("Syn_SeqPlugin expects a Syn_SeqDataLoader after .encode()")

        df = X.dataframe()
        col_list = list(df.columns)
        n_cols = len(col_list)

        # capture user parameters if they exist
        if method is not None:
            self.method_list = method
        if variable_selection is not None:
            self.variable_selection = variable_selection

        # Expand/assign final column methods
        final_methods = []
        for i, col in enumerate(col_list):
            if i < len(self.method_list):
                final_methods.append(self.method_list[i])
            else:
                # fallback
                fallback = self.default_first_method if i == 0 else self.default_other_method
                final_methods.append(fallback)

        # Print final method assignments
        print("[INFO] Final column method assignment:")
        for col, m in zip(col_list, final_methods):
            print(f"  {col}: {m}")

        # Build a default variable_selection matrix if user doesn't supply one
        vs_matrix = pd.DataFrame(0, index=col_list, columns=col_list)
        for i in range(n_cols):
            vs_matrix.iloc[i, :i] = 1

        # overlay user-specified variable_selection
        for col, preds in self.variable_selection.items():
            if col not in vs_matrix.index:
                continue
            vs_matrix.loc[col, :] = 0
            for p in preds:
                if p in vs_matrix.columns:
                    vs_matrix.loc[col, p] = 1

        print("[INFO] Final variable selection matrix:")
        print(vs_matrix)

        # "Train" each column
        self._column_models.clear()
        for i, col in enumerate(col_list):
            chosen_method = final_methods[i]
            predictor_cols = vs_matrix.columns[(vs_matrix.loc[col] == 1)].tolist()

            model_info = self._train_column_model(
                df,
                target_col=col,
                predictor_cols=predictor_cols,
                method=chosen_method,
            )
            self._column_models[col] = {
                "method": chosen_method,
                "predictors": predictor_cols,
                "model": model_info,
            }

        self._model_trained = True
        return self

    def _generate(
        self,
        count: int,
        syn_schema: Schema,
        **kwargs: Any
    ) -> DataLoader:
        """
        Our core row-by-row synthetic generation logic, invoked by parent's `_safe_generate`.

        Steps:
          - Create an empty DF of size 'count'
          - For each column in order, sample values from the column model
          - Return as a GenericDataLoader
        """
        col_list = list(self._column_models.keys())
        syn_df = pd.DataFrame(index=range(count))

        for col in col_list:
            info = self._column_models[col]
            method = info["method"]
            preds = info["predictors"]
            model_obj = info["model"]

            new_values = self._generate_for_column(
                count,
                col,
                method,
                preds,
                model_obj,
                partial_df=syn_df,
            )
            syn_df[col] = new_values

        return GenericDataLoader(syn_df)

 

src/synthcity/plugins/core/models/syn_seq_constraints.py
# File: syn_seq_constraints.py

from typing import Any, List, Dict, Tuple, Union
import pandas as pd
import numpy as np

# We import the base Constraints to inherit from
from synthcity.plugins.core.constraints import Constraints

# 
# A single sub-rule is (feature, op, value).
# For "chained" logic, we might interpret:
#    "For rows that pass all sub-rules (1..k-1), apply sub-rule k as a filter or correction."
#
# Example constraint input:
#   {
#       "N1": [
#           ("C1", "in", ["AAA","BBB"]),
#           ("N1", ">", 125)
#       ]
#   }
# means:
#  1) If a row passes (C1 in [AAA,BBB]), 
#  2) Then also enforce (N1 > 125) on that row.
#
# If the first sub-rule is not satisfied, the second doesn't apply to that row.
# If the first sub-rule is satisfied but not the second => row fails entirely.
#

class SynSeqConstraints(Constraints):
    """
    An extension that supports "chained" constraints for sequential logic:
      - If sub-rule 1 is satisfied => we must also pass sub-rule 2, and so on.
      - 'chained_rules' can be a dict of { targetCol : [ (feature, op, val), (feature, op, val), ... ] }.
        Example:
          {
            "N1": [
                ("C1", "in", ["AAA","BBB"]),
                ("N1", ">", 125)
            ]
          }
        read as: "If (C1 in [AAA,BBB]) => enforce (N1 > 125)".
      
      Each sub-rule uses the same _eval logic for <, <=, >, >=, ==, in, etc.
      If a sub-rule fails => that entire row fails (filtered out) if using .match(), 
      or is corrected if possible (.correct).
    """

    def __init__(
        self,
        # You can still pass standard constraints as "rules",
        # or pass new "chained_rules" in dict format
        rules: List[Tuple[str, str, Any]] = None,
        chained_rules: Dict[str, List[Tuple[str, str, Any]]] = None,
        seq_id_feature: str = "seq_id",
        seq_time_id_feature: str = "seq_time_id",
        **kwargs: Any,
    ):
        # Let the base constructor handle standard 'rules'
        super().__init__(rules=rules if rules else [])
        self.seq_id_feature = seq_id_feature
        self.seq_time_id_feature = seq_time_id_feature

        # We'll store the "chained" sub-rules in a separate structure
        # keyed by "target column" or "some label"
        self.chained_rules = chained_rules if chained_rules else {}

    def match(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Overridden. We can do row-level or entire-sequence filtering. 
        If you still want entire-sequence filtering, you can define match_sequential() and call it below.
        For demonstration, let's do row-level filtering with chained sub-rules.
        """
        df_copy = X.copy()
        # first, apply base constraints (self.rules) at row-level:
        base_mask = super().filter(df_copy)
        df_filtered = df_copy[base_mask].copy()
        if df_filtered.empty:
            return df_filtered

        # next, apply "chained" constraints
        df_filtered = self._match_chained(df_filtered)
        return df_filtered

    def _match_chained(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        For each entry in self.chained_rules => interpret a *sequence* of sub-rules.
        If the row passes sub-rule[0], sub-rule[1], ... sub-rule[n-2], 
        then sub-rule[n-1] is also enforced. 
        If it fails at any step => the row is filtered out or "corrected" (depending on your logic).
        
        For simplicity below, we do "if sub-rule[i] is satisfied => proceed, else row is out."
        """
        df = X.copy()
        for target_key, rule_chain in self.chained_rules.items():
            # We interpret the chain in order
            # e.g. [("C1","in", [...]), ("N1",">",125)]
            # step i=0 => a filter => if row fails => out
            # step i=1 => a further filter => if row fails => out
            # etc.

            # We'll build a mask for these sub-rules
            keep_mask = pd.Series([True]*len(df), index=df.index)

            for (feature, op, operand) in rule_chain:
                cur_mask = self._eval(df, feature, op, operand)
                # only keep the rows that pass this sub-rule
                keep_mask = keep_mask & cur_mask

            # after we apply all sub-rules in the chain, 
            # rows that didn't pass => out
            df = df[keep_mask].copy()
            if df.empty:
                break

        return df

    def _eval(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.Index:
        """
        If we want to also handle direct substitution '=' or '==' or to skip it, we can override _eval.
        If the user wants a different meaning for '=' in chain logic, we can do so.
        Otherwise, we rely on base Constraints._eval for <, <=, >, >=, ==, in, dtype, etc.
        """
        # If you want direct substitution for '=' or '==', do so in _correct or in some separate logic.
        if op in ["=", "=="]:
            # interpret as equality check
            return (X[feature] == operand) | X[feature].isna()
        else:
            # fallback to base method
            return super()._eval(X, feature, op, operand)

    def _correct(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.DataFrame:
        """
        If user wants direct substitution for '=' => set that col's entire column to operand. 
        Or you can do row-level changes only for failing rows, etc.
        """
        if op in ["=", "=="]:
            X.loc[:, feature] = operand
            return X
        return super()._correct(X, feature, op, operand)

    # If you want entire-sequence logic, you'd define match_sequential below:

    # Example
    #     constraint = {
    #   "N1": [
    #     ["C1", "in", ["AAA","BBB"]],
    #     ["N1", ">", 125]
    #   ]
    # }

    # def match_sequential(self, X: pd.DataFrame) -> pd.DataFrame:
    #     """
    #     Example method that applies constraints at the group (sequence) level:
    #       - if a sub-rule fails for ANY row => remove entire sequence
    #       - or do direct substitution in entire sequence
    #     """
    #     df_copy = X.copy()
    #     base_mask = super().filter(df_copy)
    #     # group by seq_id
    #     grouped = df_copy.groupby(self.seq_id_feature)
    #
    #     keep_seq_ids = []
    #     for seq_id, group in grouped:
    #         # if all rows pass => keep entire sequence
    #         if base_mask[group.index].all():
    #             # next, check chained
    #             # for each sub-rule chain, we can test if group passes
    #             # if not => exclude entire seq
    #             # or we can do partial correction
    #             # ...
    #             keep_seq_ids.append(seq_id)
    #
    #     return df_copy[df_copy[self.seq_id_feature].isin(keep_seq_ids)]

REFERENCE CODE FROM Synthcity

src/synthcity/plugins/generic/plugin_ctgan.py
"""
Reference: "Modeling Tabular Data using Conditional GAN", Xu, Lei et al.
"""

# stdlib
from pathlib import Path
from typing import Any, List, Optional, Union

# third party
import numpy as np
import pandas as pd

# Necessary packages
from pydantic import validate_arguments
from torch.utils.data import sampler

# synthcity absolute
from synthcity.metrics.weighted_metrics import WeightedMetrics
from synthcity.plugins.core.dataloader import DataLoader
from synthcity.plugins.core.distribution import (
    CategoricalDistribution,
    Distribution,
    FloatDistribution,
    IntegerDistribution,
)
from synthcity.plugins.core.models.tabular_gan import TabularGAN
from synthcity.plugins.core.plugin import Plugin
from synthcity.plugins.core.schema import Schema
from synthcity.utils.constants import DEVICE


class CTGANPlugin(Plugin):
    """
    .. inheritance-diagram:: synthcity.plugins.generic.plugin_ctgan.CTGANPlugin
        :parts: 1


    Conditional Tabular GAN implementation.

    Args:
        generator_n_layers_hidden: int
            Number of hidden layers in the generator
        generator_n_units_hidden: int
            Number of hidden units in each layer of the Generator
        generator_nonlin: string, default 'leaky_relu'
            Nonlinearity to use in the generator. Can be 'elu', 'relu', 'selu' or 'leaky_relu'.
        n_iter: int
            Maximum number of iterations in the Generator.
        generator_dropout: float
            Dropout value. If 0, the dropout is not used.
        discriminator_n_layers_hidden: int
            Number of hidden layers in the discriminator
        discriminator_n_units_hidden: int
            Number of hidden units in each layer of the discriminator
        discriminator_nonlin: string, default 'leaky_relu'
            Nonlinearity to use in the discriminator. Can be 'elu', 'relu', 'selu' or 'leaky_relu'.
        discriminator_n_iter: int
            Maximum number of iterations in the discriminator.
        discriminator_dropout: float
            Dropout value for the discriminator. If 0, the dropout is not used.
        lr: float
            learning rate for optimizer.
        weight_decay: float
            l2 (ridge) penalty for the weights.
        batch_size: int
            Batch size
        random_state: int
            random seed to use
        clipping_value: int, default 0
            Gradients clipping value. Zero disables the feature
        encoder_max_clusters: int
            The max number of clusters to create for continuous columns when encoding
        adjust_inference_sampling: bool
            Adjust the marginal probabilities in the synthetic data to closer match the training set. Active only with the ConditionalSampler
        # early stopping
        n_iter_print: int
            Number of iterations after which to print updates and check the validation loss.
        n_iter_min: int
            Minimum number of iterations to go through before starting early stopping
        patience: int
            Max number of iterations without any improvement before early stopping is trigged.
        patience_metric: Optional[WeightedMetrics]
            If not None, the metric is used for evaluation the criterion for early stopping.
        # Core Plugin arguments
        workspace: Path.
            Optional Path for caching intermediary results.
        compress_dataset: bool. Default = False.
            Drop redundant features before training the generator.
        sampling_patience: int.
            Max inference iterations to wait for the generated data to match the training schema.
    Example:
        >>> from sklearn.datasets import load_iris
        >>> from synthcity.plugins import Plugins
        >>>
        >>> X, y = load_iris(as_frame = True, return_X_y = True)
        >>> X["target"] = y
        >>>
        >>> plugin = Plugins().get("ctgan", n_iter = 100)
        >>> plugin.fit(X)
        >>>
        >>> plugin.generate(50)

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        n_iter: int = 2000,
        generator_n_layers_hidden: int = 2,
        generator_n_units_hidden: int = 500,
        generator_nonlin: str = "relu",
        generator_dropout: float = 0.1,
        generator_opt_betas: tuple = (0.5, 0.999),
        discriminator_n_layers_hidden: int = 2,
        discriminator_n_units_hidden: int = 500,
        discriminator_nonlin: str = "leaky_relu",
        discriminator_n_iter: int = 1,
        discriminator_dropout: float = 0.1,
        discriminator_opt_betas: tuple = (0.5, 0.999),
        lr: float = 1e-3,
        weight_decay: float = 1e-3,
        batch_size: int = 200,
        random_state: int = 0,
        clipping_value: int = 1,
        lambda_gradient_penalty: float = 10,
        encoder_max_clusters: int = 10,
        encoder: Any = None,
        dataloader_sampler: Optional[sampler.Sampler] = None,
        device: Any = DEVICE,
        patience: int = 5,
        patience_metric: Optional[WeightedMetrics] = None,
        n_iter_print: int = 50,
        n_iter_min: int = 100,
        adjust_inference_sampling: bool = False,
        # core plugin arguments
        workspace: Path = Path("workspace"),
        compress_dataset: bool = False,
        sampling_patience: int = 500,
        **kwargs: Any
    ) -> None:
        super().__init__(
            device=device,
            random_state=random_state,
            sampling_patience=sampling_patience,
            workspace=workspace,
            compress_dataset=compress_dataset,
            **kwargs
        )
        if patience_metric is None:
            patience_metric = WeightedMetrics(
                metrics=[("detection", "detection_mlp")],
                weights=[1],
                workspace=workspace,
            )
        self.generator_n_layers_hidden = generator_n_layers_hidden
        self.generator_n_units_hidden = generator_n_units_hidden
        self.generator_nonlin = generator_nonlin
        self.n_iter = n_iter
        self.generator_dropout = generator_dropout
        self.generator_opt_betas = generator_opt_betas
        self.discriminator_n_layers_hidden = discriminator_n_layers_hidden
        self.discriminator_n_units_hidden = discriminator_n_units_hidden
        self.discriminator_nonlin = discriminator_nonlin
        self.discriminator_n_iter = discriminator_n_iter
        self.discriminator_dropout = discriminator_dropout
        self.discriminator_opt_betas = discriminator_opt_betas

        self.lr = lr
        self.weight_decay = weight_decay
        self.batch_size = batch_size
        self.random_state = random_state
        self.clipping_value = clipping_value
        self.lambda_gradient_penalty = lambda_gradient_penalty

        self.encoder_max_clusters = encoder_max_clusters
        self.encoder = encoder
        self.dataloader_sampler = dataloader_sampler

        self.device = device
        self.patience = patience
        self.patience_metric = patience_metric
        self.n_iter_min = n_iter_min
        self.n_iter_print = n_iter_print
        self.adjust_inference_sampling = adjust_inference_sampling

    @staticmethod
    def name() -> str:
        return "ctgan"

    @staticmethod
    def type() -> str:
        return "generic"

    @staticmethod
    def hyperparameter_space(**kwargs: Any) -> List[Distribution]:
        return [
            IntegerDistribution(name="generator_n_layers_hidden", low=1, high=4),
            IntegerDistribution(
                name="generator_n_units_hidden", low=50, high=150, step=50
            ),
            CategoricalDistribution(
                name="generator_nonlin", choices=["relu", "leaky_relu", "tanh", "elu"]
            ),
            IntegerDistribution(name="n_iter", low=100, high=1000, step=100),
            FloatDistribution(name="generator_dropout", low=0, high=0.2),
            IntegerDistribution(name="discriminator_n_layers_hidden", low=1, high=4),
            IntegerDistribution(
                name="discriminator_n_units_hidden", low=50, high=150, step=50
            ),
            CategoricalDistribution(
                name="discriminator_nonlin",
                choices=["relu", "leaky_relu", "tanh", "elu"],
            ),
            IntegerDistribution(name="discriminator_n_iter", low=1, high=5),
            FloatDistribution(name="discriminator_dropout", low=0, high=0.2),
            CategoricalDistribution(name="lr", choices=[1e-3, 2e-4, 1e-4]),
            CategoricalDistribution(name="weight_decay", choices=[1e-3, 1e-4]),
            CategoricalDistribution(name="batch_size", choices=[100, 200, 500]),
            IntegerDistribution(name="encoder_max_clusters", low=2, high=20),
        ]

    def _prepare_cond(
        self, cond: Optional[Union[pd.DataFrame, pd.Series, np.ndarray, list]]
    ) -> Optional[np.ndarray]:
        if cond is None:
            return None

        cond = np.asarray(cond)
        if len(cond.shape) == 1:
            cond = cond.reshape(-1, 1)

        return cond

    def _fit(self, X: DataLoader, *args: Any, **kwargs: Any) -> "CTGANPlugin":
        cond: Optional[Union[pd.DataFrame, pd.Series]] = None
        if "cond" in kwargs:
            cond = self._prepare_cond(kwargs["cond"])

        self.model = TabularGAN(
            X.dataframe(),
            cond=cond,
            n_units_latent=self.generator_n_units_hidden,
            batch_size=self.batch_size,
            generator_n_layers_hidden=self.generator_n_layers_hidden,
            generator_n_units_hidden=self.generator_n_units_hidden,
            generator_nonlin=self.generator_nonlin,
            generator_nonlin_out_discrete="softmax",
            generator_nonlin_out_continuous="none",
            generator_lr=self.lr,
            generator_residual=True,
            generator_n_iter=self.n_iter,
            generator_batch_norm=False,
            generator_dropout=0,
            generator_weight_decay=self.weight_decay,
            generator_opt_betas=self.generator_opt_betas,
            generator_extra_penalties=[],
            discriminator_n_units_hidden=self.discriminator_n_units_hidden,
            discriminator_n_layers_hidden=self.discriminator_n_layers_hidden,
            discriminator_n_iter=self.discriminator_n_iter,
            discriminator_nonlin=self.discriminator_nonlin,
            discriminator_batch_norm=False,
            discriminator_dropout=self.discriminator_dropout,
            discriminator_lr=self.lr,
            discriminator_weight_decay=self.weight_decay,
            discriminator_opt_betas=self.discriminator_opt_betas,
            encoder=self.encoder,
            clipping_value=self.clipping_value,
            lambda_gradient_penalty=self.lambda_gradient_penalty,
            encoder_max_clusters=self.encoder_max_clusters,
            dataloader_sampler=self.dataloader_sampler,
            device=self.device,
            patience=self.patience,
            patience_metric=self.patience_metric,
            n_iter_min=self.n_iter_min,
            n_iter_print=self.n_iter_print,
            adjust_inference_sampling=self.adjust_inference_sampling,
        )
        self.model.fit(X.dataframe(), cond=cond)

        return self

    def _generate(self, count: int, syn_schema: Schema, **kwargs: Any) -> DataLoader:
        cond: Optional[Union[pd.DataFrame, pd.Series]] = None
        if "cond" in kwargs:
            cond = self._prepare_cond(kwargs["cond"])

        return self._safe_generate(self.model.generate, count, syn_schema, cond=cond)


plugin = CTGANPlugin


src/synthcity/plugins/core/models/tabular_gan.py
# stdlib
from typing import Any, Callable, Optional, Union

# third party
import numpy as np
import pandas as pd
import torch
from pydantic import validate_arguments
from scipy.optimize import minimize
from scipy.special import logsumexp
from sklearn.preprocessing import OneHotEncoder

# synthcity absolute
from synthcity.metrics.weighted_metrics import WeightedMetrics
from synthcity.utils.constants import DEVICE
from synthcity.utils.samplers import BaseSampler, ConditionalDatasetSampler

# synthcity relative
from .gan import GAN
from .tabular_encoder import TabularEncoder


class TabularGAN(torch.nn.Module):
    """
    .. inheritance-diagram:: synthcity.plugins.core.models.tabular_gan.TabularGAN
        :parts: 1


    GAN for tabular data.

    This class combines GAN and tabular encoder to form a generative model for tabular data.

    Args:
        X: pd.DataFrame
            Reference dataset, used for training the tabular encoder
        n_units_latent: int
            Number of latent units
        cond: Optional
            Optional conditional
        generator_n_layers_hidden: int
            Number of hidden layers in the generator
        generator_n_units_hidden: int
            Number of hidden units in each layer of the Generator
        generator_nonlin: string, default 'elu'
            Nonlinearity to use in the generator. Can be 'elu', 'relu', 'selu' or 'leaky_relu'.
        generator_n_iter: int
            Maximum number of iterations in the Generator.
        generator_batch_norm: bool
            Enable/disable batch norm for the generator
        generator_dropout: float
            Dropout value. If 0, the dropout is not used.
        generator_residual: bool
            Use residuals for the generator
        generator_nonlin_out: Optional[List[Tuple[str, int]]]
            List of activations. Useful with the TabularEncoder
        generator_lr: float = 2e-4
            Generator learning rate, used by the Adam optimizer
        generator_weight_decay: float = 1e-3
            Generator weight decay, used by the Adam optimizer
        generator_opt_betas: tuple = (0.9, 0.999)
            Generator initial decay rates, used by the Adam Optimizer
        generator_extra_penalties: list
            Additional penalties for the generator. Values: "identifiability_penalty"
        generator_extra_penalty_cbks: List[Callable]
            Additional loss callabacks for the generator. Used by the TabularGAN for the conditional loss
        discriminator_n_layers_hidden: int
            Number of hidden layers in the discriminator
        discriminator_n_units_hidden: int
            Number of hidden units in each layer of the discriminator
        discriminator_nonlin: string, default 'relu'
            Nonlinearity to use in the discriminator. Can be 'elu', 'relu', 'selu' or 'leaky_relu'.
        discriminator_n_iter: int
            Maximum number of iterations in the discriminator.
        discriminator_batch_norm: bool
            Enable/disable batch norm for the discriminator
        discriminator_dropout: float
            Dropout value for the discriminator. If 0, the dropout is not used.
        discriminator_lr: float
            Discriminator learning rate, used by the Adam optimizer
        discriminator_weight_decay: float
            Discriminator weight decay, used by the Adam optimizer
        discriminator_opt_betas: tuple
            Initial weight decays for the Adam optimizer
        batch_size: int
            Batch size
        n_iter_print: int
            Number of iterations after which to print updates and check the validation loss.
        random_state: int
            random_state used
        n_iter_min: int
            Minimum number of iterations to go through before starting early stopping
        clipping_value: int, default 0
            Gradients clipping value. Zero disables the feature
        lambda_gradient_penalty: float = 10
            Weight for the gradient penalty
        lambda_identifiability_penalty: float = 0.1
            Weight for the identifiability penalty, if enabled
        dataloader_sampler: Optional[sampler.Sampler]
            Optional sampler for the dataloader, useful for conditional sampling
        device: Any = DEVICE
            CUDA/CPU
        adjust_inference_sampling: bool
            Adjust the marginal probabilities in the synthetic data to closer match the training set. Active only with the ConditionalSampler
        # privacy settings
        dp_enabled: bool
            Train the discriminator with Differential Privacy guarantees
        dp_delta: Optional[float]
            Optional DP delta: the probability of information accidentally being leaked. Usually 1 / len(dataset)
        dp_epsilon: float = 3
            DP epsilon: privacy budget, which is a measure of the amount of privacy that is preserved by a given algorithm. Epsilon is a number that represents the maximum amount of information that an adversary can learn about an individual from the output of a differentially private algorithm. The smaller the value of epsilon, the more private the algorithm is. For example, an algorithm with an epsilon of 0.1 preserves more privacy than an algorithm with an epsilon of 1.0.
        dp_max_grad_norm: float
            max grad norm used for gradient clipping
        dp_secure_mode: bool = False,
             if True uses noise generation approach robust to floating point arithmetic attacks.

        encoder_max_clusters: int
            The max number of clusters to create for continuous columns when encoding
        encoder:
            Pre-trained tabular encoder. If None, a new encoder is trained.
        encoder_whitelist:
            Ignore columns from encoding
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        X: pd.DataFrame,
        n_units_latent: int,
        cond: Optional[Union[pd.DataFrame, pd.Series, np.ndarray]] = None,
        generator_n_layers_hidden: int = 2,
        generator_n_units_hidden: int = 150,
        generator_nonlin: str = "leaky_relu",
        generator_nonlin_out_discrete: str = "softmax",
        generator_nonlin_out_continuous: str = "none",
        generator_n_iter: int = 1000,
        generator_batch_norm: bool = False,
        generator_dropout: float = 0.01,
        generator_lr: float = 1e-3,
        generator_weight_decay: float = 1e-3,
        generator_opt_betas: tuple = (0.9, 0.999),
        generator_residual: bool = True,
        generator_extra_penalties: list = [],  # "identifiability_penalty"
        discriminator_n_layers_hidden: int = 3,
        discriminator_n_units_hidden: int = 300,
        discriminator_nonlin: str = "leaky_relu",
        discriminator_n_iter: int = 1,
        discriminator_batch_norm: bool = False,
        discriminator_dropout: float = 0.1,
        discriminator_lr: float = 1e-3,
        discriminator_weight_decay: float = 1e-3,
        discriminator_opt_betas: tuple = (0.9, 0.999),
        batch_size: int = 64,
        random_state: int = 0,
        clipping_value: int = 0,
        lambda_gradient_penalty: float = 10,
        lambda_identifiability_penalty: float = 0.1,
        encoder_max_clusters: int = 20,
        encoder: Any = None,
        encoder_whitelist: list = [],
        dataloader_sampler: Optional[BaseSampler] = None,
        device: Any = DEVICE,
        patience: int = 10,
        patience_metric: Optional[WeightedMetrics] = None,
        n_iter_print: int = 50,
        n_iter_min: int = 100,
        adjust_inference_sampling: bool = False,
        # privacy settings
        dp_enabled: bool = False,
        dp_epsilon: float = 3,
        dp_delta: Optional[float] = None,
        dp_max_grad_norm: float = 2,
        dp_secure_mode: bool = False,
    ) -> None:
        super(TabularGAN, self).__init__()
        self.columns = X.columns
        self.batch_size = batch_size
        self.sample_prob: Optional[np.ndarray] = None
        self._adjust_inference_sampling = adjust_inference_sampling
        n_units_conditional = 0

        if encoder is not None:
            self.encoder = encoder
        else:
            self.encoder = TabularEncoder(
                max_clusters=encoder_max_clusters, whitelist=encoder_whitelist
            ).fit(X)

        self.cond_encoder: Optional[OneHotEncoder] = None
        if cond is not None:
            cond = np.asarray(cond)
            if len(cond.shape) == 1:
                cond = cond.reshape(-1, 1)

            self.cond_encoder = OneHotEncoder(handle_unknown="ignore").fit(cond)
            cond = self.cond_encoder.transform(cond).toarray()

            n_units_conditional = cond.shape[-1]

        self.predefined_conditional = cond is not None

        if (
            dataloader_sampler is None and not self.predefined_conditional
        ):  # don't mix conditionals
            dataloader_sampler = ConditionalDatasetSampler(
                self.encoder.transform(X),
                self.encoder.layout(),
            )
            n_units_conditional = dataloader_sampler.conditional_dimension()

        self.dataloader_sampler = dataloader_sampler

        def _generator_cond_loss(
            real_samples: torch.tensor,
            fake_samples: torch.Tensor,
            cond: Optional[torch.Tensor],
        ) -> torch.Tensor:
            if cond is None or self.predefined_conditional:
                return 0

            losses = []

            idx = 0
            cond_idx = 0

            for item in self.encoder.layout():
                length = item.output_dimensions

                if item.feature_type != "discrete":
                    idx += length
                    continue

                # create activate feature mask
                mask = cond[:, cond_idx : cond_idx + length].sum(axis=1).bool()

                if mask.sum() == 0:
                    idx += length
                    continue

                if not (fake_samples[mask, idx : idx + length] >= 0).all():
                    raise RuntimeError(
                        f"Invalid samples after softmax = {fake_samples[mask, idx : idx + length]}"
                    )
                # fake_samples are after the Softmax activation
                # we filter active features in the mask
                item_loss = torch.nn.NLLLoss()(
                    torch.log(fake_samples[mask, idx : idx + length] + 1e-8),
                    torch.argmax(real_samples[mask, idx : idx + length], dim=1),
                )
                losses.append(item_loss)

                cond_idx += length
                idx += length

            if idx != real_samples.shape[1]:
                raise ValueError(
                    f"Invalid offset idx = {idx}; real_samples.shape = {real_samples.shape}"
                )

            if len(losses) == 0:
                return 0

            loss = torch.stack(losses, dim=-1)

            return loss.sum() / len(real_samples)

        self.model = GAN(
            self.encoder.n_features(),
            n_units_latent=n_units_latent,
            n_units_conditional=n_units_conditional,
            batch_size=batch_size,
            generator_n_layers_hidden=generator_n_layers_hidden,
            generator_n_units_hidden=generator_n_units_hidden,
            generator_nonlin=generator_nonlin,
            generator_nonlin_out=self.encoder.activation_layout(
                discrete_activation=generator_nonlin_out_discrete,
                continuous_activation=generator_nonlin_out_continuous,
            ),
            generator_n_iter=generator_n_iter,
            generator_batch_norm=generator_batch_norm,
            generator_dropout=generator_dropout,
            generator_lr=generator_lr,
            generator_residual=generator_residual,
            generator_weight_decay=generator_weight_decay,
            generator_opt_betas=generator_opt_betas,
            generator_extra_penalties=generator_extra_penalties,
            generator_extra_penalty_cbks=[_generator_cond_loss],
            discriminator_n_units_hidden=discriminator_n_units_hidden,
            discriminator_n_layers_hidden=discriminator_n_layers_hidden,
            discriminator_n_iter=discriminator_n_iter,
            discriminator_nonlin=discriminator_nonlin,
            discriminator_batch_norm=discriminator_batch_norm,
            discriminator_dropout=discriminator_dropout,
            discriminator_lr=discriminator_lr,
            discriminator_weight_decay=discriminator_weight_decay,
            discriminator_opt_betas=discriminator_opt_betas,
            lambda_gradient_penalty=lambda_gradient_penalty,
            lambda_identifiability_penalty=lambda_identifiability_penalty,
            clipping_value=clipping_value,
            n_iter_print=n_iter_print,
            random_state=random_state,
            # early stopping
            n_iter_min=n_iter_min,
            dataloader_sampler=dataloader_sampler,
            device=device,
            patience=patience,
            patience_metric=patience_metric,
            # privacy
            dp_enabled=dp_enabled,
            dp_epsilon=dp_epsilon,
            dp_delta=dp_delta,
            dp_max_grad_norm=dp_max_grad_norm,
            dp_secure_mode=dp_secure_mode,
        )

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def encode(self, X: pd.DataFrame) -> pd.DataFrame:
        return self.encoder.transform(X)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def decode(self, X: pd.DataFrame) -> pd.DataFrame:
        return self.encoder.inverse_transform(X)

    def get_encoder(self) -> TabularEncoder:
        return self.encoder

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def fit(
        self,
        X: pd.DataFrame,
        cond: Optional[Union[pd.DataFrame, pd.Series, np.ndarray]] = None,
        fake_labels_generator: Optional[Callable] = None,
        true_labels_generator: Optional[Callable] = None,
        encoded: bool = False,
    ) -> Any:
        # preprocessing
        if encoded:
            X_enc = X
        else:
            X_enc = self.encode(X)

        if cond is not None and self.cond_encoder is not None:
            cond = np.asarray(cond)
            if len(cond.shape) == 1:
                cond = cond.reshape(-1, 1)

            cond = self.cond_encoder.transform(cond).toarray()

        if not self.predefined_conditional and self.dataloader_sampler is not None:
            cond = self.dataloader_sampler.get_dataset_conditionals()

        if cond is not None:
            if len(cond) != len(X_enc):
                raise ValueError(
                    f"Invalid conditional shape. {cond.shape} expected {len(X_enc)}"
                )

        # training
        self.model.fit(
            np.asarray(X_enc),
            np.asarray(cond),
            fake_labels_generator=fake_labels_generator,
            true_labels_generator=true_labels_generator,
        )

        # post processing
        self.adjust_inference_sampling(self._adjust_inference_sampling)

        return self

    def adjust_inference_sampling(self, enabled: bool) -> None:
        if self.predefined_conditional or self.dataloader_sampler is None:
            return

        self._adjust_inference_sampling = enabled

        if enabled:
            real_prob = self.dataloader_sampler.conditional_probs()
            sample_prob = self._extract_sample_prob()

            self.sample_prob = self._find_sample_p(real_prob, sample_prob)
        else:
            self.sample_prob = None

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def generate(
        self,
        count: int,
        cond: Optional[Union[pd.DataFrame, pd.Series, np.ndarray]] = None,
    ) -> pd.DataFrame:
        samples = self(count, cond)
        return self.decode(pd.DataFrame(samples))

    def forward(
        self, count: int, cond: Optional[Union[pd.DataFrame, np.ndarray]] = None
    ) -> torch.Tensor:
        if cond is not None and self.cond_encoder is not None:
            cond = np.asarray(cond)
            if len(cond.shape) == 1:
                cond = cond.reshape(-1, 1)

            cond = self.cond_encoder.transform(cond).toarray()

        if not self.predefined_conditional and self.dataloader_sampler is not None:
            cond = self.dataloader_sampler.sample_conditional(count, p=self.sample_prob)

        return self.model.generate(count, cond=cond)

    def _extract_sample_prob(self) -> Optional[np.ndarray]:
        if self.predefined_conditional or self.dataloader_sampler is None:
            return None

        if self.dataloader_sampler.conditional_dimension() == 0:
            return None

        prob_list = list()
        batch_size = 10000

        for c in range(self.dataloader_sampler.conditional_dimension()):
            cond = self.dataloader_sampler.sample_conditional_for_class(batch_size, c)
            if cond is None:
                continue

            data_cond = self.model.generate(batch_size, cond=cond)

            syn_dataloader_sampler = ConditionalDatasetSampler(
                pd.DataFrame(data_cond),
                self.encoder.layout(),
            )

            prob = syn_dataloader_sampler.conditional_probs()
            prob_list.append(prob)

        prob_mat = np.stack(prob_list, axis=-1)

        return prob_mat

    def _find_sample_p(
        self, prob_real: Optional[np.ndarray], prob_mat: Optional[np.ndarray]
    ) -> Optional[np.ndarray]:
        if prob_real is None or prob_mat is None:
            return None

        def kl(
            alpha: np.ndarray, prob_real: np.ndarray, prob_mat: np.ndarray
        ) -> np.ndarray:
            # alpha: _n_categories

            # f1: same as prob_real
            alpha_tensor = alpha[None, None, :]
            f1 = logsumexp(alpha_tensor, axis=-1, b=prob_mat)
            f2 = logsumexp(alpha)
            ce = -np.sum(prob_real * f1, axis=1) + f2
            return np.mean(ce)

        try:
            res = minimize(kl, np.ones(prob_mat.shape[-1]), (prob_real, prob_mat))
        except Exception:
            return np.ones(prob_mat.shape[-1]) / prob_mat.shape[-1]

        return np.exp(res.x) / np.sum(np.exp(res.x))


src/synthcity/plugins/core/models/tabular_encoder.py
"""TabularEncoder module.
"""

# stdlib
from typing import Any, List, Optional, Sequence, Tuple, Union

# third party
import numpy as np
import pandas as pd
from pydantic import BaseModel, field_validator, validate_arguments
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import MinMaxScaler

# synthcity absolute
import synthcity.logger as log
from synthcity.utils.dataframe import discrete_columns as find_cat_cols
from synthcity.utils.serialization import dataframe_hash

# synthcity relative
from .factory import get_feature_encoder


class FeatureInfo(BaseModel):
    name: str
    feature_type: str
    transform: Any = None
    output_dimensions: int
    transformed_features: List[str]
    trans_feature_types: List[str]

    @field_validator("feature_type")
    @classmethod
    def _feature_type_validator(cls: Any, v: str) -> str:
        if v not in ["discrete", "continuous"]:
            raise ValueError(f"Invalid feature type {v}")
        return v

    @field_validator("transform")
    @classmethod
    def _transform_validator(cls: Any, v: Any) -> Any:
        if not (
            hasattr(v, "fit")
            and hasattr(v, "transform")
            and hasattr(v, "inverse_transform")
        ):
            raise ValueError(f"Invalid transform {v}")
        return v

    @field_validator("output_dimensions")
    @classmethod
    def _output_dimensions_validator(cls: Any, v: int) -> int:
        if v <= 0:
            raise ValueError(f"Invalid output_dimensions {v}")
        return v


class TabularEncoder(TransformerMixin, BaseEstimator):
    """Tabular encoder.

    Model continuous columns with a BayesianGMM and normalized to a scalar [0, 1] and a vector.
    Discrete columns are encoded using a scikit-learn OneHotEncoder.
    """

    categorical_encoder: Union[str, type] = "onehot"
    continuous_encoder: Union[str, type] = "bayesian_gmm"
    cat_encoder_params: dict = dict(handle_unknown="ignore", sparse_output=False)
    cont_encoder_params: dict = dict(n_components=10)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        *,
        whitelist: tuple = (),
        max_clusters: int = 10,
        categorical_limit: int = 10,
        categorical_encoder: Optional[Union[str, type]] = None,
        continuous_encoder: Optional[Union[str, type]] = None,
        cat_encoder_params: Optional[dict] = None,
        cont_encoder_params: Optional[dict] = None,
    ) -> None:
        """Create a data transformer.

        Args:
            whitelist (tuple):
                Columns that will not be transformed.
        """
        self.whitelist = whitelist
        self.categorical_limit = categorical_limit
        self.max_clusters = max_clusters
        if categorical_encoder is not None:
            self.categorical_encoder = categorical_encoder
        if continuous_encoder is not None:
            self.continuous_encoder = continuous_encoder
        if cat_encoder_params is not None:
            self.cat_encoder_params = cat_encoder_params
        else:
            self.cat_encoder_params = self.cat_encoder_params.copy()
        if cont_encoder_params is not None:
            self.cont_encoder_params = cont_encoder_params
        else:
            self.cont_encoder_params = self.cont_encoder_params.copy()
        if self.continuous_encoder == "bayesian_gmm":
            self.cont_encoder_params["n_components"] = max_clusters

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _fit_feature(self, feature: pd.Series, feature_type: str) -> FeatureInfo:
        """Fit the feature encoder on a column.

        Args:
            feature (pd.Series):
                A column of a dataframe.
            feature_type (str):
                Type of the feature ('discrete' or 'continuous').

        Returns:
            FeatureInfo:
                Information of the fitted feature encoder.
        """
        if feature_type == "discrete":
            encoder = get_feature_encoder(
                self.categorical_encoder, self.cat_encoder_params
            )
        else:
            encoder = get_feature_encoder(
                self.continuous_encoder, self.cont_encoder_params
            )

        encoder.fit(feature)

        return FeatureInfo(
            name=feature.name,
            feature_type=feature_type,
            transform=encoder,
            output_dimensions=encoder.n_features_out,
            transformed_features=encoder.feature_names_out,
            trans_feature_types=encoder.feature_types_out,
        )

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def fit(
        self, raw_data: pd.DataFrame, discrete_columns: Optional[List] = None
    ) -> Any:
        """Fit the ``TabularEncoder``.

        This step also counts the #columns in matrix data and span information.
        """
        if discrete_columns is None:
            discrete_columns = find_cat_cols(raw_data, self.categorical_limit)

        self.output_dimensions = 0

        self._column_raw_dtypes = raw_data.infer_objects().dtypes
        self._column_transform_info_list: Sequence[FeatureInfo] = []

        for name in raw_data.columns:
            if name in self.whitelist:
                continue
            column_hash = dataframe_hash(raw_data[[name]])
            log.info(f"Encoding {name} {column_hash}")
            ftype = "discrete" if name in discrete_columns else "continuous"
            column_transform_info = self._fit_feature(raw_data[name], ftype)

            self.output_dimensions += column_transform_info.output_dimensions
            self._column_transform_info_list.append(column_transform_info)

        return self

    def _transform_feature(
        self, column_transform_info: FeatureInfo, feature: pd.Series
    ) -> pd.DataFrame:
        encoder = column_transform_info.transform
        return pd.DataFrame(
            encoder.transform(feature).values,
            columns=column_transform_info.transformed_features,
        )

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def transform(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        """Take raw data and output a matrix data."""
        if len(self._column_transform_info_list) == 0:
            return pd.DataFrame(np.zeros((len(raw_data), 0)))

        column_data_list = []
        for name in self.whitelist:
            if name not in raw_data.columns:
                continue
            feature = raw_data[name]
            column_data_list.append(feature)

        for column_transform_info in self._column_transform_info_list:
            feature = raw_data[column_transform_info.name]
            column_data_list.append(
                self._transform_feature(column_transform_info, feature)
            )

        result = pd.concat(column_data_list, axis=1)
        result.index = raw_data.index

        return result

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _inverse_transform_feature(
        self,
        column_transform_info: FeatureInfo,
        column_data: pd.DataFrame,
    ) -> pd.Series:
        encoder = column_transform_info.transform
        return encoder.inverse_transform(column_data)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def inverse_transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Take matrix data and output raw data.

        Output uses the same type as input to the transform function.
        """
        if len(self._column_transform_info_list) == 0:
            return pd.DataFrame(np.zeros((len(data), 0)))

        st = 0
        names = []
        feature_types = []
        recovered_feature_list = []

        for name in self.whitelist:
            if name not in data.columns:
                continue
            names.append(name)
            feature_types.append(self._column_raw_dtypes)
            recovered_feature_list.append(data[name])

        for column_transform_info in self._column_transform_info_list:
            dim = column_transform_info.output_dimensions
            column_data = data.iloc[:, list(range(st, st + dim))]
            recovered_feature = self._inverse_transform_feature(
                column_transform_info, column_data
            )
            recovered_feature_list.append(recovered_feature)
            names.append(column_transform_info.name)
            st += dim

        recovered_data = np.column_stack(recovered_feature_list)
        recovered_data = pd.DataFrame(
            recovered_data, columns=names, index=data.index
        ).astype(self._column_raw_dtypes.filter(names))
        return recovered_data

    def layout(self) -> Sequence[FeatureInfo]:
        """Get the layout of the encoded dataset.

        Returns a list of tuple, describing each column as:
            - continuous, and with length 1 + number of GMM clusters.
            - discrete, and with length <N>, the length of the one-hot encoding.
        """
        return self._column_transform_info_list

    def n_features(self) -> int:
        return np.sum(
            column_transform_info.output_dimensions
            for column_transform_info in self._column_transform_info_list
        )

    def get_column_info(self, name: str) -> FeatureInfo:
        for column_transform_info in self._column_transform_info_list:
            if column_transform_info.name == name:
                return column_transform_info

        raise RuntimeError(f"Unknown column {name}")

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def activation_layout(
        self, discrete_activation: str, continuous_activation: str
    ) -> Sequence[Tuple[str, int]]:
        """Get the layout of the activations.

        Returns a list of tuple, describing each column as:
            - continuous, and with length 1 + number of GMM clusters.
            - discrete, and with length <N>, the length of the one-hot encoding.
        """
        out = []
        acts = dict(discrete=discrete_activation, continuous=continuous_activation)
        for column_transform_info in self._column_transform_info_list:
            ct = column_transform_info.trans_feature_types[0]
            d = 0
            for t in column_transform_info.trans_feature_types:
                if t != ct:
                    out.append((acts[ct], d))
                    ct = t
                    d = 0
                d += 1
            out.append((acts[ct], d))
        return out


class BinEncoder(TabularEncoder):
    """Binary encoder (for SurvivalGAN).

    Model continuous columns with a BayesianGMM and normalized to a scalar [0, 1] and a vector.
    Discrete columns are encoded using a scikit-learn OneHotEncoder.
    """

    continuous_encoder = "bayesian_gmm"
    cont_encoder_params = dict(n_components=2)
    categorical_encoder = "passthrough"  # "onehot"
    cat_encoder_params = dict()  # dict(handle_unknown="ignore", sparse=False)

    def _transform_feature(
        self, column_transform_info: FeatureInfo, feature: pd.Series
    ) -> pd.DataFrame:
        if column_transform_info.feature_type == "discrete":
            return super()._transform_feature(column_transform_info, feature)
        bgm = column_transform_info.transform
        out = bgm.transform(feature)
        return pd.DataFrame(
            out.values[:, 1:].argmax(axis=1), columns=[bgm.feature_name_in]
        )

    def _inverse_transform_feature(
        self, column_transform_info: FeatureInfo, column_data: pd.DataFrame
    ) -> pd.Series:
        if column_transform_info == "discrete":
            return super()._inverse_transform_feature(
                column_transform_info, column_data
            )
        bgm = column_transform_info.transform
        components = column_data.values.reshape(-1)
        features = bgm.means[components]
        return pd.Series(features, name=bgm.feature_name_in)


class TimeSeriesTabularEncoder(TransformerMixin, BaseEstimator):
    """TimeSeries Tabular encoder.

    Model continuous columns with a BayesianGMM and normalized to a scalar [0, 1] and a vector.
    Discrete columns are encoded using a scikit-learn OneHotEncoder.
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        max_clusters: int = 10,
        categorical_limit: int = 10,
        whitelist: list = [],
    ) -> None:
        self.max_clusters = max_clusters
        self.categorical_limit = categorical_limit
        self.whitelist = whitelist

    def fit_temporal(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        discrete_columns: Optional[List] = None,
    ) -> "TimeSeriesTabularEncoder":
        # Temporal
        self.temporal_encoder = TabularEncoder(
            max_clusters=self.max_clusters,
            categorical_limit=self.categorical_limit,
            whitelist=self.whitelist,
        )
        temporal_features = temporal_data[0].columns

        temporal_arr = np.asarray(temporal_data)
        temporal_arr = np.swapaxes(temporal_arr, -1, 0).reshape(
            len(temporal_features), -1
        )
        temporal_df = pd.DataFrame(temporal_arr.T, columns=temporal_features)

        self.temporal_encoder.fit(temporal_df)

        # Temporal horizons
        self.observation_times_encoder = MinMaxScaler().fit(
            np.asarray(observation_times).reshape(-1, 1)
        )

        return self

    def fit(
        self,
        static_data: pd.DataFrame,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        discrete_columns: Optional[List] = None,
    ) -> "TimeSeriesTabularEncoder":
        # Static
        self.static_encoder = TabularEncoder(
            max_clusters=self.max_clusters,
            categorical_limit=self.categorical_limit,
            whitelist=self.whitelist,
        )
        self.static_encoder.fit(static_data, discrete_columns=discrete_columns)

        # Temporal
        self.fit_temporal(
            temporal_data, observation_times, discrete_columns=discrete_columns
        )

        return self

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def transform_observation_times(
        self,
        observation_times: List,
    ) -> List:
        horizons_encoded = (
            self.observation_times_encoder.transform(
                np.asarray(observation_times).reshape(-1, 1)
            )
            .reshape(len(observation_times), -1)
            .tolist()
        )
        return horizons_encoded

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def transform_temporal(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
    ) -> Tuple[pd.DataFrame, List]:
        temporal_encoded = []
        for item in temporal_data:
            temporal_encoded.append(self.temporal_encoder.transform(item))

        horizons_encoded = self.transform_observation_times(observation_times)

        return temporal_encoded, horizons_encoded

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def transform_static(
        self,
        static_data: pd.DataFrame,
    ) -> pd.DataFrame:
        static_encoded = self.static_encoder.transform(static_data)

        return static_encoded

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def transform(
        self,
        static_data: pd.DataFrame,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
    ) -> Tuple[pd.DataFrame, pd.DataFrame, List]:
        static_encoded = self.transform_static(static_data)

        temporal_encoded, horizons_encoded = self.transform_temporal(
            temporal_data, observation_times
        )

        return static_encoded, temporal_encoded, horizons_encoded

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def fit_transform_temporal(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
    ) -> Tuple[pd.DataFrame, List]:
        return self.fit_temporal(temporal_data, observation_times).transform_temporal(
            temporal_data, observation_times
        )

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def fit_transform(
        self,
        static_data: pd.DataFrame,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
    ) -> Tuple[pd.DataFrame, pd.DataFrame, List]:
        return self.fit(static_data, temporal_data, observation_times).transform(
            static_data, temporal_data, observation_times
        )

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def inverse_transform_observation_times(
        self,
        observation_times: List,
    ) -> pd.DataFrame:
        horizons_decoded = (
            self.observation_times_encoder.inverse_transform(
                np.asarray(observation_times).reshape(-1, 1)
            )
            .reshape(len(observation_times), -1)
            .tolist()
        )
        return horizons_decoded

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def inverse_transform_temporal(
        self,
        temporal_encoded: List[pd.DataFrame],
        observation_times: List,
    ) -> pd.DataFrame:
        temporal_decoded = []
        for item in temporal_encoded:
            temporal_decoded.append(self.temporal_encoder.inverse_transform(item))

        horizons_decoded = self.inverse_transform_observation_times(observation_times)

        return temporal_decoded, horizons_decoded

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def inverse_transform_static(
        self,
        static_encoded: pd.DataFrame,
    ) -> pd.DataFrame:
        static_decoded = self.static_encoder.inverse_transform(static_encoded)
        return static_decoded

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def inverse_transform(
        self,
        static_encoded: pd.DataFrame,
        temporal_encoded: List[pd.DataFrame],
        observation_times: List,
    ) -> pd.DataFrame:
        static_decoded = self.inverse_transform_static(static_encoded)

        temporal_decoded, horizons_decoded = self.inverse_transform_temporal(
            temporal_encoded, observation_times
        )
        return static_decoded, temporal_decoded, horizons_decoded

    def layout(self) -> Tuple[List, List]:
        return self.static_encoder.layout(), self.temporal_encoder.layout()

    def n_features(self) -> Tuple:
        return self.static_encoder.n_features(), self.temporal_encoder.n_features()

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def activation_layout_temporal(
        self, discrete_activation: str, continuous_activation: str
    ) -> Any:
        return self.temporal_encoder.activation_layout(
            discrete_activation, continuous_activation
        )

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def activation_layout(
        self, discrete_activation: str, continuous_activation: str
    ) -> Tuple:
        return self.static_encoder.activation_layout(
            discrete_activation, continuous_activation
        ), self.temporal_encoder.activation_layout(
            discrete_activation, continuous_activation
        )


class TimeSeriesBinEncoder(TransformerMixin, BaseEstimator):
    """Time series Bin encoder.

    Model continuous columns with a BayesianGMM and normalized to a scalar [0, 1] and a vector.
    Discrete columns are encoded using a scikit-learn OneHotEncoder.
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        max_clusters: int = 10,
        categorical_limit: int = 10,
        continuous_encoder: str = "gmm",
    ) -> None:
        """Create a data transformer.

        Args:
            max_clusters (int):
                Maximum number of Gaussian distributions in Bayesian GMM.
        """
        self.encoder = BinEncoder(
            max_clusters=max_clusters,
            categorical_limit=categorical_limit,
            continuous_encoder=continuous_encoder,
        )

    def _prepare(
        self,
        static_data: pd.DataFrame,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
    ) -> pd.DataFrame:
        temporal_init = np.asarray(temporal_data)[:, 0, :].squeeze()
        temporal_init_df = pd.DataFrame(temporal_init, columns=temporal_data[0].columns)

        out = pd.concat(
            [
                static_data.reset_index(drop=True),
                temporal_init_df.reset_index(drop=True),
            ],
            axis=1,
        )
        out.columns = np.asarray(range(len(out.columns)))
        out.columns = out.columns.astype(str)

        return out

    def fit(
        self,
        static_data: pd.DataFrame,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        discrete_columns: Optional[List] = None,
    ) -> "TimeSeriesBinEncoder":
        """Fit the TimeSeriesBinEncoder"""

        data = self._prepare(static_data, temporal_data, observation_times)

        self.encoder.fit(data, discrete_columns=discrete_columns)
        return self

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def transform(
        self,
        static_data: pd.DataFrame,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
    ) -> pd.DataFrame:
        """Take raw data and output a matrix data."""
        data = self._prepare(static_data, temporal_data, observation_times)
        return self.encoder.transform(data)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def fit_transform(
        self,
        static: pd.DataFrame,
        temporal: List[pd.DataFrame],
        observation_times: List,
    ) -> pd.DataFrame:
        return self.fit(static, temporal, observation_times).transform(
            static, temporal, observation_times
        )


src/synthcity/plugins/core/models/feature_encoder.py
# stdlib
from typing import Any, List, Optional, Type, Union

# third party
import numpy as np
import pandas as pd
from pydantic import validate_arguments
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.mixture import BayesianGaussianMixture
from sklearn.preprocessing import (
    LabelEncoder,
    MinMaxScaler,
    OneHotEncoder,
    OrdinalEncoder,
    QuantileTransformer,
    RobustScaler,
    StandardScaler,
)


def validate_shape(x: np.ndarray, n_dim: int) -> np.ndarray:
    if n_dim == 1:
        if x.ndim == 2:
            x = np.squeeze(x, axis=1)
        if x.ndim != 1:
            raise ValueError("array must be 1D")
        return x
    elif n_dim == 2:
        if x.ndim == 1:
            x = x.reshape(-1, 1)
        if x.ndim != 2:
            raise ValueError("array must be 2D")
        return x
    else:
        raise ValueError("n_dim must be 1 or 2")


FeatureEncoder = Any  # tried to use ForwardRef but it didn't work under mypy


class FeatureEncoder(TransformerMixin, BaseEstimator):  # type: ignore
    """
    Base feature encoder with sklearn-style API.
    """

    n_dim_in: int = 1
    n_dim_out: int = 2
    n_features_out: int
    feature_name_in: str
    feature_names_out: List[str]
    feature_types_out: List[str]
    categorical: bool = False  # used by get_feature_types_out

    def __init__(
        self, n_dim_in: Optional[int] = None, n_dim_out: Optional[int] = None
    ) -> None:
        super().__init__()
        if n_dim_in is not None:
            self.n_dim_in = n_dim_in
        if n_dim_out is not None:
            self.n_dim_out = n_dim_out

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def fit(self, x: pd.Series, y: Any = None, **kwargs: Any) -> FeatureEncoder:
        self.feature_name_in = x.name
        self.feature_type_in = self._get_feature_type(x)
        input = validate_shape(x.values, self.n_dim_in)
        output = self._fit(input, **kwargs)._transform(input)
        self._out_shape = (-1, *output.shape[1:])  # for inverse_transform
        output = validate_shape(output, self.n_dim_out)
        if self.n_dim_out == 1:
            self.n_features_out = 1
        else:
            self.n_features_out = output.shape[1]
        self.feature_names_out = self.get_feature_names_out()
        self.feature_types_out = self.get_feature_types_out(output)
        return self

    def _fit(self, x: np.ndarray, **kwargs: Any) -> FeatureEncoder:
        return self

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def transform(self, x: pd.Series) -> Union[pd.DataFrame, pd.Series]:
        data = validate_shape(x.values, self.n_dim_in)
        out = self._transform(data)
        out = validate_shape(out, self.n_dim_out)
        if self.n_dim_out == 1:
            return pd.Series(out, name=self.feature_name_in)
        else:
            return pd.DataFrame(out, columns=self.feature_names_out)

    def _transform(self, x: np.ndarray) -> np.ndarray:
        return x

    def get_feature_names_out(self) -> List[str]:
        n = self.n_features_out
        if n == 1:
            return [self.feature_name_in]
        else:
            return [f"{self.feature_name_in}_{i}" for i in range(n)]

    def get_feature_types_out(self, output: np.ndarray) -> List[str]:
        t = self._get_feature_type(output)
        return [t] * self.n_features_out

    def _get_feature_type(self, x: Any) -> str:
        if self.categorical:
            return "discrete"
        elif np.issubdtype(x.dtype, np.floating):
            return "continuous"
        elif np.issubdtype(x.dtype, np.datetime64):
            return "datetime"
        else:
            return "discrete"

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def inverse_transform(self, df: Union[pd.DataFrame, pd.Series]) -> pd.Series:
        y = df.values.reshape(self._out_shape)
        x = self._inverse_transform(y)
        x = validate_shape(x, 1)
        return pd.Series(x, name=self.feature_name_in)

    def _inverse_transform(self, data: np.ndarray) -> np.ndarray:
        return data

    @classmethod
    def wraps(
        cls: type, encoder_class: TransformerMixin, **params: Any
    ) -> Type[FeatureEncoder]:
        """Wraps sklearn transformer to FeatureEncoder."""

        class WrappedEncoder(FeatureEncoder):
            n_dim_in = 2  # most sklearn transformers accept 2D input

            def __init__(self, *args: Any, **kwargs: Any) -> None:
                self.encoder = encoder_class(*args, **kwargs)

            def _fit(self, x: np.ndarray, **kwargs: Any) -> FeatureEncoder:
                self.encoder.fit(x, **kwargs)
                return self

            def _transform(self, x: np.ndarray) -> np.ndarray:
                return self.encoder.transform(x)

            def _inverse_transform(self, data: np.ndarray) -> np.ndarray:
                return self.encoder.inverse_transform(data)

            def get_feature_names_out(self) -> List[str]:
                return list(self.encoder.get_feature_names_out([self.feature_name_in]))

        for attr in ("__name__", "__qualname__", "__doc__"):
            setattr(WrappedEncoder, attr, getattr(encoder_class, attr))
        for attr, val in params.items():
            setattr(WrappedEncoder, attr, val)

        return WrappedEncoder


OneHotEncoder = FeatureEncoder.wraps(
    OneHotEncoder, categorical=True, handle_unknown="ignore"
)
OrdinalEncoder = FeatureEncoder.wraps(OrdinalEncoder, categorical=True)
LabelEncoder = FeatureEncoder.wraps(LabelEncoder, n_dim_out=1, categorical=True)
StandardScaler = FeatureEncoder.wraps(StandardScaler)
MinMaxScaler = FeatureEncoder.wraps(MinMaxScaler)
RobustScaler = FeatureEncoder.wraps(RobustScaler)


class DatetimeEncoder(FeatureEncoder):
    """Datetime variables encoder"""

    n_dim_out = 1

    def _transform(self, x: np.ndarray) -> np.ndarray:
        return pd.to_numeric(x).astype(float)

    def _inverse_transform(self, data: np.ndarray) -> np.ndarray:
        return pd.to_datetime(data)


class BayesianGMMEncoder(FeatureEncoder):
    """Bayesian Gaussian Mixture encoder"""

    n_dim_in = 2

    def __init__(
        self,
        n_components: int = 10,
        random_state: int = 0,
        weight_threshold: float = 0.005,
        clip_output: bool = True,
        std_multiplier: int = 4,
    ) -> None:
        self.n_components = n_components
        self.weight_threshold = weight_threshold
        self.clip_output = clip_output
        self.std_multiplier = std_multiplier
        self.model = BayesianGaussianMixture(
            n_components=n_components,
            random_state=random_state,
            weight_concentration_prior=1e-3,
        )

    def _fit(self, x: np.ndarray, **kwargs: Any) -> "BayesianGaussianMixture":
        self.min_value = x.min()
        self.max_value = x.max()

        self.model.fit(x)
        self.weights = self.model.weights_
        self.means = self.model.means_.reshape(-1)
        self.stds = np.sqrt(self.model.covariances_).reshape(-1)

        return self

    def _transform(self, x: np.ndarray) -> np.ndarray:
        means = self.means.reshape(1, -1)
        stds = self.stds.reshape(1, -1)

        # predict cluster value
        normalized_values = (x - means) / (self.std_multiplier * stds)

        # predict cluster
        component_probs = self.model.predict_proba(x)

        components = np.argmax(component_probs, axis=1)

        normalized = normalized_values[np.arange(len(x)), components]
        if self.clip_output:  # why use 0.99 instead of 1?
            normalized = np.clip(normalized, -0.99, 0.99)
        normalized = normalized.reshape(-1, 1)

        components = np.eye(self.n_components, dtype=int)[components]
        return np.hstack([normalized, components])

    def get_feature_names_out(self) -> List[str]:
        name = self.feature_name_in
        return [f"{name}.value"] + [
            f"{name}.component_{i}" for i in range(self.n_components)
        ]

    def get_feature_types_out(self, output: np.ndarray) -> List[str]:
        return ["continuous"] + ["discrete"] * self.n_components

    def _inverse_transform(self, data: np.ndarray) -> np.ndarray:
        components = np.argmax(data[:, 1:], axis=1)

        data = data[:, 0]
        if self.clip_output:
            data = np.clip(data, -1.0, 1.0)

        # recreate data
        mean_t = self.means[components]
        std_t = self.stds[components]
        reversed_data = data * self.std_multiplier * std_t + mean_t

        # clip values
        return np.clip(reversed_data, self.min_value, self.max_value)


@FeatureEncoder.wraps
class GaussianQuantileTransformer(QuantileTransformer):
    """Quantile transformer with Gaussian distribution"""

    def __init__(
        self,
        *,
        ignore_implicit_zeros: bool = False,
        subsample: int = 10000,
        random_state: Any = None,
        copy: bool = True,
    ) -> None:
        super().__init__(
            n_quantiles=None,
            output_distribution="normal",
            ignore_implicit_zeros=ignore_implicit_zeros,
            subsample=subsample,
            random_state=random_state,
            copy=copy,
        )

    def fit(self, x: np.ndarray, y: Any = None) -> "GaussianQuantileTransformer":
        self.n_quantiles = max(min(len(x) // 30, 1000), 10)
        return super().fit(x, y)


src/synthcity/plugins/core/plugin.py
# stdlib
import importlib.util
import platform
import sys
from abc import ABCMeta, abstractmethod
from importlib.abc import Loader
from pathlib import Path
from typing import Any, Callable, Dict, Generator, List, Optional, Type, Union

# third party
import pandas as pd
from pydantic import ConfigDict, validate_arguments

# synthcity absolute
import synthcity.logger as log
from synthcity.metrics.plots import plot_marginal_comparison, plot_tsne
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.dataloader import (
    DataLoader,
    GenericDataLoader,
    TimeSeriesDataLoader,
    TimeSeriesSurvivalDataLoader,
    create_from_info,
)
from synthcity.plugins.core.distribution import (
    CategoricalDistribution,
    Distribution,
    FloatDistribution,
    IntegerDistribution,
)
from synthcity.plugins.core.schema import Schema
from synthcity.plugins.core.serializable import Serializable
from synthcity.utils.constants import DEVICE
from synthcity.utils.reproducibility import enable_reproducible_results
from synthcity.utils.serialization import load_from_file, save_to_file

PLUGIN_NAME_NOT_SET: str = "plugin_name_not_set"
PLUGIN_TYPE_NOT_SET: str = "plugin_type_not_set"


class Plugin(Serializable, metaclass=ABCMeta):
    """
    .. inheritance-diagram:: synthcity.plugins.core.plugin.Plugin
        :parts: 1

    Base class for all plugins.

    Each derived class must implement the following methods:
        type() - a static method that returns the type of the plugin. e.g., debug, generative, bayesian, etc.
        name() - a static method that returns the name of the plugin. e.g., ctgan, random_noise, etc.
        hyperparameter_space() - a static method that returns the hyperparameters that can be tuned during AutoML.
        _fit() - internal method, called by `fit` on each training set.
        _generate() - internal method, called by `generate`.

    If any method implementation is missing, the class constructor will fail.

    Args:
        strict: bool. Default = True
            If True, is raises an exception if the generated data is not following the requested constraints. If False, it returns only the rows that match the constraints.
        workspace: Path
            Path for caching intermediary results
        compress_dataset: bool. Default = False
            Drop redundant features before training the generator.
        device:
            PyTorch device: cpu or cuda.
        random_state: int
            Random seed
        sampling_patience: int.
            Max inference iterations to wait for the generated data to match the training schema.
        sampling_strategy: str
            Internal parameter for schema. marginal or uniform.
    """

    model_config = ConfigDict(arbitrary_types_allowed=True, validate_assignment=True)

    def __init__(
        self,
        sampling_patience: int = 500,
        strict: bool = True,
        device: Any = DEVICE,
        random_state: int = 0,
        workspace: Path = Path("workspace"),
        compress_dataset: bool = False,
        sampling_strategy: str = "marginal",  # uniform, marginal
    ) -> None:
        if self.name() == PLUGIN_NAME_NOT_SET:
            raise ValueError(
                f"Plugin {self.__class__.__name__} `name` was not set, use Plugins().add({self.__class__.__name__}, {self.__class__})"
            )
        if self.type() == PLUGIN_TYPE_NOT_SET:
            raise ValueError(
                f"Plugin {self.__class__.__name__} `type` was not set, use Plugins().add({self.__class__.__name__}, {self.__class__})"
            )

        super().__init__()

        enable_reproducible_results(random_state)

        self._schema: Optional[Schema] = None
        self._training_schema: Optional[Schema] = None
        self._data_encoders: Optional[Dict] = None

        self.sampling_strategy = sampling_strategy
        self.sampling_patience = sampling_patience
        self.strict = strict
        self.device = device
        self.random_state = random_state
        self.compress_dataset = compress_dataset

        workspace.mkdir(parents=True, exist_ok=True)
        self.workspace = workspace

        self.fitted = False
        self.expecting_conditional = False

    @staticmethod
    @abstractmethod
    def hyperparameter_space(**kwargs: Any) -> List[Distribution]:
        """Returns the hyperparameter space for the derived plugin."""
        ...

    @classmethod
    def sample_hyperparameters(cls, *args: Any, **kwargs: Any) -> Dict[str, Any]:
        """Sample value from the hyperparameter space for the current plugin."""
        param_space = cls.hyperparameter_space(*args, **kwargs)

        results = {}

        for hp in param_space:
            results[hp.name] = hp.sample()[0]

        return results

    @classmethod
    def sample_hyperparameters_optuna(
        cls, trial: Any, *args: Any, **kwargs: Any
    ) -> Dict[str, Any]:
        param_space = cls.hyperparameter_space(*args, **kwargs)

        results = {}

        for hp in param_space:
            if isinstance(hp, IntegerDistribution):
                results[hp.name] = trial.suggest_int(hp.name, hp.low, hp.high, hp.step)
            elif isinstance(hp, FloatDistribution):
                results[hp.name] = trial.suggest_float(hp.name, hp.low, hp.high)
            elif isinstance(hp, CategoricalDistribution):
                results[hp.name] = trial.suggest_categorical(hp.name, hp.choices)
            else:
                raise RuntimeError(f"unknown distribution type {hp}")

        return results

    @staticmethod
    @abstractmethod
    def name() -> str:
        """The name of the plugin."""
        return PLUGIN_NAME_NOT_SET

    @staticmethod
    @abstractmethod
    def type() -> str:
        """The type of the plugin."""
        return PLUGIN_TYPE_NOT_SET

    @classmethod
    def fqdn(cls) -> str:
        """The Fully-Qualified name of the plugin."""
        return cls.type() + "." + cls.name()

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def fit(self, X: Union[DataLoader, pd.DataFrame], *args: Any, **kwargs: Any) -> Any:
        """Training method the synthetic data plugin.

        Args:
            X: DataLoader.
                The reference dataset.
            cond: Optional, Union[pd.DataFrame, pd.Series, np.ndarray]
                Optional Training Conditional.
                The training conditional can be used to control to output of some models, like GANs or VAEs. The content can be anything, as long as it maps to the training dataset X.
                Usage example:
                    >>> from sklearn.datasets import load_iris
                    >>> from synthcity.plugins.core.dataloader import GenericDataLoader
                    >>> from synthcity.plugins.core.constraints import Constraints
                    >>>
                    >>> # Load in `test_plugin` the generative model of choice
                    >>> # ....
                    >>>
                    >>> X, y = load_iris(as_frame=True, return_X_y=True)
                    >>> X["target"] = y
                    >>>
                    >>> X = GenericDataLoader(X)
                    >>> test_plugin.fit(X, cond=y)
                    >>>
                    >>> count = 10
                    >>> X_gen = test_plugin.generate(count, cond=np.ones(count))
                    >>>
                    >>> # The Conditional only optimizes the output generation
                    >>> # for GANs and VAEs, but does NOT guarantee the samples
                    >>> # are only from that condition.
                    >>> # If you want to guarantee that output contains only
                    >>> # "target" == 1 samples, use Constraints.
                    >>>
                    >>> constraints = Constraints(
                    >>>     rules=[
                    >>>         ("target", "==", 1),
                    >>>     ]
                    >>> )
                    >>> X_gen = test_plugin.generate(count,
                    >>>         cond=np.ones(count),
                    >>>         constraints=constraints
                    >>>        )
                    >>> assert (X_gen["target"] == 1).all()

        Returns:
            self
        """
        if isinstance(X, (pd.DataFrame)):
            X = GenericDataLoader(X)

        if "cond" in kwargs and kwargs["cond"] is not None:
            self.expecting_conditional = True

        enable_reproducible_results(self.random_state)

        self.data_info = X.info()

        self._schema = Schema(
            data=X,
            sampling_strategy=self.sampling_strategy,
            random_state=self.random_state,
        )

        if X.is_tabular():
            X, self._data_encoders = X.encode()
            if self.compress_dataset:
                X_hash = X.hash()
                bkp_file = (
                    self.workspace
                    / f"compressed_df_{X_hash}_{platform.python_version()}.bkp"
                )
                if not bkp_file.exists():
                    X_compressed_context = X.compress()
                    save_to_file(bkp_file, X_compressed_context)

                X, self.compress_context = load_from_file(bkp_file)

        self._training_schema = Schema(
            data=X,
            sampling_strategy=self.sampling_strategy,
            random_state=self.random_state,
        )

        output = self._fit(X, *args, **kwargs)
        self.fitted = True

        return output

    @abstractmethod
    def _fit(self, X: DataLoader, *args: Any, **kwargs: Any) -> "Plugin":
        """Internal training method the synthetic data plugin.

        Args:
            X: DataLoader.
                The reference dataset.
            cond: Optional, Union[pd.DataFrame, pd.Series, np.ndarray]
                Training Conditional
        Returns:
            self
        """
        ...

    @validate_arguments
    def generate(
        self,
        count: Optional[int] = None,
        constraints: Optional[Constraints] = None,
        random_state: Optional[int] = None,
        **kwargs: Any,
    ) -> DataLoader:
        """Synthetic data generation method.

        Args:
            count: optional int.
                The number of samples to generate. If None, it generated len(reference_dataset) samples.
            cond: Optional, Union[pd.DataFrame, pd.Series, np.ndarray].
                Optional Generation Conditional. The conditional can be used only if the model was trained using a conditional too.
                If provided, it must have `count` length.
                Not all models support conditionals. The conditionals can be used in VAEs or GANs to speed-up the generation under some constraints. For model agnostic solutions, check out the `constraints` parameter.
            constraints: optional Constraints.
                Optional constraints to apply on the generated data. If none, the reference schema constraints are applied. The constraints are model agnostic, and will filter the output of the generative model.
                The constraints are a list of rules. Each rule is a tuple of the form (<feature>, <operation>, <value>).

                Valid Operations:
                    - "<", "lt" : less than <value>
                    - "<=", "le": less or equal with <value>
                    - ">", "gt" : greater than <value>
                    - ">=", "ge": greater or equal with <value>
                    - "==", "eq": equal with <value>
                    - "in": valid for categorical features, and <value> must be array. for example, ("target", "in", [0, 1])
                    - "dtype": <value> can be a data type. For example, ("target", "dtype", "int")

                Usage example:
                    >>> from synthcity.plugins.core.constraints import Constraints
                    >>> constraints = Constraints(
                    >>>   rules=[
                    >>>             ("InterestingFeature", "==", 0),
                    >>>         ]
                    >>>     )
                    >>>
                    >>> syn_data = syn_model.generate(
                            count=count,
                            constraints=constraints
                        ).dataframe()
                    >>>
                    >>> assert (syn_data["InterestingFeature"] == 0).all()

            random_state: optional int.
                Optional random seed to use.

        Returns:
            <count> synthetic samples
        """
        if not self.fitted:
            raise RuntimeError("Fit the generator first")

        if self._schema is None:
            raise RuntimeError("Fit the model first")

        if random_state is not None:
            enable_reproducible_results(random_state)

        has_gen_cond = "cond" in kwargs and kwargs["cond"] is not None
        if has_gen_cond and not self.expecting_conditional:
            raise RuntimeError(
                "Conditional mismatch. Got inference conditional, without any training conditional"
            )

        if count is None:
            count = self.data_info["len"]

        # We use the training schema for the generation
        gen_constraints = self.training_schema().as_constraints()
        if constraints is not None:
            gen_constraints = gen_constraints.extend(constraints)

        syn_schema = Schema.from_constraints(gen_constraints)

        X_syn = self._generate(count=count, syn_schema=syn_schema, **kwargs)

        if X_syn.is_tabular():
            if self.compress_dataset:
                X_syn = X_syn.decompress(self.compress_context)
            if self._data_encoders is not None:
                X_syn = X_syn.decode(self._data_encoders)

        # The dataset is decompressed here, we can use the public schema
        gen_constraints = self.schema().as_constraints()
        if constraints is not None:
            gen_constraints = gen_constraints.extend(constraints)

        if not X_syn.satisfies(gen_constraints) and self.strict:
            raise RuntimeError(
                f"Plugin {self.name()} failed to meet the synthetic constraints."
            )

        if self.strict:
            X_syn = X_syn.match(gen_constraints)

        return X_syn

    @abstractmethod
    def _generate(
        self,
        count: int,
        syn_schema: Schema,
        **kwargs: Any,
    ) -> DataLoader:
        """Internal synthetic data generation method.

        Args:
            count: optional int.
                The number of samples to generate. If None, it generated len(reference_dataset) samples.
            syn_schema:
                The schema/constraints that need to be satisfied by the synthetic data.
            cond: Optional, Union[pd.DataFrame, pd.Series, np.ndarray]
                Generation Conditional

        Returns:
            <count> synthetic samples
        """
        ...

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _safe_generate(
        self, gen_cbk: Callable, count: int, syn_schema: Schema, **kwargs: Any
    ) -> DataLoader:
        constraints = syn_schema.as_constraints()

        data_synth = pd.DataFrame([], columns=self.training_schema().features())
        for it in range(self.sampling_patience):
            # sample
            iter_samples = gen_cbk(count, **kwargs)
            iter_samples_df = pd.DataFrame(
                iter_samples, columns=self.training_schema().features()
            )

            # Handle protected columns
            for col in syn_schema.protected_cols:
                if col not in iter_samples_df.columns:
                    # Sample the protected column using its distribution
                    iter_samples_df[col] = syn_schema.domain[col].sample(count)

            # validate schema
            iter_samples_df = self.training_schema().adapt_dtypes(iter_samples_df)

            if self.strict:
                iter_samples_df = constraints.match(iter_samples_df)
                iter_samples_df = iter_samples_df.drop_duplicates()

            data_synth = pd.concat([data_synth, iter_samples_df], ignore_index=True)

            if len(data_synth) >= count:
                break

        data_synth = self.training_schema().adapt_dtypes(data_synth).head(count)

        return create_from_info(data_synth, self.data_info)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _safe_generate_time_series(
        self, gen_cbk: Callable, count: int, syn_schema: Schema, **kwargs: Any
    ) -> DataLoader:
        if self.data_info["data_type"] not in ["time_series", "time_series_survival"]:
            raise ValueError(
                f"Invalid data type for time series = {self.data_info['data_type']}"
            )
        constraints = syn_schema.as_constraints()

        data_synth = pd.DataFrame([], columns=self.training_schema().features())
        data_info = self.data_info
        offset = 0
        seq_offset = 0
        for it in range(self.sampling_patience):
            # sample
            if self.data_info["data_type"] == "time_series":
                static, temporal, observation_times, outcome = gen_cbk(
                    count - offset, **kwargs
                )
                loader = TimeSeriesDataLoader(
                    temporal_data=temporal,
                    observation_times=observation_times,
                    static_data=static,
                    outcome=outcome,
                    seq_offset=seq_offset,
                )
            elif self.data_info["data_type"] == "time_series_survival":
                static, temporal, observation_times, T, E = gen_cbk(
                    count - offset, **kwargs
                )
                loader = TimeSeriesSurvivalDataLoader(
                    temporal_data=temporal,
                    observation_times=observation_times,
                    static_data=static,
                    T=T,
                    E=E,
                    seq_offset=seq_offset,
                )

            # validate schema
            iter_samples_df = loader.dataframe()
            id_col = loader.info()["seq_id_feature"]

            iter_samples_df = self.training_schema().adapt_dtypes(iter_samples_df)

            if self.strict:
                iter_samples_df = constraints.match(iter_samples_df)

            if len(iter_samples_df) == 0:
                continue

            data_synth = pd.concat([data_synth, iter_samples_df], ignore_index=True)
            offset = len(data_synth[id_col].unique())
            seq_offset = max(data_synth[id_col].unique()) + 1

            if offset >= count:
                break

        data_synth = self.training_schema().adapt_dtypes(data_synth)
        return create_from_info(data_synth, data_info)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _safe_generate_images(
        self, gen_cbk: Callable, count: int, syn_schema: Schema, **kwargs: Any
    ) -> DataLoader:
        data_synth = gen_cbk(count, **kwargs)

        return create_from_info(data_synth, self.data_info)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def schema_includes(self, other: Union[DataLoader, pd.DataFrame]) -> bool:
        """Helper method to test if the reference schema includes a Dataset

        Args:
            other: DataLoader.
                The dataset to test

        Returns:
            bool, if the schema includes the dataset or not.

        """
        other_schema = Schema(data=other)
        return self.schema().includes(other_schema)

    def schema(self) -> Schema:
        """The reference schema"""
        if self._schema is None:
            raise RuntimeError("Fit the model first")

        return self._schema

    def training_schema(self) -> Schema:
        """The internal schema"""
        if self._training_schema is None:
            raise RuntimeError("Fit the model first")

        return self._training_schema

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def plot(
        self,
        plt: Any,
        X: DataLoader,
        count: Optional[int] = None,
        plots: list = ["marginal", "associations", "tsne"],
        **kwargs: Any,
    ) -> Any:
        """Plot the real-synthetic distributions.

        Args:
            plt: output
            X: DataLoader.
                The reference dataset.

        Returns:
            self
        """
        X_syn = self.generate(count=count, **kwargs)

        if "marginal" in plots:
            plot_marginal_comparison(plt, X, X_syn)
        if "tsne" in plots:
            plot_tsne(plt, X, X_syn)


PLUGIN_CATEGORY_REGISTRY: Dict[str, List[str]] = dict()
PLUGIN_REGISTRY: Dict[str, Type[Plugin]] = dict()


class PluginLoader:
    """Plugin loading utility class.
    Used to load the plugins from the current folder.
    """

    @validate_arguments
    def __init__(self, plugins: list, expected_type: Type, categories: list) -> None:
        global PLUGIN_CATEGORY_REGISTRY
        PLUGIN_CATEGORY_REGISTRY = {cat: [] for cat in categories}
        self._refresh()
        self._available_plugins = {}
        for plugin in plugins:
            stem = Path(plugin).stem.split("plugin_")[-1]
            cls = self._load_single_plugin_impl(plugin)
            if cls is None:
                continue
            self._available_plugins[stem] = plugin
        self._expected_type = expected_type

    def _refresh(self) -> None:
        """Refresh the list of available plugins"""
        self._plugins: Dict[str, Type[Plugin]] = PLUGIN_REGISTRY
        self._categories: Dict[str, List[str]] = PLUGIN_CATEGORY_REGISTRY

    @validate_arguments
    def _load_single_plugin_impl(self, plugin_name: str) -> Optional[Type]:
        """Helper for loading a single plugin implementation"""
        plugin = Path(plugin_name)
        name = plugin.stem
        ptype = plugin.parent.name

        module_name = f"synthcity.plugins.{ptype}.{name}"

        failed = False
        for retry in range(2):
            try:
                if module_name in sys.modules:
                    mod = sys.modules[module_name]
                else:
                    spec = importlib.util.spec_from_file_location(module_name, plugin)
                    if spec is None:
                        raise RuntimeError("invalid spec")
                    if not isinstance(spec.loader, Loader):
                        raise RuntimeError("invalid plugin type")

                    mod = importlib.util.module_from_spec(spec)
                    if module_name not in sys.modules:
                        sys.modules[module_name] = mod

                    spec.loader.exec_module(mod)
                cls = mod.plugin
                if cls is None:
                    log.critical(f"module disabled: {plugin_name}")
                    return None

                failed = False
                break
            except BaseException as e:
                log.critical(f"load failed: {e}")
                failed = True

        if failed:
            log.critical(f"module {name} load failed")
            return None

        return cls

    @validate_arguments
    def _load_single_plugin(self, plugin_name: str) -> bool:
        """Helper for loading a single plugin"""
        cls = self._load_single_plugin_impl(plugin_name)
        if cls is None:
            return False

        self.add(cls.name(), cls)
        return True

    def list(self) -> List[str]:
        """Get all the available plugins."""
        self._refresh()
        all_plugins = list(self._plugins.keys()) + list(self._available_plugins.keys())
        plugins = []
        for plugin in all_plugins:
            if self.get_type(plugin).type() in self._categories:
                plugins.append(plugin)
        return list(set(plugins))

    def types(self) -> List[Type]:
        """Get the loaded plugins types"""
        self._refresh()
        return list(self._plugins.values())

    def _add_category(self, category: str, name: str) -> "PluginLoader":
        """Add a new plugin category"""
        log.debug(f"Registering plugin category {category}")
        if (
            category in PLUGIN_CATEGORY_REGISTRY
            and name in PLUGIN_CATEGORY_REGISTRY[category]
        ):
            raise TypeError(
                f"Plugin {name} is already registered as category: {category}"
            )
        if PLUGIN_CATEGORY_REGISTRY.get(category, None) is not None:
            PLUGIN_CATEGORY_REGISTRY[category].append(name)
        else:
            PLUGIN_CATEGORY_REGISTRY[category] = [name]
        return self

    def add(self, name: str, cls: Type) -> "PluginLoader":
        """Add a new plugin"""
        global PLUGIN_REGISTRY
        global PLUGIN_CATEGORY_REGISTRY
        self._refresh()
        if name in self._plugins:
            log.info(f"Plugin {name} already exists. Overwriting")

        if not issubclass(cls, self._expected_type):
            raise ValueError(
                f"Plugin {name} must derive the {self._expected_type} interface."
            )

        if (
            cls.type() not in PLUGIN_CATEGORY_REGISTRY.keys()
            or name not in PLUGIN_CATEGORY_REGISTRY.get(cls.type(), [])
        ):
            self._add_category(str(cls.type()), name)
        PLUGIN_REGISTRY[name] = cls
        return self

    @validate_arguments
    def load(self, buff: bytes) -> Any:
        """Load serialized plugin"""
        return Plugin.load(buff)

    @validate_arguments
    def get(self, name: str, *args: Any, **kwargs: Any) -> Any:
        """Create a new object from a plugin.
        Args:
            name: str. The name of the plugin
            &args, **kwargs. Plugin specific arguments

        Returns:
            The new object
        """
        self._refresh()
        if name not in self._plugins and name not in self._available_plugins:
            raise ValueError(f"Plugin {name} doesn't exist.")

        if name not in self._plugins:
            self._load_single_plugin(self._available_plugins[name])

        if name not in self._plugins:
            raise ValueError(f"Plugin {name} cannot be loaded.")

        return self._plugins[name](*args, **kwargs)

    @validate_arguments
    def get_type(self, name: str) -> Type:
        """Get the class type of a plugin.
        Args:
            name: str. The name of the plugin

        Returns:
            The class of the plugin
        """
        self._refresh()
        if name not in self._plugins and name not in self._available_plugins:
            raise ValueError(f"Plugin {name} doesn't exist.")

        if name not in self._plugins:
            self._load_single_plugin(self._available_plugins[name])

        if name not in self._plugins:
            raise ValueError(f"Plugin {name} doesn't exist.")

        return self._plugins[name]

    def __iter__(self) -> Generator:
        """Iterate the loaded plugins."""
        self._refresh()
        for x in self._plugins:
            yield x

    def __len__(self) -> int:
        """The number of available plugins."""
        return len(self.list())

    @validate_arguments
    def __getitem__(self, key: str) -> Any:
        return self.get(key)

    def reload(self) -> "PluginLoader":
        global PLUGIN_CATEGORY_REGISTRY
        global PLUGIN_REGISTRY
        PLUGIN_CATEGORY_REGISTRY = dict()
        PLUGIN_REGISTRY = dict()
        return self


src/synthcity/plugins/core/schema.py
# stdlib
from typing import Any, Dict, Generator, List, Optional, Union

# third party
import pandas as pd
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    field_validator,
    model_validator,
    validate_arguments,
)

# synthcity absolute
import synthcity.logger as log
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.dataloader import DataLoader, GenericDataLoader
from synthcity.plugins.core.distribution import (
    CategoricalDistribution,
    DatetimeDistribution,
    Distribution,
    FloatDistribution,
    IntegerDistribution,
    PassThroughDistribution,
)


class Schema(BaseModel):
    """
    Utility class for defining the schema of a Dataset.

    Constructor Args:
        domain: Dict
            A dictionary of feature_name: Distribution.
        sampling_strategy: str
            Taking value of "marginal" (default) or "uniform" (for debugging).
        protected_cols: List[str]
            List of columns that are exempt from distributional constraints (e.g. ID column)
        random_state: int
            Random seed (default 0)
        data: Any
            (Optional) the data set
    """

    sampling_strategy: str = Field(default="marginal")
    protected_cols: List[str] = []
    random_state: int = Field(default=0)
    domain: Dict = Field(default_factory=dict)

    data: Optional[Union[DataLoader, pd.DataFrame]] = Field(default=None, exclude=True)

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @field_validator("data", mode="before")
    def validate_data(cls, v: Any) -> Optional[DataLoader]:
        if v is not None:
            if isinstance(v, pd.DataFrame):
                return GenericDataLoader(v)
            elif isinstance(v, DataLoader):
                return v
            else:
                raise ValueError(
                    f"Invalid data type for 'data': {type(v)}. Expected DataLoader or pandas DataFrame."
                )
        return v

    @model_validator(mode="after")
    def initialize_domain(cls, model: "Schema") -> "Schema":
        if model.data is not None:
            X = model.data.dataframe()
            model.domain = model._infer_domain(
                X,
                sampling_strategy=model.sampling_strategy,
                random_state=model.random_state,
            )
            # Remove 'data' attribute from the model
            del model.__dict__["data"]
            if "data" in model.__fields_set__:
                model.__fields_set__.remove("data")
        return model

    @validate_arguments
    def get(self, feature: str) -> Distribution:
        """Get the Distribution of a feature.

        Args:
            feature: str. the feature name

        Returns:
            The feature distribution
        """
        if feature not in self.domain:
            raise ValueError(f"invalid feature {feature}")

        return self.domain[feature]

    @validate_arguments
    def __getitem__(self, key: str) -> Distribution:
        """Get the Distribution of a feature.

        Args:
            feature: str. the feature name

        Returns:
            The feature distribution
        """
        return self.get(key)

    def __iter__(self) -> Generator:
        """Iterate the features distribution"""
        for x in self.domain:
            yield x

    def __len__(self) -> int:
        """Get the number of features"""
        return len(self.domain)

    def includes(self, other: "Schema") -> bool:
        """Test if another schema is included in the local one."""
        for feature in other:
            if feature in self.protected_cols:
                continue
            if feature not in self.domain:
                return False

            if not self[feature].includes(other[feature]):
                return False

        return True

    def features(self) -> List:
        return list(self.domain.keys())

    def sample(self, count: int) -> pd.DataFrame:
        data = {}
        for col, dist in self.domain.items():
            samples = dist.sample(count)
            data[col] = samples
        return pd.DataFrame(data)

    def adapt_dtypes(self, X: pd.DataFrame) -> pd.DataFrame:
        """Applying the data type to a new data frame

        Args:
            X: pd.DataFrame
                A new data frame to be adapted.

        Returns:
            A data frame whose data types are coerced to be the same with the Schema.
            If the data frame contains new features, these will be retained as is.
        """
        for feature in self.domain:
            if feature not in X.columns:
                continue
            X[feature] = X[feature].astype(
                self.domain[feature].dtype(), errors="ignore"
            )

        return X

    def as_constraints(self) -> Constraints:
        rules = []
        for feature, dist in self.domain.items():
            rules.extend(dist.as_constraint().rules)
        return Constraints(rules=rules)

    @classmethod
    def from_constraints(cls, constraints: Constraints) -> "Schema":
        domain: Dict = {}
        feature_params: Dict = {}

        # Collect constraint information
        for feature, op, value in constraints.rules:
            if feature not in feature_params:
                feature_params[feature] = {
                    "name": feature,
                    "random_state": None,
                    "low": None,
                    "high": None,
                    "dtype": "float",  # Default to 'float' if not specified
                    "choices": [],
                }

            params = feature_params[feature]

            if op in ["ge", ">="]:
                if params["low"] is None or value > params["low"]:
                    params["low"] = value
            elif op in ["le", "<="]:
                if params["high"] is None or value < params["high"]:
                    params["high"] = value
            elif op in ["eq", "=="]:
                # For '==', set both 'low' and 'high' to value
                params["low"] = value
                params["high"] = value
            elif op in ["in", "isin"]:
                if isinstance(value, list):
                    params["choices"].extend(value)
                else:
                    params["choices"].append(value)
            elif op == "dtype":
                params["dtype"] = value
            else:
                # Handle other operators if necessary
                pass

        # Create distribution objects
        for feature, params in feature_params.items():
            dtype = params["dtype"]
            if dtype == "float":
                if params["low"] is None or params["high"] is None:
                    raise ValueError(
                        f"Cannot create FloatDistribution for '{feature}' without 'low' and 'high' values."
                    )
                domain[feature] = FloatDistribution(
                    name=params["name"],
                    random_state=params["random_state"],
                    low=params["low"],
                    high=params["high"],
                )
            elif dtype == "int":
                if params["low"] is None or params["high"] is None:
                    raise ValueError(
                        f"Cannot create IntegerDistribution for '{feature}' without 'low' and 'high' values."
                    )
                domain[feature] = IntegerDistribution(
                    name=params["name"],
                    random_state=params["random_state"],
                    low=int(params["low"]),
                    high=int(params["high"]),
                    step=1,  # Default step to 1 or adjust as needed
                )
            elif dtype in ["category", "object"]:
                choices = params.get("choices")
                if choices is None or not choices:
                    raise ValueError(
                        f"Cannot create CategoricalDistribution for '{feature}' without 'choices'."
                    )
                domain[feature] = CategoricalDistribution(
                    name=params["name"],
                    random_state=params["random_state"],
                    choices=list(set(choices)),
                )
            else:
                raise ValueError(
                    f"Unsupported dtype '{dtype}' for feature '{feature}'."
                )

        return cls(domain=domain)

    def _infer_domain(
        self,
        X: pd.DataFrame,
        sampling_strategy: str,
        random_state: int,
    ) -> Dict[str, Distribution]:
        feature_domain: Dict[str, Distribution] = {}

        for idx, col in enumerate(X.columns):
            col_random_state = random_state + idx + 1  # Ensure unique seeds

            try:
                if sampling_strategy == "marginal":
                    if col in self.protected_cols:
                        feature_domain[col] = PassThroughDistribution(
                            name=col,
                            data=X[col],
                            random_state=col_random_state,
                        )
                        continue

                    is_categorical = pd.api.types.is_categorical_dtype(X[col])
                    is_object = X[col].dtype == object
                    is_bool = pd.api.types.is_bool_dtype(X[col])
                    is_integer = pd.api.types.is_integer_dtype(X[col])
                    is_float = pd.api.types.is_float_dtype(X[col])
                    is_datetime = pd.api.types.is_datetime64_any_dtype(X[col])

                    if is_categorical or is_object or is_bool:
                        feature_domain[col] = CategoricalDistribution(
                            name=col,
                            data=X[col],
                            random_state=col_random_state,
                        )
                    elif is_integer:
                        feature_domain[col] = IntegerDistribution(
                            name=col,
                            data=X[col],
                            random_state=col_random_state,
                        )
                    elif is_float:
                        feature_domain[col] = FloatDistribution(
                            name=col,
                            data=X[col],
                            random_state=col_random_state,
                        )
                    elif is_datetime:
                        feature_domain[col] = DatetimeDistribution(
                            name=col,
                            data=X[col],
                            random_state=col_random_state,
                        )
                    else:
                        raise ValueError(
                            f"Unsupported data type for column '{col}' with dtype {X[col].dtype}"
                        )
                elif sampling_strategy == "uniform":

                    is_categorical = pd.api.types.is_categorical_dtype(X[col])
                    is_object = X[col].dtype == object
                    is_bool = pd.api.types.is_bool_dtype(X[col])
                    is_integer = pd.api.types.is_integer_dtype(X[col])
                    is_float = pd.api.types.is_float_dtype(X[col])
                    is_datetime = pd.api.types.is_datetime64_any_dtype(X[col])

                    if (
                        pd.api.types.is_categorical_dtype(X[col])
                        or X[col].dtype == object
                        or pd.api.types.is_bool_dtype(X[col])
                    ):
                        feature_domain[col] = CategoricalDistribution(
                            name=col,
                            choices=list(X[col].unique()),
                            random_state=col_random_state,
                            sampling_strategy=sampling_strategy,
                        )
                    elif pd.api.types.is_integer_dtype(X[col]):
                        feature_domain[col] = IntegerDistribution(
                            name=col,
                            low=X[col].min(),
                            high=X[col].max(),
                            random_state=col_random_state,
                            sampling_strategy=sampling_strategy,
                        )
                    elif pd.api.types.is_float_dtype(X[col]):
                        feature_domain[col] = FloatDistribution(
                            name=col,
                            low=X[col].min(),
                            high=X[col].max(),
                            random_state=col_random_state,
                            sampling_strategy=sampling_strategy,
                        )
                    elif pd.api.types.is_datetime64_any_dtype(X[col]):
                        feature_domain[col] = DatetimeDistribution(
                            name=col,
                            low=X[col].min(),
                            high=X[col].max(),
                            random_state=col_random_state,
                            sampling_strategy=sampling_strategy,
                        )
                else:
                    raise ValueError(
                        f"Unsupported sampling strategy '{sampling_strategy}'"
                    )
            except Exception as e:
                log.error(f"Exception occurred while processing column '{col}': {e}")
                raise
        return feature_domain


src/synthcity/plugins/core/serializable.py
# stdlib
import copy
import importlib.util
import os
from importlib.abc import Loader
from pathlib import Path
from typing import Any, Optional

# third party
from pydantic import validate_arguments

# synthcity absolute
from synthcity.utils.serialization import load as deserialize
from synthcity.utils.serialization import save as serialize
from synthcity.version import MAJOR_VERSION

module_path = Path(__file__).resolve()
module_parent_path = module_path.parent


class Serializable:
    """Utility class for model persistence."""

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        derived_module_path: Optional[Path] = None
        self.fitted = (
            False  # make sure all serializable objects are not fitted by default
        )

        search_module = self.__class__.__module__
        if not search_module.endswith(".py"):
            search_module = search_module.split(".")[-1]
            search_module += ".py"

        for path in module_path.parent.parent.rglob(search_module):
            derived_module_path = path
            break

        self.module_relative_path: Optional[Path] = None

        if derived_module_path is not None:
            relative_path = Path(
                os.path.relpath(derived_module_path, start=module_path.parent)
            )

            if not (module_parent_path / relative_path).resolve().exists():
                raise RuntimeError(
                    f"cannot find relative module path for {relative_path.resolve()}"
                )

            self.module_relative_path = relative_path

        self.module_name = self.__class__.__module__
        self.class_name = self.__class__.__qualname__
        self.raw_class = self.__class__

    def save_dict(self) -> dict:
        members: dict = {}

        for key in self.__dict__:
            data = self.__dict__[key]
            if isinstance(data, Serializable):
                members[key] = self.__dict__[key].save_dict()
            elif key == "model":
                members[key] = serialize(self.__dict__[key])
            else:
                members[key] = copy.deepcopy(self.__dict__[key])

        if "fitted" not in members:
            members["fitted"] = self.fitted  # Ensure 'fitted' is always serialized

        return {
            "source": "synthcity",
            "data": members,
            "version": self.version(),
            "class_name": self.class_name,
            "class": self.raw_class,
            "module_name": self.module_name,
            "module_relative_path": self.module_relative_path,
        }

    def save(self) -> bytes:
        return serialize(self.save_dict())

    @validate_arguments
    def save_to_file(self, path: Path) -> bytes:
        raise NotImplementedError()

    @staticmethod
    # @validate_arguments
    def load_dict(representation: dict) -> Any:
        if "source" not in representation or representation["source"] != "synthcity":
            raise ValueError("Invalid synthcity object")

        if representation["version"] != Serializable.version():
            raise RuntimeError(
                f"Invalid synthcity API version. Current version is {Serializable.version()}, but the object was serialized using version {representation['version']}"
            )

        if representation["module_relative_path"] is not None:
            module_path = module_parent_path / representation["module_relative_path"]

            if not module_path.exists():
                raise RuntimeError(f"Unknown module path {module_path}")

            spec = importlib.util.spec_from_file_location(
                representation["module_name"], module_path
            )
            if spec is None:
                raise RuntimeError("Invalid spec")

            if not isinstance(spec.loader, Loader):
                raise RuntimeError("invalid synthcity object type")

            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)

        cls = representation["class"]

        obj = cls()

        obj_dict = {}
        for key in representation["data"]:
            val = representation["data"][key]

            if (
                isinstance(val, dict)
                and "source" in val
                and val["source"] == "synthcity"
            ):
                obj_dict[key] = Serializable.load_dict(val)
            else:
                obj_dict[key] = val

        obj.__dict__ = obj_dict

        return obj

    @staticmethod
    @validate_arguments
    def load(buff: bytes) -> Any:
        representation = deserialize(buff)

        return Serializable.load_dict(representation)

    @staticmethod
    def version() -> str:
        "API version"
        return MAJOR_VERSION


src/synthcity/plugins/core/constraints.py
# stdlib
from typing import Any, Generator, List, Tuple

# third party
import numpy as np
import pandas as pd
from pydantic import BaseModel, field_validator, validate_arguments

# synthcity absolute
import synthcity.logger as log

Rule = Tuple[str, str, Any]  # Define a type alias for clarity


class Constraints(BaseModel):
    """
    .. inheritance-diagram:: synthcity.plugins.core.constraints.Constraints
        :parts: 1


    Constraints on data.

    The Constraints class allows users to specify constraints on the features. Examples include the feature value range, allowed item set, and data type.
    These constraints can be used to filter out invalid values in synthetic datasets.

    Constructor Args:
        rules: List[Tuple]
            Each tuple in the list specifies a constraint on a feature. The tuple has the form of (feature, op, thresh),
            where feature is the feature name to apply constraint on, op takes values in [
                    "<",
                    ">=",
                    "<=",
                    ">",
                    "==",
                    "lt",
                    "le",
                    "gt",
                    "ge",
                    "eq",
                    "in",
                    "dtype",
                ],
            and thresh is the threshold or data type.
    """

    rules: list[Rule] = []

    @field_validator("rules", mode="before")
    def _validate_rules(cls: Any, rules: List) -> List:
        supported_ops: list = [
            "<",
            ">=",
            "<=",
            ">",
            "==",
            "lt",
            "le",
            "gt",
            "ge",
            "eq",
            "in",
            "dtype",
        ]

        for rule in rules:
            if len(rule) < 3:
                raise ValueError(f"Invalid constraint. Expecting tuple, but got {rule}")

            feature, op, thresh = rule

            if op not in supported_ops:
                raise ValueError(
                    f"Invalid operation {op}. Supported ops: {supported_ops}"
                )
            if op in ["in"]:
                if not isinstance(thresh, list):
                    raise ValueError("Invalid type for threshold = {type(thresh)}")
            elif op in ["dtype"]:
                if not isinstance(thresh, str):
                    raise ValueError("Invalid type for threshold = {type(thresh)}")

        return rules

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _eval(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.Index:
        """Evaluation primitive.

        Args:
            X: DataFrame. The dataset to apply the constraint on.
            feature: str. The column in the dataset to apply the constraint on.
            op: str. The operation to execute for the constraint.
            operand: Any. The operand for the binary operation.

        Returns:
            The pandas.Index which matches the constraint.
        """
        if op == "lt" or op == "<":
            return (X[feature] < operand) | X[feature].isna()
        elif op == "le" or op == "<=":
            return (X[feature] <= operand) | X[feature].isna()
        elif op == "gt" or op == ">":
            return (X[feature] > operand) | X[feature].isna()
        elif op == "ge" or op == ">=":
            return (X[feature] >= operand) | X[feature].isna()
        elif op == "eq" or op == "==":
            return (X[feature] == operand) | X[feature].isna()
        elif op == "in":
            return (X[feature].isin(operand)) | X[feature].isna()
        elif op == "dtype":
            return operand in str(X[feature])
        else:
            raise RuntimeError("unsupported operation", op)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _correct(
        self, X: pd.DataFrame, feature: str, op: str, operand: Any
    ) -> pd.DataFrame:
        """Correct limits.

        Args:
            X: DataFrame. The dataset to apply the constraint on.
            feature: str. The column in the dataset to apply the constraint on.
            op: str. The operation to execute for the constraint.
            operand: Any. The operand for the binary operation.

        """
        _filter = self._eval(X, feature, op, operand)
        if op in [
            "lt",
            "le",
            "gt",
            "ge",
            "eq",
            "<",
            "<=",
            ">",
            ">=",
            "==",
        ]:
            X.loc[~_filter, feature] = operand

        return X

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def filter(self, X: pd.DataFrame) -> pd.DataFrame:
        """Apply the constraints to a DataFrame X.

        Args:
            X: DataFrame. The dataset to apply the constraints on.

        Returns:
            pandas.Index which matches all the constraints
        """
        X = pd.DataFrame(X)
        res = pd.Series([True] * len(X), index=X.index)
        for feature, op, thresh in self.rules:
            if feature not in X:
                res &= False
                break

            prev = res.sum()
            res &= self._eval(
                X,
                feature,
                op,
                thresh,
            )
            if res.sum() < prev:
                log.info(
                    f"[{feature}] quality loss for constraints {op} = {thresh}. Remaining {res.sum()}. prev length {prev}. Original dtype {X[feature].dtype}.",
                )
        return res

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def match(self, X: pd.DataFrame) -> pd.DataFrame:
        """Apply the constraints to a DataFrame X and return the filtered dataset.

        Args:
            X: DataFrame. The dataset to apply the constraints on.

        Returns:
            The filtered Dataframe
        """

        return X[self.filter(X)]

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def is_valid(self, X: pd.DataFrame) -> bool:
        """Checks if all the rows in X meet the constraints.

        Args:
            X: DataFrame. The dataset to apply the constraints on.

        Returns:
            True if all rows match the constraints, False otherwise
        """

        return self.filter(X).sum() == len(X)

    def extend(self, other: "Constraints") -> "Constraints":
        """Extend the local constraints with more constraints.

        Args:
            other: The new constraints to add.

        Returns:
            self with the updated constraints.
        """
        self.rules.extend(other.rules)

        return self

    def __len__(self) -> int:
        """The number of constraint rules."""
        return len(self.rules)

    def __iter__(self) -> Generator:
        """Iterate the constraint rules."""
        for x in self.rules:
            yield x

    def features(self) -> List:
        """Return list of feature names in an undefined order"""
        results = []
        for feature, _, _ in self.rules:
            results.append(feature)

        return list(set(results))

    def feature_constraints(self, ref_feature: str) -> List:
        """Get constraints for a given feature

        Args:
            ref_feature: str
                The name of the feature of interest.

        Returns:
            A list of tuples of (op, threshold). For example:

            [('le', 3.), ('gt', 1.)]

            If ref_feature has no constraint, None will be returned.
        """
        results = []
        for feature, op, threshold in self.rules:
            if feature != ref_feature:
                continue
            results.append((op, threshold))

        return results

    def feature_params(self, feature: str) -> Tuple:
        """Provide the parameters of Distribution from the Constraint

        This is to be used with the constraint_to_distribution function in distribution module.

        Args:
            feature: str
                The name of the feature of interest.

        Returns:
            dist_template: str
                The type of inferred distribution from ("categorical", "float", "integer")
            dist_args: Dict
                The arguments to the constructor of the Distribution.
        """

        rules = self.feature_constraints(feature)

        dist_template = "float"
        dist_args = {"low": np.iinfo(np.int64).min, "high": np.iinfo(np.int64).max}

        for op, value in rules:
            if op == "in":
                dist_template = "categorical"
                if "choices" not in dist_args:
                    dist_args["choices"] = value
                    continue
                dist_args["choices"] = [v for v in value if v in dist_args["choices"]]

            elif op == "dtype" and value in ["int", "int32", "int64", "integer"]:
                dist_template = "integer"
            elif (op == "le" or op == "<=") and value < dist_args["high"]:
                dist_args["high"] = value
                if "choices" in dist_args:
                    dist_args["choices"] = [
                        v for v in dist_args["choices"] if v <= value
                    ]
            elif (op == "lt" or op == "<") and value < dist_args["high"]:
                dist_args["high"] = value - 1
                if "choices" in dist_args:
                    dist_args["choices"] = [
                        v for v in dist_args["choices"] if v < value
                    ]
            elif (op == "ge" or op == ">=") and dist_args["low"] < value:
                dist_args["low"] = value
                if "choices" in dist_args:
                    dist_args["choices"] = [
                        v for v in dist_args["choices"] if v >= value
                    ]
            elif (op == "gt" or op == ">") and dist_args["low"] < value:
                dist_args["low"] = value + 1
                if "choices" in dist_args:
                    dist_args["choices"] = [
                        v for v in dist_args["choices"] if v > value
                    ]
            elif op == "eq" or op == "==":
                dist_args["low"] = value
                dist_args["high"] = value
                dist_args["choices"] = [value]

        return dist_template, dist_args


src/synthcity/plugins/core/dataset.py
# stdlib
from typing import List, Optional, Tuple

# third party
import numpy as np
import torch

# synthcity absolute
from synthcity.utils.constants import DEVICE


class FlexibleDataset(torch.utils.data.Dataset):
    """Helper dataset wrapper for post-processing or transforming another dataset. Used for controlling the image sizes for the synthcity models.

    The class supports adding custom transforms to existing datasets, and to subsample a set of indices.

    Args:
        data: torch.Dataset
        transform: An optional list of transforms
        indices: An optional list of indices to subsample
    """

    def __init__(
        self,
        data: torch.utils.data.Dataset,
        transform: Optional[torch.nn.Module] = None,
        indices: Optional[list] = None,
    ) -> None:
        super().__init__()

        if indices is None:
            indices = np.arange(len(data))

        self.indices = np.asarray(indices)
        self.data = data
        self.transform = transform
        self.ndarrays: Optional[Tuple[torch.Tensor, torch.Tensor]] = None

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:
        x, y = self.data[self.indices[index]]
        if self.transform:
            x = self.transform(x)
        return x, y

    def __len__(self) -> int:
        return len(self.indices)

    def shape(self) -> Tuple:
        x, _ = self[self.indices[0]]

        return (len(self), *x.shape)

    def numpy(self) -> Tuple[np.ndarray, np.ndarray]:
        if self.ndarrays is not None:
            return self.ndarrays

        x_buff = []
        y_buff = []
        for idx in range(len(self)):
            x_local, y_local = self[idx]
            x_buff.append(x_local.unsqueeze(0).cpu().numpy())
            y_buff.append(y_local)

        x = np.concatenate(x_buff, axis=0)
        y = np.asarray(y_buff)

        self.ndarrays = (x, y)
        return x, y

    def tensors(self) -> Tuple[torch.Tensor, torch.Tensor]:
        x, y = self.numpy()

        return torch.from_numpy(x), torch.from_numpy(y)

    def labels(self) -> np.ndarray:
        labels = []
        for idx in self.indices:
            _, y = self.data[idx]
            labels.append(y)

        return np.asarray(labels)

    def filter_indices(self, indices: List[int]) -> "FlexibleDataset":
        for idx in indices:
            if idx >= len(self.indices):
                raise ValueError(
                    "Invalid filtering list. {idx} not found in the current list of indices"
                )
        return FlexibleDataset(
            data=self.data, transform=self.transform, indices=self.indices[indices]
        )


class TensorDataset(torch.utils.data.Dataset):
    """Helper dataset for wrapping existing tensors

    Args:
        images: Tensor
        targets: Tensor
    """

    def __init__(
        self,
        images: torch.Tensor,
        targets: Optional[torch.Tensor],
    ) -> None:
        super().__init__()

        if targets is not None and len(targets) != len(images):
            raise ValueError("Invalid input")

        self.images = images
        self.targets = targets

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        y: Optional[torch.Tensor] = None
        x = self.images[index]

        if self.targets is not None:
            y = self.targets[index]

        return x, y

    def __len__(self) -> int:
        return len(self.images)

    def labels(self) -> Optional[np.ndarray]:
        if self.targets is None:
            return None

        return self.targets.cpu().numpy()


class ConditionalDataset(torch.utils.data.Dataset):
    """Helper dataset for wrapping existing datasets with custom tensors

    Args:
        data: torch.Dataset
        cond: Optional Tensor
    """

    def __init__(
        self,
        data: torch.utils.data.Dataset,
        cond: Optional[torch.Tensor] = None,
    ) -> None:
        super().__init__()

        if cond is not None and len(cond) != len(data):
            raise ValueError("Invalid input")

        self.data = data
        self.cond = cond

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        cond: Optional[torch.Tensor] = None
        x = self.data[index][0]

        if self.cond is not None:
            cond = self.cond[index]

        return x, cond

    def __len__(self) -> int:
        return len(self.data)


class NumpyDataset(torch.utils.data.Dataset):
    """Helper class for wrapping Numpy arrays in torch Datasets
    Args:
        X: np.ndarray
        y: np.ndarray
    """

    def __init__(self, X: np.ndarray, y: np.ndarray) -> None:
        super().__init__()

        self.X = X
        self.y = y

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:
        x = self.X[index]
        y = self.y[index]

        return torch.from_numpy(x).to(DEVICE), y

    def __len__(self) -> int:
        return len(self.X)


src/synthcity/plugins/core/distribution.py
# stdlib
from abc import ABCMeta, abstractmethod
from datetime import datetime, timedelta, timezone
from typing import Any, List, Optional, Tuple

# third party
import numpy as np
import pandas as pd
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    FieldValidationInfo,
    PrivateAttr,
    ValidationInfo,
    field_validator,
    model_validator,
)

# synthcity absolute
from synthcity.plugins.core.constraints import Constraints

Rule = Tuple[str, str, Any]  # Define a type alias for clarity


class Distribution(BaseModel, metaclass=ABCMeta):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.Distribution
        :parts: 1

    Base class of all Distributions.

    The Distribution class characterizes the **empirical** marginal distribution of the feature.
    Each derived class must implement the following methods:
        get() - Return the metadata of the Distribution.
        sample() - Sample a value from the Distribution.
        includes() - Test if another Distribution is included in the local one.
        has() - Test if a value is included in the support of the Distribution.
        as_constraint() - Convert the Distribution to a set of Constraints.
        min() - Return the minimum of the support.
        max() - Return the maximum of the support.
        __eq__() - Testing equality of two Distributions.
        dtype() - Return the data type

    Examples of derived classes include CategoricalDistribution, FloatDistribution, and IntegerDistribution.
    """

    name: str
    data: Optional[pd.Series] = None
    random_state: Optional[int] = None
    sampling_strategy: str = "marginal"
    _rng: np.random.Generator = PrivateAttr()
    # DP parameters
    marginal_distribution: Optional[pd.Series] = None

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @field_validator("marginal_distribution", mode="before")
    def _validate_marginal_distribution(
        cls: Any, v: Any, values: FieldValidationInfo
    ) -> Optional[pd.Series]:
        if "data" not in values.data or values.data["data"] is None:
            return v

        data = values.data["data"]
        if not isinstance(data, pd.Series):
            raise ValueError(f"Invalid data type {type(data)}")

        marginal = data.value_counts(dropna=False)
        del values["data"]

        return marginal

    @model_validator(mode="after")
    def initialize_rng(cls, model: "Distribution") -> "Distribution":
        """
        Initializes the random number generator after model validation.
        """
        if model.random_state is not None:
            model._rng = np.random.default_rng(model.random_state)
        else:
            model._rng = np.random.default_rng()
        return model

    def marginal_states(self) -> Optional[List]:
        if self.marginal_distribution is None:
            return None

        return self.marginal_distribution.index.values

    def marginal_probabilities(self) -> Optional[List]:
        if self.marginal_distribution is None:
            return None

        return (
            self.marginal_distribution.values / self.marginal_distribution.values.sum()
        )

    def sample_marginal(self, count: int = 1) -> Any:
        if self.marginal_distribution is None:
            return None

        return self._rng.choice(
            self.marginal_states(),
            count,
            p=self.marginal_probabilities(),
        ).tolist()

    @abstractmethod
    def get(self) -> List[Any]:
        """Return the metadata of the Distribution."""
        ...

    @abstractmethod
    def sample(self, count: int = 1) -> Any:
        """Sample a value from the Distribution."""
        ...

    @abstractmethod
    def includes(self, other: "Distribution") -> bool:
        """Test if another Distribution is included in the local one."""
        ...

    @abstractmethod
    def has(self, val: Any) -> bool:
        """Test if a value is included in the Distribution."""
        ...

    @abstractmethod
    def as_constraint(self) -> Constraints:
        """Convert the Distribution to a set of Constraints."""
        ...

    @abstractmethod
    def min(self) -> Any:
        """Get the min value of the distribution."""
        ...

    @abstractmethod
    def max(self) -> Any:
        """Get the max value of the distribution."""
        ...

    def __eq__(self, other: Any) -> bool:
        return type(self) == type(other) and self.get() == other.get()

    def __contains__(self, item: Any) -> bool:
        """
        Example:
        >>> dist = CategoricalDistribution(name="foo", choices=["a", "b", "c"])
        >>> "a" in dist
        True
        """
        return self.has(item)

    @abstractmethod
    def dtype(self) -> str:
        ...


class CategoricalDistribution(Distribution):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.CategoricalDistribution
        :parts: 1
    """

    data: Optional[pd.Series] = None
    marginal_distribution: Optional[pd.Series] = None
    choices: List[Any] = Field(default_factory=list)

    @model_validator(mode="after")
    def validate_and_initialize(
        cls, model: "CategoricalDistribution"
    ) -> "CategoricalDistribution":
        """
        Validates and initializes choices and marginal_distribution based on data or provided choices.
        Ensures that choices are unique and sorted.
        """
        if model.data is not None:
            # Set marginal_distribution based on data
            model.marginal_distribution = model.data.value_counts(normalize=True)
            model.choices = model.marginal_distribution.index.tolist()
        elif model.choices is not None:
            # Ensure choices are unique and sorted
            model.choices = sorted(set(model.choices))
            # Set uniform probabilities
            probabilities = np.ones(len(model.choices)) / len(model.choices)
            model.marginal_distribution = pd.Series(probabilities, index=model.choices)
        else:
            raise ValueError(
                "Invalid CategoricalDistribution: Provide either 'data' or 'choices'."
            )

        # Additional validation to ensure consistency
        if not isinstance(model.choices, list) or len(model.choices) == 0:
            raise ValueError(
                "CategoricalDistribution must have a non-empty 'choices' list."
            )
        if not isinstance(model.marginal_distribution, pd.Series):
            raise ValueError(
                "CategoricalDistribution must have a valid 'marginal_distribution'."
            )
        if len(model.choices) != len(model.marginal_distribution):
            raise ValueError(
                "'choices' and 'marginal_distribution' must have the same length."
            )

        return model

    def sample(self, count: int = 1) -> Any:
        """
        Samples values from the distribution based on the specified sampling strategy.
        If the distribution has only one choice, returns an array filled with that value.
        """
        if self.choices is not None and len(self.choices) == 1:
            samples = np.full(count, self.choices[0])
        else:
            if self.sampling_strategy == "marginal":
                if self.marginal_distribution is None:
                    raise ValueError(
                        "Cannot sample based on marginal distribution: marginal_distribution is not provided."
                    )
                return self._rng.choice(
                    self.marginal_distribution.index,
                    size=count,
                    p=self.marginal_distribution.values,
                )
            elif self.sampling_strategy == "uniform":
                return self._rng.choice(self.choices, size=count)
            else:
                raise ValueError(
                    f"Unsupported sampling strategy '{self.sampling_strategy}'."
                )
        return samples

    def get(self) -> List[Any]:
        """
        Returns the metadata of the distribution.
        """
        return [self.name, self.choices]

    def has(self, val: Any) -> bool:
        """
        Checks if a value is among the distribution's choices.
        """
        return val in self.choices

    def includes(self, other: "Distribution") -> bool:
        """
        Checks if another categorical distribution's choices are a subset of this distribution's choices.
        """
        if not isinstance(other, CategoricalDistribution):
            return False
        return set(other.choices).issubset(set(self.choices))

    def as_constraint(self) -> Constraints:
        """
        Converts the distribution to a set of constraints.
        """
        return Constraints(rules=[(self.name, "in", list(self.choices))])

    def min(self) -> Any:
        """
        Returns the minimum value among the choices.
        """
        return min(self.choices)

    def max(self) -> Any:
        """
        Returns the maximum value among the choices.
        """
        return max(self.choices)

    def dtype(self) -> str:
        """
        Determines the data type based on the choices.
        """
        types = {
            "object": 0,
            "float": 0,
            "int": 0,
        }
        for v in self.choices:
            if isinstance(v, float):
                types["float"] += 1
            elif isinstance(v, int):
                types["int"] += 1
            else:
                types["object"] += 1

        for t in types:
            if types[t] != 0:
                return t

        return "object"


class FloatDistribution(Distribution):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.FloatDistribution
        :parts: 1
    """

    low: Optional[float] = Field(default=None)
    high: Optional[float] = Field(default=None)
    _is_constant: bool = PrivateAttr(False)

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @model_validator(mode="after")
    def validate_and_initialize(cls, model: "FloatDistribution") -> "FloatDistribution":
        """
        Validates and initializes the distribution.
        Sets '_is_constant' based on whether 'low' equals 'high'.
        Initializes 'marginal_distribution' based on 'data' if provided.
        """
        if model.data is not None:
            # Initialize marginal_distribution based on data
            # For float data, use value_counts(normalize=True) if data has repeated values
            # This will create a discrete approximation of the distribution
            model.marginal_distribution = model.data.value_counts(
                normalize=True
            ).sort_index()
            model.low = float(model.data.min())
            model.high = float(model.data.max())
        elif model.marginal_distribution is not None:
            # Set 'low' and 'high' based on marginal_distribution
            model.low = float(model.marginal_distribution.index.min())
            model.high = float(model.marginal_distribution.index.max())
        else:
            # Ensure 'low' and 'high' are provided
            if model.low is None or model.high is None:
                raise ValueError(
                    "FloatDistribution requires 'low' and 'high' values if 'data' or 'marginal_distribution' is not provided."
                )

        # Validate that low <= high
        if model.low > model.high:
            raise ValueError(
                f"Invalid range for '{model.name}': low ({model.low}) cannot be greater than high ({model.high})."
            )

        # Set _is_constant based on low == high
        model._is_constant = model.low == model.high

        # Ensure that low and high are finite numbers
        if not np.isfinite(model.low) or not np.isfinite(model.high):
            raise ValueError(
                f"Invalid range for '{model.name}': low or high is not finite (low={model.low}, high={model.high})."
            )

        return model

    def sample(self, count: int = 1) -> Any:
        """
        Samples values from the distribution.
        If the distribution is constant, returns an array filled with the constant value.
        Otherwise, samples based on the marginal distribution or uniform sampling.
        """
        if self._is_constant:
            if self.low is None:
                raise ValueError(
                    "Cannot sample: 'low' is None for a constant distribution."
                )
            samples = np.full(count, self.low)
        else:
            if self.low is None or self.high is None:
                raise ValueError("Cannot sample: 'low' or 'high' is None.")
            if (
                self.sampling_strategy == "marginal"
                and self.marginal_distribution is not None
            ):
                # Sample based on marginal distribution
                return self._rng.choice(
                    self.marginal_distribution.index.values,
                    size=count,
                    p=self.marginal_distribution.values,
                )
            else:
                # Proceed with uniform sampling
                samples = self._rng.uniform(low=self.low, high=self.high, size=count)
        return samples

    def get(self) -> List[Any]:
        """
        Returns the metadata of the distribution.
        """
        return [self.name, self.low, self.high]

    def has(self, val: Any) -> bool:
        """
        Checks if a value is within the distribution's range.
        """
        return self.low <= val <= self.high

    def includes(self, other: "Distribution") -> bool:
        """
        Checks if another distribution is entirely within this distribution.
        """
        if self.min() is None or self.max() is None:
            return False
        if other.min() is None or other.max() is None:
            return False
        return self.min() <= other.min() and other.max() <= self.max()

    def as_constraint(self) -> Constraints:
        """
        Converts the distribution to a set of constraints.
        """
        return Constraints(
            rules=[
                (self.name, "le", self.high),
                (self.name, "ge", self.low),
                (self.name, "dtype", "float"),
            ]
        )

    def min(self) -> Any:
        """
        Returns the minimum value of the distribution.
        """
        return self.low

    def max(self) -> Any:
        """
        Returns the maximum value of the distribution.
        """
        return self.high

    def dtype(self) -> str:
        """
        Returns the data type of the distribution.
        """
        return "float"


class LogDistribution(FloatDistribution):
    low: float = np.finfo(np.float64).tiny
    high: float = np.finfo(np.float64).max

    def get(self) -> List[Any]:
        return [self.name, self.low, self.high]

    def sample(self, count: int = 1) -> Any:
        msamples = self.sample_marginal(count)
        if msamples is not None:
            return msamples
        lo, hi = np.log2(self.low), np.log2(self.high)
        return 2.0 ** self._rng.uniform(lo, hi, count)


class IntegerDistribution(Distribution):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.IntegerDistribution
        :parts: 1
    """

    low: Optional[int] = Field(default=None)
    high: Optional[int] = Field(default=None)
    step: int = Field(default=1)
    _is_constant: bool = PrivateAttr(False)

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @model_validator(mode="after")
    def validate_and_initialize(
        cls, model: "IntegerDistribution"
    ) -> "IntegerDistribution":
        """
        Validates and initializes the distribution.
        Sets '_is_constant' based on whether 'low' equals 'high'.
        Initializes 'marginal_distribution' based on 'data' if provided.
        """
        if model.data is not None:
            # Initialize marginal_distribution based on data
            model.marginal_distribution = model.data.value_counts(
                normalize=True
            ).sort_index()
            model.low = int(model.data.min())
            model.high = int(model.data.max())
        elif model.marginal_distribution is not None:
            # Infer 'low' and 'high' from the marginal distribution's index
            model.low = int(model.marginal_distribution.index.min())
            model.high = int(model.marginal_distribution.index.max())
        else:
            # Ensure 'low' and 'high' are provided
            if model.low is None or model.high is None:
                raise ValueError(
                    "IntegerDistribution requires 'low' and 'high' values if 'data' or 'marginal_distribution' is not provided."
                )

        # Validate that low <= high
        if model.low > model.high:
            raise ValueError(
                f"Invalid range for '{model.name}': low ({model.low}) cannot be greater than high ({model.high})."
            )

        # Set _is_constant based on low == high
        model._is_constant = model.low == model.high

        # Ensure that low and high are finite integers
        if not np.isfinite(model.low) or not np.isfinite(model.high):
            raise ValueError(
                f"Invalid range for '{model.name}': low or high is not finite (low={model.low}, high={model.high})."
            )

        # Ensure that 'step' is a positive integer
        if model.step <= 0:
            raise ValueError("'step' must be a positive integer.")

        # Adjust 'low' and 'high' to be compatible with 'step'
        model.low = model.low - ((model.low - (model.low % model.step)) % model.step)
        model.high = model.high - (
            (model.high - (model.high % model.step)) % model.step
        )

        # Re-validate after adjustment
        if model.low > model.high:
            raise ValueError(
                f"After adjusting with step, invalid range for '{model.name}': low ({model.low}) cannot be greater than high ({model.high})."
            )

        return model

    def sample(self, count: int = 1) -> Any:
        """
        Samples values from the distribution.
        If the distribution is constant, returns an array filled with the constant value.
        Otherwise, samples based on the marginal distribution or uniform sampling.
        """
        if self._is_constant:
            if self.low is None:
                raise ValueError(
                    "Cannot sample: 'low' is None for a constant distribution."
                )
            samples = np.full(count, self.low)
        else:
            if self.low is None or self.high is None:
                raise ValueError("Cannot sample: 'low' or 'high' is None.")
            if (
                self.sampling_strategy == "marginal"
                and self.marginal_distribution is not None
            ):
                # Sample based on marginal distribution
                return self._rng.choice(
                    self.marginal_distribution.index,
                    size=count,
                    p=self.marginal_distribution.values,
                )
            else:
                if self.low is None or self.high is None:
                    raise ValueError(
                        "Cannot sample based on uniform distribution: low or high is not provided."
                    )
                # Proceed with uniform sampling
                possible_values = np.arange(self.low, self.high + 1, self.step)
                samples = self._rng.choice(possible_values, size=count)
        return samples

    def get(self) -> List[Any]:
        """
        Returns the metadata of the distribution.
        """
        return [self.name, self.low, self.high, self.step]

    def has(self, val: Any) -> bool:
        """
        Checks if a value is within the distribution's range.
        """
        return self.low <= val <= self.high

    def includes(self, other: "Distribution") -> bool:
        """
        Checks if another distribution is entirely within this distribution.
        """
        if self.min() is None or self.max() is None:
            return False
        if other.min() is None or other.max() is None:
            return False
        return self.min() <= other.min() and other.max() <= self.max()

    def as_constraint(self) -> Constraints:
        """
        Converts the distribution to a set of constraints.
        """
        rules: List[Rule] = []
        if self.low is not None:
            rules.append((self.name, "ge", self.low))
        if self.high is not None:
            rules.append((self.name, "le", self.high))
        rules.append((self.name, "dtype", "int"))
        return Constraints(rules=rules)

    def min(self) -> Any:
        """
        Returns the minimum value of the distribution.
        """
        return self.low

    def max(self) -> Any:
        """
        Returns the maximum value of the distribution.
        """
        return self.high

    def dtype(self) -> str:
        """
        Returns the data type of the distribution.
        """
        return "int"


class IntLogDistribution(IntegerDistribution):
    low: int = Field(default=1)
    high: int = Field(default=np.iinfo(np.int64).max)

    @field_validator("step", mode="before")
    def _validate_step(cls: Any, v: int, values: ValidationInfo) -> int:
        if v != 1:
            raise ValueError("Step must be 1 for IntLogDistribution")
        return v

    def get(self) -> List[Any]:
        return [self.name, self.low, self.high]

    def sample(self, count: int = 1) -> Any:
        msamples = self.sample_marginal(count)
        if msamples is not None:
            return msamples
        lo, hi = np.log2(self.low), np.log2(self.high)
        samples = 2.0 ** self._rng.uniform(lo, hi, count)
        return samples.astype(int)


class DatetimeDistribution(Distribution):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.DatetimeDistribution
        :parts: 1
    """

    low: Optional[datetime] = Field(default=None)
    high: Optional[datetime] = Field(default=None)
    step: timedelta = Field(default=timedelta(microseconds=1))
    offset: timedelta = Field(default=timedelta(seconds=120))
    _is_constant: bool = PrivateAttr(False)  # Correctly named with leading underscore

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @model_validator(mode="after")
    def validate_low_high(cls, model: "DatetimeDistribution") -> "DatetimeDistribution":
        """
        Validates that 'low' is less than or equal to 'high'.
        Sets '_is_constant' based on whether 'low' equals 'high'.
        """
        if model.marginal_distribution is not None:
            # Infer 'low' and 'high' from the marginal distribution's index
            model.low = model.marginal_distribution.index.min()
            model.high = model.marginal_distribution.index.max()
        else:
            # If 'marginal_distribution' is not provided, ensure 'low' and 'high' are set
            if model.low is None or model.high is None:
                if model.data is not None:
                    model.low = model.data.min()
                    model.high = model.data.max()
                else:
                    # Set default finite datetime values if not provided
                    model.low = datetime.fromtimestamp(0, timezone.utc)
                    model.high = datetime.now()
        if model.low is None or model.high is None:
            raise ValueError(
                "DatetimeDistribution requires 'low' and 'high' values if 'data' or 'marginal_distribution' is not provided."
            )
        # Validate that low <= high
        if model.low > model.high:
            raise ValueError(
                f"Invalid range for {model.name}: low ({model.low}) cannot be greater than high ({model.high})."
            )

        # Set _is_constant based on low == high
        model._is_constant = model.low == model.high

        # Ensure that low and high are valid datetime objects
        if not isinstance(model.low, datetime) or not isinstance(model.high, datetime):
            raise ValueError(
                f"Invalid range for {model.name}: low or high is not a valid datetime object (low={model.low}, high={model.high})."
            )

        # Ensure that 'step' is positive and non-zero
        if model.step.total_seconds() <= 0:
            raise ValueError("'step' must be a positive timedelta.")

        return model

    def sample(self, count: int = 1) -> Any:
        """
        Samples datetime values from the distribution.
        If the distribution is constant, returns a list filled with the constant datetime value.
        Otherwise, samples based on the specified sampling strategy.
        """
        if self._is_constant:
            if self.low is None:
                raise ValueError(
                    "Cannot sample constant datetime distribution: low is not provided."
                )
            samples = [self.low for _ in range(count)]
        else:
            if self.low is None or self.high is None:
                raise ValueError(
                    "Cannot sample datetime distribution: low or high is not provided."
                )
            if self.sampling_strategy in ["marginal", "uniform"]:
                msamples = self.sample_marginal(count)
                if msamples is not None:
                    return msamples
                if self.low is None or self.high is None:
                    raise ValueError(
                        "Cannot sample based on marginal distribution: low or high is not provided."
                    )
                total_seconds = (self.high - self.low).total_seconds()
                step_seconds = self.step.total_seconds()
                steps = int(total_seconds / step_seconds)
                step_indices = self._rng.integers(0, steps + 1, count)
                samples = [self.low + self.step * int(s) for s in step_indices]
            else:
                raise ValueError(
                    f"Unsupported sampling strategy '{self.sampling_strategy}'."
                )
        return samples

    def get(self) -> List[Any]:
        """
        Returns the metadata of the distribution.
        """
        return [self.name, self.low, self.high, self.step, self.offset]

    def has(self, val: datetime) -> bool:
        """
        Checks if a datetime value is within the distribution's range.
        """
        if self.low is None or self.high is None:
            raise ValueError("Cannot determine 'has' because 'low' or 'high' is None.")
        return self.low <= val <= self.high

    def includes(self, other: "Distribution") -> bool:
        """
        Checks if another datetime distribution is entirely within this distribution, considering the offset.
        """
        if self.low is None or self.high is None:
            return False
        if other.min() is None or other.max() is None:
            return False
        return (
            self.low - self.offset <= other.min()
            and other.max() <= self.high + self.offset
        )

    def as_constraint(self) -> Constraints:
        """
        Converts the distribution to a set of constraints.
        """
        return Constraints(
            rules=[
                (self.name, "le", self.high),
                (self.name, "ge", self.low),
                (self.name, "dtype", "datetime"),
            ]
        )

    def min(self) -> Optional[datetime]:
        """
        Returns the minimum datetime value of the distribution.
        """
        return self.low

    def max(self) -> Optional[datetime]:
        """
        Returns the maximum datetime value of the distribution.
        """
        return self.high

    def dtype(self) -> str:
        """
        Returns the data type of the distribution.
        """
        return "datetime"


class PassThroughDistribution(Distribution):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.PassThroughDistribution
        :parts: 1
    """

    data: pd.Series
    _dtype: str = PrivateAttr("")

    def setup_distribution(self) -> None:
        if self.data is None:
            raise ValueError("'data' must be provided for PassThroughDistribution.")

        # No additional attributes to set up since 'data' is used directly
        # Optionally, store the data type for dtype method
        self._dtype = str(self.data.dtype)

    def sample(self, count: int = 1) -> Any:
        msamples = self.sample_marginal(count)
        if msamples is not None:
            return msamples
        return self.data.sample(
            n=count, replace=True, random_state=self.random_state
        ).values

    def as_constraint(self) -> Constraints:
        # No constraints needed for pass-through columns
        return Constraints(rules=[])

    def get(self) -> List[Any]:
        # Return the unique values or any relevant info
        return [self.name]

    def has(self, val: Any) -> bool:
        # Check if the value exists in the data
        return val in self.data.values

    def includes(self, other: "Distribution") -> bool:
        # Since we are passing through values, we can define includes as checking if all values in other are in self.data
        if isinstance(other, PassThroughDistribution):
            return set(other.data.unique()).issubset(set(self.data.unique()))
        else:
            return False

    def min(self) -> Any:
        return self.data.min()

    def max(self) -> Any:
        return self.data.max()

    def dtype(self) -> str:
        return str(self.data.dtype)


def constraint_to_distribution(constraints: Constraints, feature: str) -> Distribution:
    """Infer Distribution from Constraints.

    Args:
        constraints: Constraints
            The Constraints on features.
        feature: str
            The name of the feature in question.

    Returns:
        The inferred Distribution.
    """
    dist_name, dist_args = constraints.feature_params(feature)

    if dist_name == "categorical":
        dist_template = CategoricalDistribution
    elif dist_name == "integer":
        dist_template = IntegerDistribution
    elif dist_name == "datetime":
        dist_template = DatetimeDistribution
    else:
        dist_template = FloatDistribution

    return dist_template(name=feature, **dist_args)



